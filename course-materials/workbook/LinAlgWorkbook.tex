%**************************************%
%* Generated from MathBook XML source *%
%*    on 2015-11-03T17:12:54-06:00    *%
%*                                    *%
%*   http://mathbook.pugetsound.edu   *%
%*                                    *%
%**************************************%
\documentclass[10pt,]{book}
%% Load geometry package to allow page margin adjustments
\usepackage{geometry}
\geometry{letterpaper,total={5.0in,9.0in}}
%% Custom Preamble Entries, early (use latex.preamble.early)
%% Inline math delimiters, \(, \), made robust with next package
\usepackage{fixltx2e}
%% Page Layout Adjustments (latex.geometry)
%% For unicode character support, use the "xelatex" executable
%% If never using xelatex, the next three lines can be removed
\usepackage{ifxetex}
\ifxetex\usepackage{xltxtra}\fi
%% Symbols, align environment, bracket-matrix
\usepackage{amsmath}
\usepackage{amssymb}
%% allow more columns to a matrix
%% can make this even bigger by overiding with  latex.preamble.late  processing option
\setcounter{MaxMatrixCols}{30}
%% XML, MathJax Conflict Macros
%% Two nonstandard macros that MathJax supports automatically
%% so we always define them in order to allow their use and
%% maintain source level compatibility
%% This avoids using two XML entities in source mathematics
\newcommand{\lt}{<}
\newcommand{\gt}{>}
%% Semantic Macros
%% To preserve meaning in a LaTeX file
%% Only defined here if required in this document
%% Used for inline definitions of terms
\newcommand{\terminology}[1]{\textbf{#1}}
%% Subdivision Numbering, Chapters, Sections, Subsections, etc
%% Subdivision numbers may be turned off at some level ("depth")
%% A section *always* has depth 1, contrary to us counting from the document root
%% The latex default is 3.  If a larger number is present here, then
%% removing this command may make some cross-references ambiguous
%% The precursor variable $numbering-maxlevel is checked for consistency in the common XSL file
\setcounter{secnumdepth}{1}
%% Environments with amsthm package
%% Theorem-like enviroments in "plain" style, with or without proof
\usepackage{amsthm}
\theoremstyle{plain}
%% Numbering for Theorems, Conjectures, Examples, Figures, etc
%% Controlled by  numbering.theorems.level  processing parameter
%% Always need a theorem environment to set base numbering scheme
%% even if document has no theorems (but has other environments)
\newtheorem{theorem}{Theorem}[chapter]
%% Only variants actually used in document appear here
%% Numbering: all theorem-like numbered consecutively
%% i.e. Corollary 4.3 follows Theorem 4.2
%% Definition-like environments, normal text
%% Numbering for definition, examples is in sync with theorems, etc
%% also for free-form exercises, not in exercise sections
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
%% Localize LaTeX supplied names (possibly none)
\renewcommand*{\chaptername}{Chapter}
%% Equation Numbering
%% Controlled by  numbering.equations.level  processing parameter
\numberwithin{equation}{section}
%% Figures, Tables, Floats
%% The [H]ere option of the float package fixes floats in-place,
%% in deference to web usage, where floats are totally irrelevant
%% We redefine the figure and table environments, if used
%%   1) New mbxfigure and/or mbxtable environments are defined with float package
%%   2) Standard LaTeX environments redefined to use new environments
%%   3) Standard LaTeX environments redefined to step theorem counter
%%   4) Counter for new enviroments is set to the theorem counter before caption
%% You can remove all this figure/table setup, to restore standard LaTeX behavior
%% HOWEVER, numbering of figures/tables AND theorems/examples/remarks, etc
%% WILL ALL de-synchronize with the numbering in the HTML version
%% You can remove the [H] argument of the \newfloat command, to allow flotation and 
%% preserve numbering, BUT the numbering may then appear "out-of-order"
\usepackage{float}
\usepackage[bf]{caption} % http://tex.stackexchange.com/questions/95631/defining-a-new-type-of-floating-environment 
\usepackage{newfloat}
% Figure environment setup so that it no longer floats
\SetupFloatingEnvironment{figure}{fileext=lof,placement={H},within=chapter,name=Figure}
% figures have the same number as theorems: http://tex.stackexchange.com/questions/16195/how-to-make-equations-figures-and-theorems-use-the-same-numbering-scheme 
\makeatletter
\let\c@figure\c@theorem
\makeatother
%% Raster graphics inclusion, wrapped figures in paragraphs
\usepackage{graphicx}
%% Colors for Sage boxes and author tools (red hilites)
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
%% Program listing support, for inline code, Sage code or otherwise
\usepackage{listings}
%% We define \listingsfont to provide Bitstream Vera Mono font
%% for program listings, under both pdflatex and xelatex
%% If you remove this, define \listingsfont to be \ttfamily perhaps
\ifxetex
\usepackage{fontspec}\newfontface\listingsfont[Path]{fvmr8a.pfb}
\else
\edef\oldtt{\ttdefault}\usepackage[scaled]{beramono}\usepackage[T1]{fontenc}
\renewcommand*\ttdefault{\oldtt}\newcommand{\listingsfont}{\fontfamily{fvm}\selectfont}
\fi
%% To fix hyphens/dashes rendered in PDF as fancy minus signs by listing
%% http://tex.stackexchange.com/questions/33185/listings-package-changes-hyphens-to-minus-signs
\makeatletter
\lst@CCPutMacro\lst@ProcessOther {"2D}{\lst@ttfamily{-{}}{-{}}}
\@empty\z@\@empty
\makeatother
%% End of program listing font definition
%% Inline code, typically from "c" element
%% Global, document-wide options apply to \lstinline
%% Search/replace \lstinline by \verb to remove this dependency
%% (redefining \lstinline with \verb is unlikely to work)
\lstset{basicstyle=\footnotesize\listingsfont,breaklines=true,breakatwhitespace=true}
%% Sage's blue is 50%, we go way lighter (blue!05 would work)
\definecolor{sageblue}{rgb}{0.95,0.95,1}
%% Sage input, listings package: Python syntax, boxed, colored, line breaking
%% Indent from left margin, flush at right margin
\lstdefinestyle{sageinput}{language=Python,breaklines=true,breakatwhitespace=true,basicstyle=\footnotesize\listingsfont,columns=fixed,frame=single,backgroundcolor=\color{sageblue},xleftmargin=4ex}
%% Sage output, similar, but not boxed, not colored
\lstdefinestyle{sageoutput}{language=Python,breaklines=true,breakatwhitespace=true,basicstyle=\footnotesize\listingsfont,columns=fixed,xleftmargin=4ex}
%% More flexible list management, esp. for references and exercises
%% But also for specifying labels (ie custom order) on nested lists
\usepackage{enumitem}
%% hyperref driver does not need to be specified
\usepackage{hyperref}
%% latex.print parameter set to 'yes', all hyperlinks black and inactive
\hypersetup{draft}
\hypersetup{pdftitle={Math 2500: Linear Algebra}}
%% If you manually remove hyperref, leave in this next command
\providecommand\phantomsection{}
%% Use upright quotes rather than LaTeX's curly quotes
%% If custom font substitutions follow, this might be ineffective
%% If fonts lack upright quotes, the textcomp package is employed
\usepackage{upquote}
%% Graphics Preamble Entries
%% extpfeil package for certain extensible arrows,
%% as also provided by MathJax extension of the same name
%% NB: this package loads mtools, which loads calc, which redefines
%%     \setlength, so it can be removed if it seems to be in the 
%%     way and your math does not use:
%%     
%%     \xtwoheadrightarrow, \xtwoheadleftarrow, \xmapsto, \xlongequal, \xtofrom
%%     
%%     we have had to be extra careful with variable thickness
%%     lines in tables, and so also load this package late
\usepackage{extpfeil}
%% Custom Preamble Entries, late (use latex.preamble.late)
\newtheorem{task}{Task}

%% Convenience macros
\newcommand{\augmatrix}[2]{\left(\begin{array}{@{}#1 |c@{}} #2 \end{array}\right)}
%% Title page information for book
\title{Math 2500: Linear Algebra}
\author{}
\date{}
\begin{document}
\frontmatter
%% begin: half-title
\thispagestyle{empty}
{\centering
\vspace*{0.28\textheight}
{\Huge Math 2500: Linear Algebra}\\}
\clearpage
%% end:   half-title
%% begin: adcard
\thispagestyle{empty}
\null%
\clearpage
%% end:   adcard
%% begin: title page
%% Inspired by Peter Wilson's "titleDB" in "titlepages" CTAN package
\thispagestyle{empty}
{\centering
\vspace*{0.14\textheight}
{\Huge Math 2500: Linear Algebra}\\}
\clearpage
%% end:   title page
%% begin: copyright-page
\thispagestyle{empty}
\vspace*{\stretch{2}}
\vspace*{\stretch{1}}
\null\clearpage
%% end:   copyright-page
%% begin: preface
\chapter*{Preface}\label{preface-1}
\addcontentsline{toc}{chapter}{Preface}
This is a set of class notes designed to guide an inquiry-based course
      on linear algebra. This is not a complete resource! Rather, this text is
      meant to accompany the book Introduction to Linear Algebra by
      Gilbert Strang. Industrious students might also use it for a self-study
      course.
    %
\par
An important feature of this text is the integration of the Sage
      Mathematical Software System. Students in this course will learn to use
      Sage to perform long tedious computations (which linear algebra has in
      spades) and create visualizations.
    %
%% end:   preface
%% begin: acknowledgement
\chapter*{Acknowledgements}\label{acknowledgement-1}
\addcontentsline{toc}{chapter}{Acknowledgements}
I thank the Sage community, in particular William Stein, for creating
      such a wonderful tool and making it open-source.
    %
\par
I also thank Rob Beezer for creating the MathBook project which makes
      this particular book project available on the web.
    %
%% end:   acknowledgement
%% begin: table of contents
\setcounter{tocdepth}{1}
\renewcommand*\contentsname{Contents}
\tableofcontents
%% end:   table of contents
\mainmatter
\typeout{************************************************}
\typeout{Chapter 1 Vectors, the Dot Product, and Matrices}
\typeout{************************************************}
\chapter[Vectors, the Dot Product, and Matrices]{Vectors, the Dot Product, and Matrices}\label{chapter-basic-objects}
\typeout{************************************************}
\typeout{Introduction  }
\typeout{************************************************}

      This first chapter introduces the basic objects of linear algebra.
      You will meet vectors, the dot product, and matrices.
    %
\par

      Vectors are a generalization of the concept of number. Where real numbers
      can help us model the geometry of points on a line, vectors will allow us to
      model the geometry of a plane, or (three-dimensional) space, or even "spaces"
      with higher dimensions. We begin by learning about the algebra of vectors,
      and making connections to the geometry.
    %
\par

      The dot product is a funny kind of multiplication. It plays an important
      role in mathematics because it captures all of the basics of measurement.
      We shall learn how to use the dot product to measure lengths and angles. By
      its definition, the dot product is connected with the concept of a linear
      equation, so it will make frequent appearances in our work.
    %
\par

      Matrices are another way to generalize the concept of number. (In fact, they
      generalize the concept of vector.) We start here by learning about the
      algebra of matrices. The whole rest of this course will focus on matrices,
      their uses, and their properties.
    %
\par

      A running theme for this course is the use of the Sage mathematical software
      system. In order to get started, the fourth section of this course is
      dedicated to getting started with Sage using the SageMathCloud (SMC). You will
      make an account and run through an introductory workshop with SMC.
      Also, throughout this workbook you will find little embedded pieces of Sage
      code. These are implemented using the Sage SingleCell server. Most of these
      Sage cells are mutable. You can change the content in them and re-evaluate
      your new Sage code. I encourage you to play with these---it is a good way to
      learn the basics of Sage.
    %
\par

      The fifth and final section of the chapter is a short assignment designed to
      consolidate learning. You will get a chance to practice your skills and to
      think more deeply about the concepts you have learned.
    %
\typeout{************************************************}
\typeout{Section 1.1 Vectors}
\typeout{************************************************}
\section[Vectors]{Vectors}\label{vectors}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-1}
\begin{itemize}[label=\textbullet]
\item{}Read section 1.1 of \emph{Strang} (pages 1-7).\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Objectives}
\typeout{************************************************}
\subsection[Learning Objectives]{Learning Objectives}\label{subsection-2}
Before class, a student should be able to:
      \begin{itemize}[label=\textbullet]
\item{}Add vectors.\item{}Multiply a vector by a scalar.\item{}Compute linear combinations.\item{}Draw pictures which correspond to the above operations.\end{itemize}

      At some point, a student should be able to:
      \begin{itemize}[label=\textbullet]
\item{}Solve linear combination equations involving unknown coefficients.\item{}Solve linear combination equations involving unknown vectors.\end{itemize}

    %
\typeout{************************************************}
\typeout{Subsection  Some Discussion}
\typeout{************************************************}
\subsection[Some Discussion]{Some Discussion}\label{subsection-3}
Algebraically, a \terminology{vector} is a stack of numbers in a set of
     parentheses or brackets, like this\begin{align*}
\begin{pmatrix} 2 \\ 7 \\ 9 \end{pmatrix}, \text{ or }
          \begin{bmatrix}2 \\ 7 \\ 9 \end{bmatrix}, \text{ or }
          \begin{pmatrix} 2 & 7 & 9 \end{pmatrix}.
\end{align*}
     The individual numbers are called the \terminology{components} or \terminology{entries}
     or \terminology{coordinates} of the vector. For example, \(7\) is the second
     component of the vectors above.%
\par
The first two vectors above are called \terminology{column} vectors
        because they are stacked vertically. The third is called a \terminology{row} vector because it is arranged horizontally. For this class, we will always use column vectors, but to save space, we might sometimes write them as row vectors. It is up to you to make the switch. (We will see later how this matters!)%
\par
Vectors can take lots of different sizes. The vectors above are all \(3\)-vectors. Here is a \(2\)-vector: \[\begin{pmatrix} 71 \\ -12 \end{pmatrix}.\] Here is a \(4\)-vector: \[\begin{pmatrix} \pi \\ 0 \\ -\pi \\ 1\end{pmatrix}.\]%
\par
The main value in using vectors lies in their standard interpretations. Let's focus on \(3\)-vectors for now. The vector \(\left(\begin{smallmatrix} a \\ b \\ c\end{smallmatrix}\right)\) can represent \begin{itemize}[label=\textbullet]
\item{}A point in space described in the standard three-dimensional rectangular coordinate system with \(x\) coordinate equal to \(a\), \(y\)-coordinate equal to \(b\) and \(z\) coordinate equal to \(c\).%
\item{}An arrow in space which points from the origin \((0,0,0)\) to the point \((a,b,c)\).%
\item{}An arrow in space which points from some point \((x,y,z)\) to the point \((x+a,y+b,z+c)\).%
\end{itemize}

        %
\typeout{************************************************}
\typeout{Subsubsection  Operations on Vectors}
\typeout{************************************************}
\subsubsection[Operations on Vectors]{Operations on Vectors}\label{subsubsection-1}
There are two operations on vectors which are of utmost importance for linear algebra. (In fact, if your problem has these operations in it, there is a chance you are doing linear algebra already.)%
\typeout{************************************************}
\typeout{Paragraph  Scalar Multiplication}
\typeout{************************************************}
\paragraph[Scalar Multiplication]{Scalar Multiplication}\label{paragraph-1}
Given a number \(\lambda \in \mathbb{R}\) and a vector \(v = \left(\begin{smallmatrix} a \\ b \\ c \end{smallmatrix}\right)\), we form the new vector \[\lambda v = \left(\begin{smallmatrix} \lambda a \\ \lambda b \\ \lambda c \end{smallmatrix}\right).\]%
\typeout{************************************************}
\typeout{Paragraph  Addition}
\typeout{************************************************}
\paragraph[Addition]{Addition}\label{paragraph-2}
Given a vector \(v = \left(\begin{smallmatrix} a \\ b \\ c \end{smallmatrix}\right)\) and a vector \(w = \left(\begin{smallmatrix} d \\ e \\ f \end{smallmatrix}\right)\) of the same size, we form their \terminology{sum} \[ v+w = \left(\begin{smallmatrix} a+d \\ b+e \\ c+f \end{smallmatrix}\right) \]%
\typeout{************************************************}
\typeout{Paragraph  }
\typeout{************************************************}
\paragraph[]{}\label{paragraph-3}
These operations have ``obvious'' generalizations to vectors of different sizes. Because things go entry-by-entry, these are often called \terminology{coordinate-wise} operations.%
\par
Combining these two operations gives us the notion of a \emph{linear combination}. If \(\lambda\) and \(\mu\) are numbers and \(v\) and \(w\) are vectors of a common size, then the vector \[\lambda v + \mu w \] is a linear combination of \(v\) and \(w\).%
\typeout{************************************************}
\typeout{Subsection  Sage Instructions}
\typeout{************************************************}
\subsection[Sage Instructions]{Sage Instructions}\label{subsection-4}
\typeout{************************************************}
\typeout{Paragraph  Basic Constructions}
\typeout{************************************************}
\paragraph[Basic Constructions]{Basic Constructions}\label{paragraph-4}
A vector in Sage is constructed by applying the \lstinline?vector? command to a list. Lists are entered in square brackets with entries separated by commas, so the typical way to create a vector looks like this:%
\begin{lstlisting}[style=sageinput]
u = vector([1,1,2])
\end{lstlisting}
\par
Notice that nothing was displayed. Sage just put the vector u into memory. We can ask for it by calling it.%
\begin{lstlisting}[style=sageinput]
u
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(1, 1, 2)
\end{lstlisting}
\par
Sage defaults to displaying vectors horizontally, which is different from how we normally write them by hand.
              This is okay. You will get used to it quickly.%
\par
Sage knows how to add, multiply by scalars, and form linear combinations, and the notation for it is just as easy as you would expect.%
\begin{lstlisting}[style=sageinput]
v = vector([-1,-1,2])
u + v
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(0, 0, 4)
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
pi * u
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(pi, pi, 2*pi)
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
3*u + 4*v
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(-1, -1, 14)
\end{lstlisting}
\par
If you ask Sage to plot a vector, you get this kind of picture:%
\begin{lstlisting}[style=sageinput]
plot(v)
\end{lstlisting}
\par
And in two dimensions something similar...%
\begin{lstlisting}[style=sageinput]
a = vector([-1,1])
plot(a)
\end{lstlisting}
\par
If you find that you want a vector to have its tail someplace
              that is not the origin, use the \lstinline?arrow? command.%
\begin{lstlisting}[style=sageinput]
plot(arrow([1,1],[2,3], color='red',
           arrowsize=2, width=2),
     figsize=5, aspect_ratio=1)
\end{lstlisting}
\par
Note that Sage cut off some of this plot! Also, I used some options
              just to show them off. The \lstinline?arrow? command works in three dimensions,
              too.
            %
\typeout{************************************************}
\typeout{Paragraph  Interactive Demonstrations}
\typeout{************************************************}
\paragraph[Interactive Demonstrations]{Interactive Demonstrations}\label{paragraph-5}
This is a Sage "interact." You can use this to explore the idea of
            linear combinations of \(2\)-vectors.%
\begin{lstlisting}[style=sageinput]
@interact(layout= {'top':[['a','c','e'],['b','d','f'],['l','m']]})
def two_dim_plot(a=input_box(1,width=10), b=input_box(2,width=10),c=input_box(2,width=10), d=input_box(1,width=10),
                 l=input_box(1,width=10), m=input_box(1,width=10), e=input_box(2,width=10),f=input_box(2,width=10)):
    two_dim = arrow([0,0], [a,b], color ='red') + arrow([0,0],[c,d],color='blue')
    two_dim+= arrow([0,0], [l*a,l*b], color='red') + arrow([m*c,m*d],[l*a+m*c,l*b+m*d],color='red')
    two_dim+= arrow([0,0], [m*c,m*d], color='blue') + arrow([l*a,l*b],[l*a+m*c,l*b+m*d],color='blue')
    two_dim+= point([e,f],size=20,color='black',zorder=2)+ arrow([l*a,l*b],[l*a+m*c,l*b+m*d],color='blue')
    two_dim+= text('v = (a,b)', [a-.1,b+.1], color='red') + text('w=(c,d)', [c+.1,d-.1],color='purple')
    two_dim+= text('l*v + m*w', [l*a+m*c+.1, l*b+m*d+.1],color='purple') + text('P=(e,f)', [e+.1,f+.1],color='black')
    two_dim+= arrow([0,0],[l*a+m*c,l*b+m*d],color='purple', arrowsize=1, width=1)
    two_dim.show(axes=True)
\end{lstlisting}
\par
This is a different Sage interact. You can use this one to
                explore linear combinations of \(2\)-vectors.
              %
\begin{lstlisting}[style=sageinput]
@interact(layout= {'top':[['a','c','e'],['b','d','f'],['l','m']]})
def two_dim_plot(a=input_box(1,width=10), b=input_box(2,width=10),c=input_box(2,width=10), d=input_box(1,width=10),
                 l=input_box(1,width=10), m=input_box(1,width=10), e=input_box(2,width=10),f=input_box(2,width=10)):
    two_dim = arrow([0,0], [a,b], color ='red') + arrow([0,0],[c,d],color='blue')
    two_dim+= arrow([0,0], [l*a,l*b], color='red') + arrow([m*c,m*d],[l*a+m*c,l*b+m*d],color='red')
    two_dim+= arrow([0,0], [m*c,m*d], color='blue') + arrow([l*a,l*b],[l*a+m*c,l*b+m*d],color='blue')
    two_dim+= point([e,f],size=20,color='black',zorder=2)+ arrow([l*a,l*b],[l*a+m*c,l*b+m*d],color='blue')
    two_dim+= text('v = (a,b)', [a-.1,b+.1], color='red') + text('w=(c,d)', [c+.1,d-.1],color='purple')
    two_dim+= text('l*v + m*w', [l*a+m*c+.1, l*b+m*d+.1],color='purple') + text('P=(e,f)', [e+.1,f+.1],color='black')
    two_dim+= arrow([0,0],[l*a+m*c,l*b+m*d],color='purple', arrowsize=1, width=1)
    two_dim.show(axes=True)
\end{lstlisting}
\begin{task}
\label{task-1}
Find an example of numbers \(\lambda\) and \(\mu\) so that \[\lambda \begin{pmatrix} 1 \\ 2 \end{pmatrix} = \mu \begin{pmatrix} 2 \\ -1 \end{pmatrix}\] or describe why no such example can exist.%
\end{task}
\begin{task}
\label{task-2}
Find a vector \(b = \left( \begin{smallmatrix} b_1 \\ b_2
                  \end{smallmatrix} \right)\) so that \[ \begin{pmatrix} 2
                  \\ 7 \end{pmatrix} + \begin{pmatrix} b_1 \\ b_2 \end{pmatrix}
                  = \begin{pmatrix} 10 \\ -3 \end{pmatrix}\] or describe why
                  no such example can exist.%
\end{task}
\begin{task}
\label{task-3}
Find a vector \(b = \left( \begin{smallmatrix} b_1 \\ b_2
                  \end{smallmatrix} \right)\) so that this equation has at
                  least one solution \(\lambda\) \[\begin{pmatrix} 1 \\ -2
                  \end{pmatrix} + \lambda \begin{pmatrix} b_1 \\ b_2 \end{pmatrix}
                  = \begin{pmatrix} 2 \\ 3 \end{pmatrix}\] or describe why no
                  such example can exist.%
\end{task}
\begin{task}
\label{task-4}
Give examples of numbers \(a\) and \(b\) such that \[ a \begin{pmatrix} 2 \\ 1 \end{pmatrix} + b \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} 7 \\ 5 \end{pmatrix}\] or explain why no such numbers exist.%
\end{task}
In the situations like the last exercise, the pair of numbers \(a, b\) is called a \terminology{solution} to the equation.%
\begin{task}
\label{task-5}
Give an example of a vector \(X = \left( \begin{smallmatrix} x \\ y \end{smallmatrix} \right)\) so that the equation \[a \begin{pmatrix} 2 \\ 1 \end{pmatrix} + b X = \begin{pmatrix}7 \\ 5 \end{pmatrix} \] has no solution \((a,b)\), or explain why no such example exists.%
\end{task}
\begin{task}
\label{task-6}
Give an example of a number \(\lambda\) so that \[\lambda \begin{pmatrix} 7 \\ -1 \\ 2 \end{pmatrix} + 3 \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 49 \\ -7 \\ 20 \end{pmatrix}\] or explain why no such number exists.%
\end{task}
\begin{task}
\label{task-7}
Give an example of numbers \(\lambda\) and \(\mu\) which are a solution to the equation \[ \lambda \begin{pmatrix} 7 \\ -1 \\ 2 \end{pmatrix} + \mu \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 49 \\ -7 \\ 20 \end{pmatrix}\] or explain why no such solution exists.%
\end{task}
\begin{task}
\label{task-8}
Give an example of a vector \(w = \begin{pmatrix} x \\ y \\
                  z \end{pmatrix}\) so that the equation \[a \begin{pmatrix}
                  1 \\ 1 \\ 0 \end{pmatrix} + b \begin{pmatrix} 0 \\ 0 \\ 1
                  \end{pmatrix} = \begin{pmatrix} x \\ y \\ z \end{pmatrix}\]
                  has no solution \((a,b)\), or explain why no such vector
                  exists.%
\end{task}
\begin{task}
\label{task-9}
Give an example of a vector \(w = \begin{pmatrix} x \\ y \\ z
                  \end{pmatrix}\) so that the equation \[ a \begin{pmatrix}
                  1 \\ 1 \\ 0 \end{pmatrix} + b \begin{pmatrix} 0 \\ 0 \\ 1
                  \end{pmatrix} = \begin{pmatrix} x \\ y \\ z \end{pmatrix}\]
                  has exactly one solution \((a,b)\), or explain why no such
                  vector exists.%
\end{task}
\begin{task}
\label{task-10}
Give an example of a vector \(X =
                  \begin{pmatrix} x \\ y \\ z\end{pmatrix}\) such that the
                  equation \[ a \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} + b
                  \begin{pmatrix} 0 \\ 0 \\ 1\end{pmatrix} + c \begin{pmatrix} 1
                  \\ 1 \\ 1\end{pmatrix} = \begin{pmatrix}x \\ y \\ z \end{pmatrix}
                \] has no solutions \((a,b,c)\), or explain why no such vector
                  exists.%
\end{task}
\begin{task}
\label{task-11}
Give an example of a vector \(X = \begin{pmatrix} x \\ y \\ z\end{pmatrix}\) such that the equation \[ a \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} + b \begin{pmatrix} 0 \\ 0 \\ 1\end{pmatrix} + c \begin{pmatrix} 1 \\ 1 \\ 1\end{pmatrix} = \begin{pmatrix}x \\ y \\ z \end{pmatrix}\] has exactly one solution, or explain why no such vector exists.%
\end{task}
\begin{task}
\label{task-12}
Give an example of a vector \(X = \begin{pmatrix} x \\ y \\ z\end{pmatrix}\) such that the equation \[ a \begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix} + b \begin{pmatrix} 0 \\ -1 \\ 0\end{pmatrix} + c \begin{pmatrix} x \\ y \\ z\end{pmatrix} = \begin{pmatrix}3 \\ 7 \\ 7 \end{pmatrix}\] has no solutions, or explain why no such vector exists.%
\end{task}
\typeout{************************************************}
\typeout{Section 1.2 The Dot Product}
\typeout{************************************************}
\section[The Dot Product]{The Dot Product}\label{dot-product}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-5}
\begin{itemize}[label=\textbullet]
\item{}Read section 1.2 of \emph{Strang}\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Objectives}
\typeout{************************************************}
\subsection[Learning Objectives]{Learning Objectives}\label{subsection-6}

      Before class, a student should be able to
      \begin{itemize}[label=\textbullet]
\item{}Compute the dot product of two given vectors.\item{}Compute the length of a given vector.\item{}Normalize a given vector.\item{}Recognize that \(u \cdot v =0\) is the same as "\(u\) and \(
            v\) are orthogonal."\item{}
          Compute the angle between two given vectors using the cosine formula.
        \end{itemize}

      At some point, a student should be able to
      \begin{itemize}[label=\textbullet]
\item{}
          Interpret the statements \(u\cdot v < 0 \) and \(u \cdot v > 0\)
          geometrically.
        \item{}Pass back and forth between linear equations and equations involving
          dot products.
        \item{}Make pictures of level sets of the dot product operation.\end{itemize}

    %
\typeout{************************************************}
\typeout{Subsection  Discussion}
\typeout{************************************************}
\subsection[Discussion]{Discussion}\label{subsection-7}

      The dot product is a wonderful tool for encoding the geometry of Euclidean
      space, but it can be a bit mysterious at first. As Strang shows,
      it somehow holds all of the information you need to measure lengths and
      angles.
    %
\par

      What does this weird thing have to do with linear algebra? A dot product
      with a "variable vector" is a way of writing a linear equation. For example,
      \begin{gather*}
\begin{pmatrix} 7 \\ 3 \\ -2 \end{pmatrix} \cdot \begin{pmatrix}
        x \\ y \\ z \end{pmatrix} = 7x+3y-2z.
\end{gather*}
      Sometimes this will allow us to connect linear algebra to geometry, and use
      geometric thinking to answer algebraic questions.
    %
\typeout{************************************************}
\typeout{Subsection  Sage and the dot product}
\typeout{************************************************}
\subsection[Sage and the dot product]{Sage and the dot product}\label{subsection-8}
\typeout{************************************************}
\typeout{Subsubsection  Basic Commands}
\typeout{************************************************}
\subsubsection[Basic Commands]{Basic Commands}\label{subsubsection-2}

      Sage has a built-in dot product command \lstinline?u.dot_product(v)?. This
      will return the dot product of the vectors \(u\) and \(v\).
    %
\begin{lstlisting}[style=sageinput]
u = vector([1,1,1]); v = vector([-1,0,1])
u.dot_product(v)
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
0
\end{lstlisting}
\par

      It also has a built-in command for computing lengths. Here sage uses the
      synonym "norm": \lstinline?u.norm()?. Of course, you can also call this like
      a function instead of like a method: \lstinline?norm(u)?.
    %
\begin{lstlisting}[style=sageinput]
u.norm(), v.norm()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(sqrt(3), sqrt(2))
\end{lstlisting}
\par

      There is no built-in command for angles. You just have to compute them
      using the cosine formula, like below. (I will break up the computation,
      but it is easy to do it all with one line.)
    %
\begin{lstlisting}[style=sageinput]
num = u.dot_product(v)
den = u.norm() * v.norm()
angle = arccos(num / den)
angle
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
1/2*pi
\end{lstlisting}
\par

      Of course, Sage's \lstinline?arccos? command returns a result in \emph{radians}.
      To switch to degrees, you must convert.
    %
\begin{lstlisting}[style=sageinput]
angle*180/pi
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
90
\end{lstlisting}
\par
Often, it is helpful to normalize a vector. You can do this with the
      \lstinline?normalized? method like this:
    %
\begin{lstlisting}[style=sageinput]
u.normalized()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(1/3*sqrt(3), 1/3*sqrt(3), 1/3*sqrt(3))
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsubsection  Sage Interacts}
\typeout{************************************************}
\subsubsection[Sage Interacts]{Sage Interacts}\label{subsubsection-3}
Interacts would go here...%
\typeout{************************************************}
\typeout{Subsection  Exercises about the Dot Product}
\typeout{************************************************}
\subsection[Exercises about the Dot Product]{Exercises about the Dot Product}\label{subsection-9}
\begin{task}
\label{task-13}
What shape is the set of solutions \(\left(\begin{smallmatrix} x \\ y
        \end{smallmatrix}\right)\) to the equation\[
            \begin{pmatrix} 3 \\ 7\end{pmatrix} \cdot \begin{pmatrix} x \\ y
            \end{pmatrix} = 5?
        \]
        %
\par
That is, if we look at all possible vectors \(\left(\begin{smallmatrix}
          x \\ y \end{smallmatrix}\right)\) which make the equation true, what shape
          does this make in the plane?%
\par
What happens if we change the vector \(\left(\begin{smallmatrix} 3
          \\ 7 \end{smallmatrix}\right)\) to some other vector? What happens
          if we change the number \(5\) to some other number?
        %
\par
What happens if instead of \(2\)-vectors, we use \(3\)-vectors?
        %
\end{task}
\begin{task}
\label{task-14}
Find an example of two \(2\)-vectors \(v\) and \(w\)
      so that \(\left(\begin{smallmatrix}1 \\ 2 \end{smallmatrix}\right)\cdot v
      =0\) and \(\left(\begin{smallmatrix}1 \\ 2 \end{smallmatrix}\right)\cdot
      w = 0\), or explain why such an example is not possible.
      \end{task}
\begin{task}
\label{task-15}
Let \(v = \left(\begin{smallmatrix}3\\-1
        \end{smallmatrix}\right)\). Find an example of a pair of vectors
        \(u\) and \(w\) such that \(v \cdot u < 0\) and
        \(v \cdot w < 0\) and \(w \cdot u = 0\), or explain why no such
        pair of vectors can exist.
      \end{task}
\begin{task}
\label{task-16}
Find an example of three \(2\)-vectors \(u\), \(v\),
        and \(w\) so that \( u \cdot v < 0\) and \(u\cdot w < 0 \)
        and \(v \cdot w < 0\), or explain why no such example exists.
      \end{task}
\begin{task}
\label{task-17}

        Find an example of a number \(c\) so that \[
        \begin{pmatrix} 1 \\ -1 \end{pmatrix} \cdot \begin{pmatrix} x \\ y
          \end{pmatrix} = c
        \] has the vector \(\left(\begin{smallmatrix}4 \\ 7 \end{smallmatrix}\right)\)
        as a solution, or explain why no such number exists.
      \end{task}
\begin{task}
\label{task-18}

        Let \(v = \left(\begin{smallmatrix}2\\1\end{smallmatrix}\right)\) and
        \(w=\left(\begin{smallmatrix}-3\\4\end{smallmatrix}\right)\). Find
        an example of a number \(c\) so that \begin{gather*}
v \cdot \begin{pmatrix}1\\-1\end{pmatrix} = c \quad\text{ and }
        \quad w \cdot \begin{pmatrix}1\\-1\end{pmatrix} = c,
\end{gather*}
        or explain why this is not possible.
      \end{task}
\begin{task}
\label{task-19}

        Let \(P = \left(\begin{smallmatrix}-3\\4\end{smallmatrix}\right)\).
        Find an example of numbers \(c\) and \(d\) so that \begin{gather*}
\begin{pmatrix} 2\\-1\end{pmatrix}\cdot P = c \quad\text{ and }
        \quad \begin{pmatrix} 1\\-1\end{pmatrix}\cdot P = d,
\end{gather*}
        or explain why no such example is possible.
      \end{task}
Now we move to three dimensions!%
\begin{task}
\label{task-20}

        Let \(V = \left(\begin{smallmatrix}1\\1\\1\end{smallmatrix}\right)\).
        Find a unit vector of the form \(X = \left(\begin{smallmatrix}x\\y\\0
        \end{smallmatrix}\right)\) so that \(V\cdot X = \sqrt{2}\), or explain
        why no such vector exists.
      \end{task}
\begin{task}
\label{task-21}

        Find an example of numbers \(c\), \(d\), and \(e\) so that there
        is no solution vector \(X = \left(\begin{smallmatrix}x\\y\\z
        \end{smallmatrix}\right)\) which simultaneously satisfies the three
        equations \begin{gather*}
\begin{pmatrix} 1\\1\\1\end{pmatrix}\cdot X = c, \qquad
          \begin{pmatrix} 2\\2\\2\end{pmatrix}\cdot X = d, \qquad
          \begin{pmatrix} 0\\0\\1\end{pmatrix}\cdot X = e,
\end{gather*}
          or explain why no such numbers exist.
      \end{task}
\begin{task}
\label{task-22}

        Find an example of numbers \(c\), \(d\), and \(e\) so that there
        is no solution vector \(X = \left(\begin{smallmatrix}x\\y\\z
        \end{smallmatrix}\right)\) which simultaneously satisfies the three
        equations \begin{gather*}
\begin{pmatrix} 1\\1\\1\end{pmatrix}\cdot X = c, \qquad
          \begin{pmatrix} 0\\1\\1\end{pmatrix}\cdot X = d, \qquad
          \begin{pmatrix} 0\\0\\1\end{pmatrix}\cdot X = e,
\end{gather*}
          or explain why no such numbers exist.
      \end{task}
\typeout{************************************************}
\typeout{Section 1.3 Matrices}
\typeout{************************************************}
\section[Matrices]{Matrices}\label{matrices}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-10}
\begin{itemize}[label=\textbullet]
\item{}Read \emph{Strang} section 1.3 (pages 22-27).\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-11}
Before class starts, a student should be able to:
      \begin{itemize}[label=\textbullet]
\item{}multiply a matrix times a vector
          \begin{itemize}[label=$\circ$]
\item{}as a linear combination of columns\item{}as a set of dot products, row times column\end{itemize}

        \item{}translate back and forth between our three representations
          \begin{itemize}[label=$\circ$]
\item{}a system of linear equations,\item{}a linear combination of vectors equation, and\item{}a matrix equation \(Ax=b\).\end{itemize}

        \item{}Correctly identify the rows and columns of a matrix\item{}describe what is meant by a lower triangular matrix\end{itemize}

    At some point, as student should be comfortable with these concepts, which
    get a very brief informal introduction in this section:
    \begin{itemize}[label=\textbullet]
\item{}linear dependence and linear independence\item{}the inverse of a matrix\end{itemize}

  %
\typeout{************************************************}
\typeout{Subsection  Discussion}
\typeout{************************************************}
\subsection[Discussion]{Discussion}\label{subsection-12}

      A \terminology{matrix} is a two-dimensional array of numbers like this:\[
        A = \begin{pmatrix} 2 & 1 \\ 1 & 1 \end{pmatrix}.
      \]
      Sometimes it helps to think of a matrix as a collection of its \terminology{rows}
      which are read across:\[
        M = \begin{pmatrix} \longrightarrow \\ \longrightarrow \end{pmatrix}
      \]
      and sometimes it helps to think of a matrix as a collection of its
      \terminology{columns} which are read down:\[
        M = \begin{pmatrix} \downarrow & \downarrow \end{pmatrix}.
      \]
    %
\par

      It is often more clear to describe a matrix by giving the sizes of its
      rows and columns. An \(m\) by \(n\) matrix is one having \(m\)
      rows and \(n\) columns. It is really easy to get these reversed, so be
      careful. For example, this is a \(2\times 3\) matrix, because it has
      two rows and three columns:\[
        B = \begin{pmatrix} 1 & 1 & 2 \\ 3 & 5 & 8 \end{pmatrix}
      \]
      A matrix is called a \terminology{square} matrix when the number of rows and
      the number of columns is equal. The matrix \(A\) that I wrote down
      above is square because it is a \(2\times 2\) matrix.
    %
\typeout{************************************************}
\typeout{Subsubsection  Multiplying Matrices and Vectors}
\typeout{************************************************}
\subsubsection[Multiplying Matrices and Vectors]{Multiplying Matrices and Vectors}\label{subsubsection-4}

        It is possible to multiply a matrix by a vector like this:\[
          \begin{pmatrix} 1 & 1 & 2 \\ 3 & 5 & 8 \end{pmatrix}
          \begin{pmatrix} 13 \\ 21 \\ 34 \end{pmatrix} =
          \begin{pmatrix} 102 \\ 416 \end{pmatrix}
        \]
        For this to work, it is absolutely crucial that the sizes match up
        properly. If the matrix is \(m\) by \(n\), then the vector must have
        size \(n\). In the above example \(m = 2\) and \(n=3\).
      %
\par

        Later, we shall see that the word "multiplication" is not really the
        best choice here. It is better to think of the matrix as "acting on"
        the vector and turning it into a new vector. For now, the word
        multiplication will serve.
      %
\par

        How exactly does one define this matrix--vector multiplication?
      %
\typeout{************************************************}
\typeout{Paragraph  Linear Combination of Columns Approach}
\typeout{************************************************}
\paragraph[Linear Combination of Columns Approach]{Linear Combination of Columns Approach}\label{paragraph-6}

          The first way to perform the matrix-vector multiplication is to think
          of the vector as holding some coefficients for forming a linear
          combination of the columns of the matrix. In our example, it looks
          like this:\[
            \begin{pmatrix} 1 & 1 & 2 \\ 3 & 5 & 8 \end{pmatrix}
            \begin{pmatrix} 13 \\ 21 \\ 34 \end{pmatrix} =
            13 \begin{pmatrix} 1 \\ 3 \end{pmatrix} + 21 \begin{pmatrix} 1 \\ 5
            \end{pmatrix} + 34 \begin{pmatrix} 2 \\ 8 \end{pmatrix} =
            \begin{pmatrix} 102 \\ 416 \end{pmatrix}
          \]
        %
\typeout{************************************************}
\typeout{Paragraph  Dot Products with the Rows Approach}
\typeout{************************************************}
\paragraph[Dot Products with the Rows Approach]{Dot Products with the Rows Approach}\label{paragraph-7}

          The second way is to think of the matrix as a bundle of vectors lying
          along the rows of the matrix, and use the dot product. In our example
          above, this means that we consider the vectors\[
            r_1 = \begin{pmatrix} 1 \\ 1 \\ 2 \end{pmatrix}, \quad
            r_2 = \begin{pmatrix} 3 \\ 5 \\ 8 \end{pmatrix}, \text{ and }
            v = \begin{pmatrix} 13 \\ 21 \\ 34 \end{pmatrix}
          \]
          (notice I've rewritten the rows as columns) and then perform this
          kind of operation:\[
            \begin{pmatrix} 1 & 1 & 2 \\ 3 & 5 & 8 \end{pmatrix}
            \begin{pmatrix} 13 \\ 21 \\ 34 \end{pmatrix} = \begin{pmatrix} r_1
            \\ r_2 \end{pmatrix} v =  \begin{pmatrix} r_1 \cdot v \\ r_2 \cdot
            v \end{pmatrix} =
            \begin{pmatrix} 102 \\ 416 \end{pmatrix} .
          \]
        %
\par

      Two important remarks:
      \begin{itemize}[label=\textbullet]
\item{}
          Note that these operations only make sense if the sizes match up properly.
        \item{}
          Note that the two versions of the operation give you the same results.
        \end{itemize}

      %
\typeout{************************************************}
\typeout{Subsubsection  Matrix Equations}
\typeout{************************************************}
\subsubsection[Matrix Equations]{Matrix Equations}\label{subsubsection-5}

        There are many situations in linear algebra that can be rewritten in
        the form of an equation that looks like this:\[
          A v = b
        \]
        where \(A\) is a matrix, and \(v\) and \(b\) are vectors. The
        interesting case is when we know \(A\) and \(b\), but we want to
        find the unknown \(v\). We will call this a
        \terminology{matrix-vector equation}.
      %
\par

        Let's consider the case where you are given some \terminology{square}
        matrix \(A\). Sometimes one can find another matrix \(B\) so that
        no matter what vector \(b\) is chosen in the matrix-vector equation
        above, the  solution vector takes the form \(v = Bb\). When this
        happens, we say that \(A\) is \terminology{invertible} and call \(B\)
        the \terminology{inverse} of \(A\). It is common to use the notation
        \(A^{-1}\) in place of \(B\). This is a wonderful situation to be
        in! Eventually, we will want to figure out some test for when a given
        matrix is invertible, and find some ways to compute the inverse.
      %
\typeout{************************************************}
\typeout{Subsubsection  A Note about Vectors}
\typeout{************************************************}
\subsubsection[A Note about Vectors]{A Note about Vectors}\label{subsubsection-6}

        This reading also has a brief introduction to the idea of a set of
        vectors being \terminology{linearly depedent} or \terminology{linearly independent}.
        Strang is coy about the precise definition, so here it is:
      %
\par

        A set of vectors \(v_1, v_2, \dots, v_n\) is called \terminology{linearly
        depdendent} when there is some choice of numbers
        \(a_1, a_2, \dots, a_n\) which are not all zero so that the
        linear combination\[
          a_1 v_1 + a_2 v_2 + \dots + a_n v_n = 0
        \]
        A set of vectors which is not linearly dependent is called
        \terminology{linearly independent}.
      %
\par

        This is a little funny the first time you read it. Note that for any
        set of vectors, you can make a linear combination of those vectors
        come out as \(0\). Simply choose all of the coefficients to be
        zero. But that is so easy to do we call it \terminology{trivial}. What the
        definition is asking is that we find a \emph{nontrival linear
        combination of the vectors to make zero}.
      %
\typeout{************************************************}
\typeout{Subsection  Sage and Matrices}
\typeout{************************************************}
\subsection[Sage and Matrices]{Sage and Matrices}\label{subsection-13}

      Sage has many useful commands for working with linear algebra, and
      given the central role played by matrices in this subject, there are
      lots of things Sage can do with matrices. We'll focus here on just basic
      construction and matrix--vector multiplication.
    %
\typeout{************************************************}
\typeout{Subsubsection  The matrix construction command}
\typeout{************************************************}
\subsubsection[The matrix construction command]{The matrix construction command}\label{subsubsection-7}
The command to construct a matrix is pretty straightforward. One types
        \lstinline?matrix(r, c, [list of entries])? where \lstinline?r? is
        the number of rows and \lstinline?c? is the number of columns. The entries
        should be read across the rows starting with the top row.
      %
\begin{lstlisting}[style=sageinput]
A = matrix(2,3, [1,2,3,5,8,13]); A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1  2  3]
[ 5  8 13]
\end{lstlisting}
\par
If you wish, you can structure that list of entries to be a list of lists,
        where each sublist is a row in your matrix.
      %
\begin{lstlisting}[style=sageinput]
B = matrix(2,3, [[1,2,3], [5,8,13]]); B
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1  2   3]
[ 5  8  13]
\end{lstlisting}
\par

        Every once in a while, it might matter to you what kinds of numbers you
        put into the matrix. Sage will let you specify them by putting in an optional
        argument like this: \lstinline?matrix(number type, r, c, [list of entries])?
      %
\begin{lstlisting}[style=sageinput]
C = matrix(ZZ, 2, 2, [2,1,1,1])
C # the best matrix
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[2 1]
[1 1]
\end{lstlisting}
\par

        The notation \lstinline?ZZ? means "integers." There are other sets of numbers
        here:
        \begin{itemize}[label=\textbullet]
\item{}\lstinline?QQ? the rational numbers (with exact arithmetic)\item{}\lstinline?RR? the real numbers (with computer precision arithmetic)\item{}\lstinline?CC? the complex numbers\item{}\lstinline?AA? the set of all algebraic numbers, that is, all of the
            numbers that are roots of some polynomial with integer coefficients
          \end{itemize}

        You can find out what kind of entries a matrix thinks it has by calling the
        \lstinline?.parent()? method on it.
      %
\begin{lstlisting}[style=sageinput]
A.parent()
# this should say something about the integers
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Full MatrixSpace of 2 by 3 dense matrices over Integer Ring
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
D = matrix(QQ, 3,3, [[1,0,1],[2/3, 1, 0],[0,0,9/5]])
# this should say something about the rationals
D.parent()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[  1   0   1]
[2/3   1   0]
[  0   0 9/5]
Full MatrixSpace of 3 by 3 dense matrices over Rational Field
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsubsection  Building a matrix from rows or columns}
\typeout{************************************************}
\subsubsection[Building a matrix from rows or columns]{Building a matrix from rows or columns}\label{subsubsection-8}

        It is possible to build a matrix by bundling together a bunch of vectors,
        too. Let's start with an example made using rows.
      %
\begin{lstlisting}[style=sageinput]
v1 = vector([2,1]); v2= vector([3,4])
# construct E with rows v1 and v2, then display
E = matrix([ v1, v2]); E
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[2 1]
[3 4]
\end{lstlisting}
\par

        Sage prefers rows. I wish it were the other way, but I am sure there is
        a good reason it prefers rows. If you want to make a matrix whose columns
        are the vectors \lstinline?v1? and \lstinline?v2?, you can use the
        \lstinline?transpose? method. We'll talk more about the operation of transpose
        later, but it basically "switches rows for columns and vice versa."
      %
\begin{lstlisting}[style=sageinput]
F = matrix([v1,v2]).transpose(); F
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[2 3]
[1 4]
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsubsection  Matrix action on vectors}
\typeout{************************************************}
\subsubsection[Matrix action on vectors]{Matrix action on vectors}\label{subsubsection-9}

        Of course, Sage knows how to perform the action of a matrix on a vector.
      %
\begin{lstlisting}[style=sageinput]
C; v1
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[2 1]
[1 1]
(2, 1)
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
C*v1
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(5, 3)
\end{lstlisting}
\par
And if you get the sizes wrong, it will return an error.%
\begin{lstlisting}[style=sageinput]
A; v1
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1  2  3]
[ 5  8 13]
(2, 1)
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A*v1
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Error in lines 1-1
...
TypeError: unsupported operand parent(s) for '*': 'Full MatrixSpace of 2 by 3 dense matrices over Integer Ring' and 'Ambient free module of rank 2 over the principal ideal domain Integer Ring'
\end{lstlisting}
\par
If you really need it, Sage can tell you about inverses.%
\begin{lstlisting}[style=sageinput]
A.is_invertible()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
False
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
C.is_invertible()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
C.inverse()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1 -1]
[-1  2]
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-14}
\begin{task}
\label{task-23}

          Make an example of a matrix \(\left(\begin{smallmatrix} 1 &
          \bullet \\ -1 & \bullet \end{smallmatrix}\right)\) so that the
          equation\[
          \begin{pmatrix} 1 & \bullet \\ -1 & \bullet \end{pmatrix}
          \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 1 \\ -1
          \end{pmatrix}
          \]
          has exactly one solution, or explain why this is not possible.
        %
\par

          Interpret this as a statement about \(2\)-vectors and draw the
          picture which corresponds.
        %
\end{task}
\begin{task}
\label{task-24}

          Make an example of a matrix \(\left(\begin{smallmatrix} 4 &
          8 & \bullet \\ 3 & 6 & \bullet \\ 1 & 2 &
          \bullet \end{smallmatrix}\right)\) so that the equation\[
            \begin{pmatrix} 4 & 8 & \bullet \\ 3 & 6 & \bullet \\ 1 & 2 & \bullet \end{pmatrix} \begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} 8 \\ 6 \\ 2 \end{pmatrix}
          \]
          has exactly one solution, or explain why this is not possible.
        %
\par

          Interpret this as a statment about \(3\)-vectors and draw the
          picture which corresponds.
        %
\end{task}
\begin{task}
\label{task-25}

          Make an example of a matrix \(\left( \begin{smallmatrix} 2 & -1 \\
          \bullet & \bullet \end{smallmatrix}\right)\) so that the equation\[
            \begin{pmatrix} 2 & -1 \\ \bullet & \bullet \end{pmatrix}
            \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 7 \\ 3
            \end{pmatrix}
          \]
          has exactly one solution, or explain why this is not possible.
        %
\par

          Interpret this as a statement about a pair of lines in the plane and
          draw the picture which corresponds.
        %
\end{task}
\begin{task}
\label{task-26}

          Make an example of a matrix \(\left( \begin{smallmatrix} 1 & 0
          & 1\\ 1 & 1 & 3 \\ \bullet & \bullet & \bullet
          \end{smallmatrix}\right)\) so that the equation\[
            \begin{pmatrix} 1 & 0 & 1\\ 1 & 1 & 3 \\ \bullet
            & \bullet & \bullet \end{pmatrix}\begin{pmatrix} x \\ y \\ z
            \end{pmatrix} = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}
          \]
          has no solutions, or explain why this is not possible.
        %
\par

          Interpret this as a statement about a planes in space and
          draw the picture which corresponds.
        %
\end{task}
\begin{task}
\label{task-27}

          Find a triple of numbers \(x\), \(y\), and \(z\) so that the
          linear combination\[
            x \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} +
            y \begin{pmatrix} 4 \\ 5\\ 6 \end{pmatrix} +
            z \begin{pmatrix} 7 \\ 8 \\ 9 \end{pmatrix}
          \]
          yields the zero vector, or explain why this is not possible.
        %
\par

          Rewrite the above as an equation which involves a matrix.
        %
\par

          Plot the three vectors and describe the geometry of the situation.
        %
\end{task}
\begin{task}
\label{task-28}

          The vectors\[
            r_1 = \begin{pmatrix} 1 \\ 4 \\ 7 \end{pmatrix}, \qquad
            r_2 = \begin{pmatrix} 2 \\ 5 \\ 8 \end{pmatrix}, \quad \text{ and } \quad
            r_3 = \begin{pmatrix} 3 \\ 6 \\ 9 \end{pmatrix}
          \]
          are linearly dependent because they lie in a common plane (through
          the origin). Find a normal vector to this plane.
        %
\par

          Since the vectors are linearly dependent, there must be (infinitely)
          many choices of scalars \(x\), \(y\), and \(z\) so that
          \(x r_1 + y r_2 + z r_3 = 0\). Find two sets of such numbers.
        %
\end{task}
\begin{task}
\label{task-29}

          Consider the equation\[
            \begin{pmatrix} 2 & 1 \\ 1 & 1 \end{pmatrix}
            \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} b_1 \\ b_2
            \end{pmatrix}.
          \]
          We are interested in being able to solve this for \(x\) and \(y\)
          for any given choice of the numbers \(b_1\) and \(b_2\).
          Figure out a way to do this by writing \(x\) and \(y\) in terms
          of \(b_1\) and \(b_2\).
        %
\par

          Rewrite your solution in the form\[
            \begin{pmatrix} x \\ y \end{pmatrix} = b_1 \begin{pmatrix}
            \bullet \\ \bullet\end{pmatrix} + b_2 \begin{pmatrix}  \bullet
            \\ \bullet \end{pmatrix}.
          \]
        %
\par

          How is this related to the inverse of the matrix
          \(A = \left( \begin{smallmatrix} 2 & 1 \\ 1 & 1
          \end{smallmatrix} \right)\)?
        %
\end{task}
\begin{task}
\label{task-30}

          Find an example of a number \(c\) and a vector \(\left(
          \begin{smallmatrix} b_1 \\ b_2 \end{smallmatrix}\right)\)
          so that the equation\[
            \begin{pmatrix} 3 & 51 \\ c & 17 \end{pmatrix}
            \begin{pmatrix} x \\ y \end{pmatrix} =
            \begin{pmatrix} b_1 \\ b_2 \end{pmatrix}
          \]
          does not have a solution, or explain why no such example exists.
        %
\par

          Explain your solution in terms of
          \begin{itemize}[label=\textbullet]
\item{}lines in the plane,\item{}\(2\)-vectors and linear combinations, and\item{}invertibility of a matrix.\end{itemize}

        %
\end{task}
\typeout{************************************************}
\typeout{Section 1.4 Getting Started with Sage}
\typeout{************************************************}
\section[Getting Started with Sage]{Getting Started with Sage}\label{start-sage}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-15}
It it is time to get comfortable with the basics of using Sage. Begin by
      setting up an account with SageMathCloud.%
\begin{itemize}[label=\textbullet]
\item{}Be sure that you have login access to computers in student labs across
          campus. Before class, drop by a campus lab and make sure your credentials
          work properly and you can sign in to a machine. For our tutorial session
          we will use one of the labs on the first floor of Wright Hall, so one
          of those would make a good choice.%

        \item{}Go to the course web site and follow the links to the Sage Intro
          workshop.%

        \item{}Complete the first two steps of the workshop set up process:
          \begin{enumerate}
\item{}Start: where you make an account at SageMathCloud.\item{}Get: where you create your first "project" and populate it with
              files for a tutorial.\end{enumerate}
%

        \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Objectives}
\typeout{************************************************}
\subsection[Learning Objectives]{Learning Objectives}\label{subsection-16}
At the end of this assignment, a student should have an account at
        SageMathCloud and should be able to log into the service without trouble.
        The student will also have a first project with some files.%
\typeout{************************************************}
\typeout{Section 1.5 Going Further with the Basic Objects}
\typeout{************************************************}
\section[Going Further with the Basic Objects]{Going Further with the Basic Objects}\label{basic-objects-going-further}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-17}
\begin{itemize}[label=\textbullet]
\item{}
        Go back through the exercises in this chapter. Complete any items you
        did not complete the first time through. Prepare any that we have not
        discussed in class so that you will be ready to present them.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion}
\typeout{************************************************}
\subsection[Discussion]{Discussion}\label{subsection-18}
Now we take a short break to revisit and consolidate the learning you
      have done so far. Revisit the reading and the exercises you have done in
      Chapter One: Basic Objects. The important feature of this work should be
      learning to think about your own thinking. This sort of \terminology{meta-cognition}
      characterizes expert learners. Eventually, you want to be able to monitor
      your work at all times and recognize when you understand deeply and when
      you do not. This will allow you to self-correct.
    %
\par
To help you get started with meta-cognition, I listed learning goals in
      each section. To go further, you need to explicitly go through the process
      of reviewing what you can do and what you cannot. Here are some prompts to
      help you get started with this process.
      \begin{itemize}[label=\textbullet]
\item{}
          Review the learning goals from each section. Can you do the things
          described? Can you do them sometimes, or have you mastered them so you
          can do them consistently?
        \item{}
          Look through all of the tasks and go deeper into them. Can you
          connect each exercise to one of our pictures? Try to build a mental
          model of how the exercise and its solution work.
        \item{}
          If your first solution to an exercise involve a ``guess-and-check''
          approach, can you now complete the exercise in a \emph{purposeful}
          and systematic manner?
        \item{}
          Make a list of concepts or exercises that are not clear to you. Phrase
          each item in your list as a question, and make each question as
          specific as possible. Talk with fellow students or your
          instructor until you can answer your own questions.
        \end{itemize}

    %
\typeout{************************************************}
\typeout{Chapter 2 Linear Equations}
\typeout{************************************************}
\chapter[Linear Equations]{Linear Equations}\label{chapter-2}
\typeout{************************************************}
\typeout{Introduction  }
\typeout{************************************************}

      This chapter is dedicated to the first major problem of
      linear algebra: solving systems of linear equations. Restated as a set of
      questions, we will consider these.
    %
\begin{itemize}[label=\textbullet]
\item{}
        What is the set of solutions to a given system of linear equations?
      \item{}
        When does a given system have solution, and when does it not?
      \item{}
        If there is a solution, how many solutions are there?
      \item{}
        What ways do we have of describing the collection of solutions?
      \item{}
        Is there a computationally effective way to find those solutions?
      \end{itemize}
\par

      Though we begin with the first question, we answer the last one first. As
      we explore the process of finding solutions, we will start to build the
      tools we need to finish answering the other questions in a later chapter.
      In this chapter, we aim to get as complete understanding as we can for at
      least a special case: \terminology{square systems}.
    %
\par

      We will begin with a study of the two ways that vectors let us make pictures
      of systems of linear equations. Then we take up a basic process for finding
      solutions, where matrices will appear as a convenient notational device.
      But as we dig a little further, matrices will become interesting in their
      own way, so we will study those. But what we study will relate back to
      the fundamental issue of solving systems of linear equations.
    %
\par

      This chapter has two short review sections in it. One just after we learn
      about elimination, and another at the end of the chapter.
    %
\typeout{************************************************}
\typeout{Section 2.1 Three Geometric Models}
\typeout{************************************************}
\section[Three Geometric Models]{Three Geometric Models}\label{three-pictures}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-19}
\begin{itemize}[label=\textbullet]
\item{}
        Read section 2.1 of \emph{Strang} (pages 31-40).
      \item{}
        Read the following and complete the exercises below
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-20}

      Before class, a student should be able to:
    %
\begin{itemize}[label=\textbullet]
\item{}Translate back and forth between the three algebraic representations:
        \begin{itemize}[label=$\circ$]
\item{}
            A system of linear equations.
          \item{}
            An equation involving a linear combination of vectors.
          \item{}
            A matrix equation.
          \end{itemize}

      \item{}
        Can write down the \(n \times n\) \terminology{identity matrix}.
      \end{itemize}
\par

      Sometime in the near future, a student should be able to:
    %
\begin{itemize}[label=\textbullet]
\item{}Given a system, interpret and plot the ``row picture''.\item{}Given a system, interpret and plot the ``column picture''.\item{}Use a matrix as a model of a \terminology{transformation}, including stating
        the \terminology{domain} and the \terminology{range}.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion}
\typeout{************************************************}
\subsection[Discussion]{Discussion}\label{subsection-21}

      Now we have a little experience with vectors and related things, it is
      time to be aware of what we have done so we can use it as a foundation for
      future work. So far, we have talked about two geometric interpretations
      for a system of linear equations, the \terminology{row picture} and the
      \terminology{column picture}.
    %
\par

      Does the following picture make sense to you?
    %
\begin{figure}
\centering
\includegraphics[width=350pt,]{images/ThreePics.png}\caption{The three geometric models of linear algebra\label{figure-1}}
\end{figure}
\par

      A deep understanding of linear algebra will involve a level of comfort with
      each of these three views of the subject in the diagram, and also the ability
      to pass back and forth between them.
    %
\typeout{************************************************}
\typeout{Subsubsection  The Transformational View}
\typeout{************************************************}
\subsubsection[The Transformational View]{The Transformational View}\label{subsubsection-10}

        We have seen that matrices can be made to "act upon" vectors by a kind of
        multiplication. In particular, if \(A\) is an \(m\times n\) matrix,
        then \(A\) can be multiplied (on the left) with a column vector of size
        \(n\), and the result is a column vector of size \(m\).
      %
\par

        This makes \(A\) into a kind of \terminology{function}. (We will use
        the synonyms \terminology{mapping} or \terminology{transformation}, too.)
        For every vector \(v\) of size \(n\), the matrix \(A\) allows
        us to compute a new vector \(T_A(v) = Av\) of size \(m\). This is
        the basic example of what we will eventually call a \terminology{linear
        transformation}.\begin{align*}
\mathbb{R}^n & \xrightarrow[]{T_A} \mathbb{R}^m\\
v & \xmapsto[]{\phantom{T_A}} Av.
\end{align*}
        One of our long term goals is to find a way to think about the geometry
        of linear algebra from this viewpoint, too.
      %
\typeout{************************************************}
\typeout{Subsection  Sage and Plotting for Linear Algebra}
\typeout{************************************************}
\subsection[Sage and Plotting for Linear Algebra]{Sage and Plotting for Linear Algebra}\label{subsection-22}
There are a few new Sage commands that might be useful here. We have
      already seen how to take linear equations and turn them into vectors
      and then turn the vector equation into a matrix equation. But Sage can
      help us move in the other direction, too.
    %
\par

      The keys are commands to pull out the rows and columns from a given matrix.
      Let's start with a simple situation where the matrix equation is
      \[
        \begin{pmatrix} 2 & 1 \\ 1 & 1 \end{pmatrix}
        \begin{pmatrix} x & y \end{pmatrix} =
        \begin{pmatrix} 3 & 4 \end{pmatrix}.
      \]
    %
\begin{lstlisting}[style=sageinput]
A = matrix(QQ, 2,2, [2,1,1,1]); A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[2 1]
[1 1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
x, y = var('x y')
X = vector([x,y]); X
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(x, y)
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
b = vector([3,4]); b
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(3, 4)
\end{lstlisting}
\par

      To get Sage to pull out the columns, we can use the \lstinline?.columns()?
      method. If we want just one column, we can use the \lstinline?.column()? method,
      but then we have to remember to specify which column we want.
    %
\begin{lstlisting}[style=sageinput]
A.columns() # this will return a list
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[(2, 1), (1, )]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.column(1)
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(1, 1)
\end{lstlisting}
\par

      \emph{Big important note:} Sage always numbers lists starting with zero.
      so the first element of every list is the \(0\) entry, and the second
      element is the \(1\) entry.
    %
\par

      Now it is possible to make Sage do things like this:
    %
\begin{lstlisting}[style=sageinput]
column_plot = plot(A.column(0), color='red')
column_plot+= plot(A.column(1), color='blue')
column_plot+= plot(b, color='purple')
show(column_plot, figsize=5, aspect_ratio=1)
\end{lstlisting}
\par

      This is an example of the a column picture.
    %
\par

      One can also pull out the rows with corresponding row methods. And if you
      recall the way that matrix multiplication works if you think of rows, you
      can make a row picture.
    %
\begin{lstlisting}[style=sageinput]
A.rows()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[(2, 1), (1, 1)]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
expr1 = A.row(0).dot_product(X) == b[0]
expr2 = A.row(1).dot_product(X) == b[1]

print expr1
print expr2
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
2*x + y == 3
x + y == 4
\end{lstlisting}
\par

      And now the picture:
    %
\begin{lstlisting}[style=sageinput]
row_plot = implicit_plot(expr1, [x,-5,5], [y,-1,9], color='blue')
row_plot+= implicit_plot(expr2, [x,-5,5], [y,-1,9], color='red')
show(row_plot, axes=True)
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-23}
\begin{task}
\label{task-31}

        Make an example of a system of linear equations which some students might
        find challenging to change into an equation involving a linear combination.
        Explain what the challenge is and how you can think clearly to overcome it.
      \end{task}
\begin{task}
\label{task-32}

        Make an example of a linear combination equation which some students might
        find challenging to change into a system of linear equations. Explain
        what the challenge is and how you can think clearly to overcome it.
      \end{task}
\begin{task}
\label{task-33}

        Consider the matrix equation\[
        \begin{pmatrix}1 & 2 & 4 \\ 2 & 0 & 1 \end{pmatrix}
          \begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} -1 \\ 3
          \end{pmatrix}.
        \]\begin{enumerate}
\item{}
            Draw a diagram representing the row picture of this matrix equation.
          \item{}
            Draw a diagram representing the column picture of this matrix equation.
          \end{enumerate}
\end{task}
\begin{task}
\label{task-34}

        Make an example of a system of linear equations so that the
        corresponding column picture is about linear combinations of four 2-vecs
        becoming the zero vector.
      \end{task}
\begin{task}
\label{task-35}

        Find a linear combination equation so that the corresponding system of
        linear equations corresponds to finding the intersection of three lines
        in the plane.
      \end{task}
\begin{task}
\label{task-36}

        Find an example of a vector \(b\) so that the equation\[
          \begin{pmatrix} -1 & 2 \\ 5 & -9 \end{pmatrix} v = b
        \]
        has no solution \(v\), or explain why it is impossible to find
        such an example.
      \end{task}
\begin{task}
\label{task-37}

        In each of the below, find an example of a matrix \(B\) which has the
        described effect.
        \begin{enumerate}
\item{}
            \(
              B\begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix}y \\ x \end{pmatrix}
            \)
          \item{}
            Rotates vectors through \(45^{\circ}\) counter-clockwise.
          \item{}
            Reflects vectors across the \(y\)axis.
          \item{}
            \(
              B\begin{pmatrix} x \\ y \end{pmatrix} =
              \begin{pmatrix} x+y \\ y \end{pmatrix}.
            \)
          \end{enumerate}
\end{task}
\typeout{************************************************}
\typeout{Section 2.2 Solving Systems}
\typeout{************************************************}
\section[Solving Systems]{Solving Systems}\label{elimination}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-24}
\begin{itemize}[label=\textbullet]
\item{}Read section 2.2 of Strang (pages 45-51).\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-25}

      Before class, a student should be able to do the following things.
    %
\begin{itemize}[label=\textbullet]
\item{}Clearly state and use the following vocabulary words: pivot, multiplier,
        triangular matrix, back substitution, singular, non-singular
      \end{itemize}
\par

      Sometime after class, a student should be able to the following things.
    %
\begin{itemize}[label=\textbullet]
\item{}Perform elimination to put a system of linear equations into triangular form.\item{}Solve small systems by hand.\item{}Explain the two failure modes for elimination, and describe which leads to
        no solutions, and which leads to infinitely many solutions.\item{}Solve larger systems with the help of a computer algebra package (Sage).\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion: Elimination for Solving Systems of Linear Equations}
\typeout{************************************************}
\subsection[Discussion: Elimination for Solving Systems of Linear Equations]{Discussion: Elimination for Solving Systems of Linear Equations}\label{subsection-26}

        Now we begin the process of learning how to solve a system of linear
        equations systematically through a process called \terminology{elimination}.
      %
\typeout{************************************************}
\typeout{Subsubsection  Some terminology}
\typeout{************************************************}
\subsubsection[Some terminology]{Some terminology}\label{subsubsection-11}
A typical system looks something like this:
          \[
            \left\{ \begin{array}{rrrrrrrr}
                  3 x_1 & +&  2 x_2 & - & \pi x_3 & = & 0 \\
                  -4 x_1 & -& 33 x_2 & + & x_3 & = & 12
                  \end{array}\right.
          \]
          This situation is \emph{two} equations in \emph{three} unknowns.
          The unknowns here are the three numbers \(x_1\), \(x_2\) and
          \(x_3\) for which we search. Usually, we bundle the numbers
          together as a vector \((x_1, x_2, x_3)\). If we can find a vector
          which makes all of the equations true simultaneously, we call that
          vector a \terminology{solution}.
        %
\par

          Keep in mind that the process involves eliminating instances of the
          variable below \terminology{pivots}. Strang describes the process
          pretty well, and gives good examples. What Strang describes in this
          section is sometimes called \terminology{the forward pass} elimination.
        %
\par

          Watch out for situations which are \terminology{singular} in that they
          have fewer pivots than unknowns. A system is called
          \terminology{non-singular} if it has as many pivots as unknowns.
        %
\typeout{************************************************}
\typeout{Subsubsection  Keeping track of things}
\typeout{************************************************}
\subsubsection[Keeping track of things]{Keeping track of things}\label{subsubsection-12}

          Playing with all of the equations is nice, but all that really
          matters is the collection of coefficients, and the numbers on the
          right hand sides of the equal signs. Experienced solvers get tired
          of copying notation from line to line in a computation, so they only
          keep track of the matrix of coefficients, \terminology{augmented} by
          the vector on the right-hand side. In the example above, that
          augmented matrix is
          \[
            \augmatrix{ccc}{
            3 & 2 & -\pi & 0 \\ -4 & -33 & 1 & 12
            }
          \]
          All of the row operations can be performed on just this augmented
          matrix, without losing any of the essential information.
        %
\typeout{************************************************}
\typeout{Subsection  Sage and Row Operations}
\typeout{************************************************}
\subsection[Sage and Row Operations]{Sage and Row Operations}\label{subsection-27}

        The process of elimination for systems of equations involves performing
        operations on the equations. When translated to matrix form, it involves
        operations on the rows of the coefficient matrix. The corresponding
        matrix methods come in two types.
      %
\par

        The first type of method modifies the matrix ``in place'', which means
        that it \emph{Changes the input matrix}.
      %
\begin{itemize}[label=\textbullet]
\item{}\lstinline?A.rescale_row(r, num)? multiplies row \lstinline?r? by the factor of \lstinline?num?.\item{}\lstinline?A.swap_rows(r1, r2)? switches the places of rows \lstinline?r1? and \lstinline?r2?.\item{}\lstinline?A.add_multiple_of_row(target, useful, num)?. This adds \lstinline?num?
          times row \lstinline?useful? to row \lstinline?target?.\end{itemize}
\par

        Throughout, please remember that Sage uses \(0\)-based indexing! So
        the rows are labeled \lstinline?0, 1, 2, ...?
      %
\begin{lstlisting}[style=sageinput]
A = matrix(QQ, 3,3, [0,2,4, 1,1,5, 6,2,5]); A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[0 2 4]
[1 1 5]
[6 2 5]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.swap_rows(0,1); A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 1 5]
[0 2 4]
[6 2 5]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.add_multiple_of_row(2,0,-6)
A # this should add -6 times row 0 to row 2
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[  1   1   5]
[  0   2   4]
[  0  -4 -25]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.rescale_row(1,1/2); A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[  1   1   5]
[  0   1   2]
[  0  -4 -25]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.add_multiple_of_row(2,1,4)
A # this should add 4 times row 2 to row 2
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[  1   1   5]
[  0   1   2]
[  0   0 -17]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.rescale_row(2,-1/17); A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 1 5]
[0 1 2]
[0 0 1]
\end{lstlisting}
\par

        This just did the whole process of \terminology{forward pass elimination}.
        (Well, we did a bit more than Strang would. He wouldn't rescale the rows.)
      %
\par

        Sometimes you do not want to change the matrix \lstinline?A?. If instead, you
        want to leave \lstinline?A? alone, you can use these methods, which return
        a new object and do not change \lstinline?A?.
      %
\begin{itemize}[label=\textbullet]
\item{}\lstinline?A.with_rescaled_row(r, num)?\item{}\lstinline?A.with_swapped_rows(r1, r2)?\item{}\lstinline?A.with_added_multiple_of_row(t, u, num)?\end{itemize}
\par

        Let's do the same operations as above, but without changing \lstinline?A?.
        This will mean making a bunch of new matrices. In fact, let's also change
        the name of our matrix to \lstinline?B?
      %
\begin{lstlisting}[style=sageinput]
B = matrix(QQ, 3,3, [0,2,4, 1,1,5, 6,2,5]); B
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[0 2 4]
[1 1 5]
[6 2 5]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
B1 = B.with_swapped_rows(0,1); B1
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 1 5]
[0 2 4]
[6 2 5]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
B2 = B1.with_added_multiple_of_row(2,0,-6)
B2 # this should add -6 times row 0 to row 2
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[  1   1   5]
[  0   2   4]
[  0  -4 -25]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
B3 = B2.with_rescaled_row(1,1/2); B3
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[  1   1   5]
[  0   1   2]
[  0  -4 -25]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
B4 = B3.with_added_multiple_of_row(2,1,4)
B4 # this should add 4 times row 2 to row 2
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[  1   1   5]
[  0   1   2]
[  0   0 -17]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
B5 = B4.with_rescaled_row(2,-1/17); B5
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 1 5]
[0 1 2]
[0 0 1]
\end{lstlisting}
\par

        This second option has some advantages. At any point, you can revise your
        work, because the original matrix is still in memory, and so are all of
        the intermediate steps. Let's display all six of the matrices at once to
        see that they all still exist.
      %
\begin{lstlisting}[style=sageinput]
B, B1, B2, B3, B4, B5
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-28}
\begin{task}
\label{task-38}

          Use the elimination method to transform this system into an easier
          one. (Can you make it triangular?) Circle the pivots in the final
          result.
          \[
            \left\{
            \begin{array}{rrrrrrr}
            2x & + & 3y & + &  z & = & 8\\
            4x & + & 7y & + & 5z & = & 20 \\
               & - & 2y & + & 2z & = & 0
            \end{array}\right.
          \]
          What two operations do you use to do this efficiently? Now use back
          substitution to solve the system.
        \end{task}
\begin{task}
\label{task-39}

            \emph{(Sage Exercise)}:
            Because the last system can be transformed in two operations, there
            are three equivalent systems generated through the process
            (the original, the intermediate, and the final).
          %
\par

            Make row picture plots for each of the three systems. [Hint: Sage]
            How do the operations transform the pictures?
          %
\end{task}
\begin{task}
\label{task-40}

          Suppose that a system of three equations in three unknowns has two
          solutions \((a,b,c)\) and \((A,B,C)\). Explain why the system must have
          other solutions than these two. Describe clearly two other solutions
          in terms of \(a,b,c,A,B,C\).
        \end{task}
\begin{task}
\label{task-41}

          Find three examples of numbers \(a\) so that elimination will fail to
          give three pivots for this coefficient matrix:
          \[
            A = \begin{pmatrix}
            a & 2 & 3 \\ a & a & 4 \\ a & a & a
            \end{pmatrix}
          \]\end{task}
\begin{task}
\label{task-42}

          How many ways can two lines in the plane meet? Make examples to
          represent as many \emph{qualitatively different} situations as you can.
        \end{task}
\begin{task}
\label{task-43}

          Complete the following to make an example of a system of two equations
          in two unknowns which is singular but still has a solution, or explain why
          no such example exists.
          \[
            \left\{
            \begin{array}{ccccc}
            2x & + & 3y & = & 1 \\
            \bullet x & + & \bullet y & = & \bullet
            \end{array}\right.
          \]\end{task}
\begin{task}
\label{task-44}

          Complete the following to a system of three equations in three
          unknowns which is singular and does not have a solution, or explain
          why no such example exists.
          \[
            \left\{
            \begin{array}{ccccccc}
               &  & 3y & - & z & = & 1 \\
            2x & - & y & + & 3z & = & 0 \\
            \bullet x & + & \bullet y & + &\bullet z &  = & \bullet
            \end{array}\right.
          \]\end{task}
\begin{task}
\label{task-45}

          Complete the following to a system of three equations in three
          unknowns which is singular but still has a solution, or explain why
          no such example exists.
          \[
            \left\{
            \begin{array}{ccccccc}
            x & + & y & + & z & = & 1 \\
            2x & + & y & + & 2z & = & 0 \\
            \bullet x & + & \bullet y & + &\bullet z &  = & \bullet
            \end{array}\right.
          \]\end{task}
\begin{task}
\label{task-46}

          How many ways can three planes in three dimensional space meet? Make
          examples to represent as many \emph{qualitatively different}
          situations as you can.
        \end{task}
\typeout{************************************************}
\typeout{Section 2.3 Elimination using Matrices}
\typeout{************************************************}
\section[Elimination using Matrices]{Elimination using Matrices}\label{matrix-elimination}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-29}
\begin{itemize}[label=\textbullet]
\item{}Read section 2.3 of \emph{Strang}.\item{}Read the discussion below and work out the exercises.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-30}

        Before class, a student should be able to:
      %
\begin{itemize}[label=\textbullet]
\item{}
          Translate a system of linear equations into the form of an augmented matrix
          and back.
        \item{}
          Perform the forward pass elimination process to an augmented matrix.
        \item{}
          Multiply a pair of square matrices having the same size.
        \item{}
          Identify the matrix which performs the operation ``add a multiple of row
          i to row j.''
        \item{}Identify the matrix which performs the operation ``swap the places of
          row i and row j.''
        \end{itemize}
\par

        Some time after class, a student should be able to:
      %
\begin{itemize}[label=\textbullet]
\item{}
          Use the steps from a forward pass elimination step to write a correct
          equation of the form
          \[
            E_{\bullet}E_{\bullet}\cdots E_{\bullet} (A\ |\ b) = (U\ |\ b')
          \]
          where \(U\) is an upper triangular matrix.
        \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion: Elimnation and Using Matrices as ``Transformations''}
\typeout{************************************************}
\subsection[Discussion: Elimnation and Using Matrices as ``Transformations'']{Discussion: Elimnation and Using Matrices as ``Transformations''}\label{subsection-31}

      Let us focus (for now) on square systems of equations, where the number of
      unknowns is equal to the number of equations.
    %
\typeout{************************************************}
\typeout{Subsubsection  The Four Ways to Write a System}
\typeout{************************************************}
\subsubsection[The Four Ways to Write a System]{The Four Ways to Write a System}\label{subsubsection-13}

        Recall that there are three equivalent ways to write the typical linear
        algebra problem: (1) a system of linear equations to be solved
        simultaneously, (2) an equation expressing some linear combination of
        vectors with unknown coefficients as equal to another vector, and (3) a
        matrix equation.
      %
\par

        Here is an example: This system of linear equations
        \[
          \left\{ \begin{array}{rrrrrrr}
              &  &3y &+ &2z &= &8 \\
            x &- & y &+ & z &= &-1 \\
          3 x &+ &2y &+ &3z &= &1
          \end{array}\right.
        \]
        is equivalent to this equation using a linear combination of vectors
        \[
        x \begin{pmatrix} 0 \\ 1 \\ 3 \end{pmatrix} +
        y \begin{pmatrix} 3 \\ -1 \\ 2 \end{pmatrix} +
        z \begin{pmatrix} 2 \\ 1 \\ 3 \end{pmatrix} =
        \begin{pmatrix} 8 \\ -1 \\ 1 \end{pmatrix}
        \]
        and both of those are equivalent to this matrix equation
        \[
          \begin{pmatrix} 0 &3 &2 \\ 1 &-1 &1 \\ 3 &2 &3
          \end{pmatrix} \begin{pmatrix} x \\ y \\ z \end{pmatrix} =
          \begin{pmatrix} 8 \\ -1 \\ 1 \end{pmatrix} .
        \]
      %
\par

        Each of these viewpoints has its advantages when talking about the
        geometry of linear algebra. But one way that things steadily improve as
        you move down the page is in the amount of notational baggage.
        From each version to the next, we lose repetitive bits of symbol that
        one can just remember from context.
        Often, this is taken one step further! We now throw away the unknowns,
        the equal signs and some of the parentheses surrounding the matrices and
        vectors, and just write an \emph{augmented} matrix.
        \[
          \augmatrix{ccc}{
          0 &3 &2 &8\\ 1 &-1 &1 &-1\\ 3 &2 &3 &1
          }
        \]
        This is the minimum fuss way to keep track of all the information you
        need to solve the original system.
      %
\typeout{************************************************}
\typeout{Subsubsection  Representing Elimination with Matrices}
\typeout{************************************************}
\subsubsection[Representing Elimination with Matrices]{Representing Elimination with Matrices}\label{subsubsection-14}

        The process of elimination starts by performing operations on the
        system of equations. In the example above, one simplification we can
        make is to add \(-3\) times equation (ii) to equation (iii).
      %
\par

        Then the new system looks like this:
        \[
          \left\{ \begin{array}{rrrrrrr}
          &  &3y &+ &2z &= &8 \\
          x &- & y &+ & z &= &-1 \\
          &  &5y &  &   &= &4
          \end{array}\right.
        \]
        Let's translate that into the linear combination format:
        \[
          x \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix} +
          y \begin{pmatrix} 3 \\ -1 \\ 5 \end{pmatrix} +
          z \begin{pmatrix} 2 \\ 1 \\ 0 \end{pmatrix} =
          \begin{pmatrix} 8 \\ -1 \\ 4 \end{pmatrix} .
        \]
        What has happened to each of the vectors? Well, in each case, we have
        added \(-3\) times the second component to the third component.
        We have seen that one way to change vectors into other vectors is by
        (left-)multiplying them by matrices. Could we be so lucky that the
        operation ``add \(-3\) times the second component to the third
        component'' is representable by a matrix operation? YES. It is not
        hard to check that the matrix we need is
        \[
          E = \begin{pmatrix} 1 &0 &0 \\ 0 &1 &0 \\ 0 &-3 &1 \end{pmatrix} .
        \]
        In fact, let's check it now for each of the four vectors we have in our system:
        \[
          E \begin{pmatrix} 0 \\ 1 \\ 3 \end{pmatrix} =
          \begin{pmatrix} 1 &0 &0 \\ 0 &1 &0 \\
          0 &-3 &1 \end{pmatrix} \begin{pmatrix} 0 \\ 1 \\ 3 \end{pmatrix}
          = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}
        \]
        and
        \[
          E \begin{pmatrix} 3 \\ -1 \\ 2 \end{pmatrix} =
          \begin{pmatrix} 1 &0 &0 \\ 0 &1 &0 \\ 0 &-3
          &1 \end{pmatrix} \begin{pmatrix} 3 \\ -1 \\ 2 \end{pmatrix} =
          \begin{pmatrix} 3 \\ -1 \\ 5 \end{pmatrix}
        \]
        and
        \[
          E \begin{pmatrix} 2 \\ 1 \\ 3 \end{pmatrix} =
          \begin{pmatrix} 1 &0 &0 \\ 0 &1 &0 \\
          0 &-3 &1 \end{pmatrix} \begin{pmatrix} 2 \\ 1 \\ 3
          \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \\ 0 \end{pmatrix}
        \]
        and
        \[
          E \begin{pmatrix} 8 \\ -1 \\ 1 \end{pmatrix} =
          \begin{pmatrix} 1 &0 &0 \\ 0 &1 &0 \\
          0 &-3 &1 \end{pmatrix} \begin{pmatrix} 8 \\ -1 \\ 1 \end{pmatrix}
          = \begin{pmatrix} 8 \\ -1 \\ 4 \end{pmatrix}.
        \]
      %
\par

        Ha Ha! It all checks out. Those are the four vectors from our second
        system. This means that we can even use a simple subsitution to rewrite
        things. (It is not obvious at the moment why this is helpful. Hang on a
        bit.)
        \[
          x* E\begin{pmatrix} 0 \\ 1 \\ 3 \end{pmatrix} +
          y* E\begin{pmatrix} 3 \\ -1 \\ 2 \end{pmatrix} +
          z* E\begin{pmatrix} 2 \\ 1 \\ 3 \end{pmatrix} =
          E\begin{pmatrix} 8 \\ -1 \\ 1 \end{pmatrix}
        \]
        Boy, it sure would be nice if we had a compact way to write that down...
      %
\typeout{************************************************}
\typeout{Subsubsection  Matrix Multiplication}
\typeout{************************************************}
\subsubsection[Matrix Multiplication]{Matrix Multiplication}\label{subsubsection-15}

        We make a compact way to write down that last equation by defining an
        operation of multiplying two matrices.
        If \(E\) and \(A\) are two matrices, we define their
        \terminology{matrix product}
        to be a new matrix as follows:
      %
\par

        First, write \(A\) as a collection of columns \(v_i\)
        \[
          A = \begin{pmatrix} v_1 &v_2 &v_3 \end{pmatrix}
        \]
        and then we declare that \(EA\) is the matrix made up of the columns
        \(Ev_i\) in the corresponding order.
        \[
          EA = \begin{pmatrix} Ev_1 &Ev_2 &Ev_3 \end{pmatrix}
        \]
      %
\par

        By way of example, we have already considered the matrices
        \[
          E = \begin{pmatrix} 1 &0 &0 \\ 0 &1 &0 \\ 0 &-3 &1 \end{pmatrix} \quad \text{ and } \quad
          A = \begin{pmatrix} 0 &3 &2 \\ 1 &-1 &1 \\ 3 &2 &3 \end{pmatrix}.
        \]
        You should check that their product is now
        \[
          EA = \begin{pmatrix} 1 &0 &0 \\ 0 &1 &0 \\ 0 &-3 &1 \end{pmatrix}\begin{pmatrix} 0 &3 &2 \\ 1 &-1 &1 \\ 3 &2 &3 \end{pmatrix} =
          \begin{pmatrix} 0 &3 &2 \\ 1 &-1 &1 \\ 0 &5 &0 \end{pmatrix}
        \]
      %
\par

        Finally, let's see how this influences our last two forms of the equations.
        The matrix form of our system was \(Ax = v\) where \(A\) is as above, and the
        vectors are \(v = \left(\begin{smallmatrix} 8 \\ -1 \\ 1
        \end{smallmatrix}\right)\) and
        \(x = \left( \begin{smallmatrix} x \\ y \\ z \end{smallmatrix}\right)\).
        The neat part is that our new definition of matrix multiplication means
        that our elimination step transformed the equation
        \[
          Ax = v \quad \text{ or }\quad
          \begin{pmatrix} 0 &3 &2 \\ 1 &-1 &1 \\ 3 &2 &3
          \end{pmatrix} \begin{pmatrix} x \\ y \\ z \end{pmatrix} =
          \begin{pmatrix} 8 \\ -1 \\ 1 \end{pmatrix}
        \]
        into the newer equation
        \[
          (EA) x = Ev \quad \text{ or } \quad \begin{pmatrix} 0 &3 &2
          \\ 1 &-1 &1 \\ 0 &5 &0 \end{pmatrix} \begin{pmatrix} x
          \\ y \\ z \end{pmatrix} = \begin{pmatrix} 8 \\ -1 \\ 4 \end{pmatrix}.
        \]
      %
\par

        The same thing works for the augmented matrix. The augmented matrix form
        is really just writing down
        \[
          \augmatrix{c}{A &v }
          =\augmatrix{ccc}{
          0 &3 &2 &8\\ 1 &-1 &1 &-1\\ 3 &2 &3 &1 }
        \]
        and the elimination step changes this into
        \[
          \augmatrix{c}{EA &Ev}
          = \augmatrix{ccc}{
          0 &3 &2 &8 \\ 1 &-1 &1 &-1 \\ 0 &5 &0 &4
          }.
        \]
      %
\typeout{************************************************}
\typeout{Subsubsection  Matrices as Transformations}
\typeout{************************************************}
\subsubsection[Matrices as Transformations]{Matrices as Transformations}\label{subsubsection-16}

        Take a moment and reflect on the key transition in what happens above.
        The most important thing that made it all work was that a matrix (the
        elimination matrix \(E\)) was used to perform some sort of operation on
        vectors.
      %
\par

        This is a key property of matrices. The matrix \(E\) defines a kind of
        function. For every vector \(w\) with three components, we can compute
        exactly one new vector \(Ew\), still with three components. This means that
        \(E\) defines a function from the set of \(3\)-vectors to the set of
        \(3\)-vectors.
      %
\typeout{************************************************}
\typeout{Subsection  Sage and Matrix Multiplication}
\typeout{************************************************}
\subsection[Sage and Matrix Multiplication]{Sage and Matrix Multiplication}\label{subsection-32}

      Sage has built-in matrix multiplication. You do the obvious thing and it
      works.
    %
\begin{lstlisting}[style=sageinput]
A = matrix(QQ, 2,2, [[2,1],[1,1]])
B = matrix(QQ, 2,2, [[0,3],[1,1]])
A, B
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(
[2 1]  [0 3]
[1 1], [1 1]
)
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A*B
\end{lstlisting}
\par

      You can check that it works with the way we defined matrix multiplication
      as a linear combination of vectors, too.
    %
\par

      First, we define the column vectors by pulling out the entries from \lstinline?B?
      and arranging them. To be sure, we ask Sage to display them as columns.
    %
\begin{lstlisting}[style=sageinput]
b1 = vector([B[0,0], B[1,0]])
b2 = vector([B[0,1], B[1,1]])
b1.column(), b2.column()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(
[0]  [3]
[1], [1]
)
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A*b1.column(), A*b2.column()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(
[1]  [7]
[1], [4]
)
\end{lstlisting}
\par

      Now we can pile these rows into a matrix and then use the transpose to
      put them in columns.
    %
\begin{lstlisting}[style=sageinput]
C = matrix([A*b1, A*b2]).transpose()
C
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 7]
[1 4]
\end{lstlisting}
\par

      And we can double check everything by asking Sage if these things are equal.
    %
\begin{lstlisting}[style=sageinput]
C == A*B
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\par

      This kind of test can be useful for checking our work! The discussion above
      has this multiplication:
    %
\begin{lstlisting}[style=sageinput]
D = matrix(QQ,3,4, [0,3,2,8,1,-1,1,-1,3,2,3,1]); D
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 0  3  2  8]
[ 1 -1  1 -1]
[ 3  2  3  1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
E = matrix.identity(3)
E[2,1] = -3
E
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1  0  0]
[ 0  1  0]
[ 0 -3  1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
E*D
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 0  3  2  8]
[ 1 -1  1 -1]
[ 0  5  0  4]
\end{lstlisting}
\par

      Ta-Da!!!
    %
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-33}
\begin{task}
\label{task-47}

      In the main example above
      \[
      \left\{ \begin{array}{rrrrrrr}
         &  &3y &+ &2z &= &8 \\
       x &- & y &+ & z &= &-1 \\
      3x &+ &2y &+ &3z &= &1
      \end{array}
      \right.
      \]
      we would rather have our first pivot in the upper left corner (i.e. the
      first row should have a non-zero coefficient for \(x\)). This can be
      achieved by swapping the positions of rows (i) and (ii).
    %
\par

      Find a matrix \(P_{12}\) so that multiplying by \(P_{12}\) on the left
      performs the corresponding row switching operation on the augmented matrix
      \[
      \augmatrix{ccc}{
      0 &3 &2 &8 \\ 1 &-1 &1 &-1 \\ 3 &2 &3 &1}
      \]
    %
\end{task}
\begin{task}
\label{task-48}

      Consider the system
      \[
      \left\{ \begin{array}{rrrrr}
      6x &- &y &= &14 \\
      97x &- &16y &= &2/3 \\
      \end{array}
      \right.
      \]
      Write this system in the other three forms: (1) an equation involving a
      linear combination of vectors; (2) an equation involving a \(2\times 2\)
      matrix; (3) an augmented matrix.
    %
\end{task}
\begin{task}
\label{task-49}

      Perform an elimination step on the system from the last exercise to put the sytem in triangular form. You should get two pivots.
    %
\par

      Write the new system in each of our four forms.
    %
\end{task}
\begin{task}
\label{task-50}

      Still working with the same system of equations, use Sage to make two column picture plots:
    %
\begin{itemize}[label=\textbullet]
\item{} One showing the three relevant column vectors from the original system.\item{} One showing the three relevant column vectors from the system after the elimination step.\end{itemize}
\par

      You may find it helpful to look through the Sage examples in previous sections
      of this workbook.
    %
\end{task}
\begin{task}
\label{task-51}

      One more time, stay with the same system of equations. Use Sage to make
      two row picture plots:
    %
\begin{itemize}[label=\textbullet]
\item{} One showing the two relevant lines in the original system.\item{} One showing the two relevant lines from the system after the elimination step.\end{itemize}
\par

      You may find it helpful to look through the Sage examples in previous sections
      of this workbook.
    %
\end{task}
\begin{task}
\label{task-52}

      Consider this system of three equations in three unknowns:
      \[
      \left\{
      \begin{array}{rrrrrrr}
      -x &+ &\frac{2}{3} y &+ &z &= &1 \\
       x &+ &           6y &+ &z &= &1 \\
      3x &  &              &+ &3z&= &1 \\
      \end{array}\right.
      \]
      Perform the elimination steps to transform this system into a triangular one.
    %
\par

      Write down the corresponding matrices you use to perform each of these
      steps on the augmented matrix version of this system.
    %
\end{task}
\begin{task}
\label{task-53}

      Still working with the system of equations from the last task, use Sage
      to make two column picture plots:
    %
\begin{itemize}[label=\textbullet]
\item{} One showing the four relevant column vectors from the original system.\item{} One showing the four relevant column vectors from the system after the elimination step.\end{itemize}
\par

      You may find it helpful to look through the Sage examples in previous sections
      of this workbook.
    %
\end{task}
\begin{task}
\label{task-54}

      One more time, stay with the system of equations from previous two tasks.
      Use Sage to make two row picture plots:
    %
\begin{itemize}[label=\textbullet]
\item{} One showing the two relevant lines in the original system.\item{} One showing the two relevant lines from the system after the elimination step.\end{itemize}
\par

      You may find it helpful to look through the Sage examples in previous sections
      of this workbook.
    %
\end{task}
\typeout{************************************************}
\typeout{Section 2.4 Going Further with Elimination}
\typeout{************************************************}
\section[Going Further with Elimination]{Going Further with Elimination}\label{lin-eq-going-further-1}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-34}
\begin{itemize}[label=\textbullet]
\item{}
        Go back through the exercises in the first three sections of this chapter.
        Complete any items you
        did not complete the first time through. Prepare any that we have not
        discussed in class so that you will be ready to present them.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion}
\typeout{************************************************}
\subsection[Discussion]{Discussion}\label{subsection-35}
Now we take a short break to revisit and consolidate the learning you
      have done so far. Revisit the reading and the exercises you have done in
      Chapter Two: Linear Equations. The important feature of this work should be
      learning to think about your own thinking. This sort of \terminology{meta-cognition}
      characterizes expert learners. Eventually, you want to be able to monitor
      your work at all times and recognize when you understand deeply and when
      you do not. This will allow you to self-correct.
    %
\par
To help you get started with meta-cognition, I listed learning goals in
      each section. To go further, you need to explicitly go through the process
      of reviewing what you can do and what you cannot. Here are some prompts to
      help you get started with this process.
      \begin{itemize}[label=\textbullet]
\item{}
          Review the learning goals from each section. Can you do the things
          described? Can you do them sometimes, or have you mastered them so you
          can do them consistently?
        \item{}
          Look through all of the tasks and go deeper into them. Can you
          connect each exercise to one of our pictures? Try to build a mental
          model of how the exercise and its solution work.
        \item{}
          If your first solution to an exercise involve a ``guess-and-check''
          approach, can you now complete the exercise in a \emph{purposeful}
          and systematic manner?
        \item{}
          Make a list of concepts or exercises that are not clear to you. Phrase
          each item in your list as a question, and make each question as
          specific as possible. Talk with fellow students or your
          instructor until you can answer your own questions.
        \end{itemize}

    %
\typeout{************************************************}
\typeout{Section 2.5 Matrix Algebra}
\typeout{************************************************}
\section[Matrix Algebra]{Matrix Algebra}\label{matrix-algebra}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-36}
\begin{itemize}[label=\textbullet]
\item{}Read section 2.4 of \emph{Strang}.\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-37}

      Before class, a student should be able to:
    %
\begin{itemize}[label=\textbullet]
\item{}Add and subtract matrices of the same size.\item{}Multiply matrices of appropriate sizes by one method.\item{}Compute powers \(A^p\) of a given square matrix \(A\).\item{}Use the distributive law for matrix multiplication and matrix addition correctly.\end{itemize}
\par

      Sometime after our meeting, a student should be able to:
    %
\begin{itemize}[label=\textbullet]
\item{}Multiply block matrices.\item{}Multiply matrices by \emph{three} methods.\item{}Give examples to show how matrix multiplication is not like
        ordinary multiplication of real numbers: including the trouble
        with commutativity, and the difficulty with inverses.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion on Matrix Algebra}
\typeout{************************************************}
\subsection[Discussion on Matrix Algebra]{Discussion on Matrix Algebra}\label{subsection-38}

      At the simplest level, this section is just about how to deal with the
      basic operations on matrices. We can add them and we can multiply them.
      We have already encountered matrix multiplication, and addition is even
      more natural.
    %
\par

      But a subtle and important thing is happening here. Matrices are taking
      on a life of their own. They are becoming first class objects, whose
      properties are interesting and possibly useful.
    %
\par

      This is an instance of the beginnings of \emph{Modern Algebra}, which
      is the study of the algebraic structures of abstracted objects. In this
      case, we study whole collections of matrices of a common shape, and we
      try to treat them like generalized numbers. Then the natural questions
      are how much like ``regular numbers'' are these matrices?
    %
\par

      Addition is about as well-behaved as you can expect, but multiplication
      is a bit trickier. Suddenly, two properties of multiplication for numbers
      don't quite work for matrices:
    %
\begin{itemize}[label=\textbullet]
\item{}multiplication does not necessarily commute: It need not be the case
        that \(AB\) is the same as \(BA\).
      \item{}we may not always have inverses: just because there is a
        matrix \(A\) which is not the zero matrix, it may not be the case
        that we can make sense of \(A^{-1}\) and get \(AA^{-1} = I\).
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Sage and Matrix Algebra}
\typeout{************************************************}
\subsection[Sage and Matrix Algebra]{Sage and Matrix Algebra}\label{subsection-39}

      Sage is aware of the basic matrix operations, and it won't let you get
      away with nonsense. Matrix multiplication and matrix addition are only
      defined if the dimensions of the matrices line up properly.
    %
\begin{lstlisting}[style=sageinput]
A = matrix(QQ, 2,3, [0,1,2,3,6,6])# A 2 by 3 matrix
B = matrix(QQ, 2,2, [4,2,3,1]) # 2 by 2 square matrix
C = matrix(QQ, 3,3, [2,1,2,1,2,1,2,1,2]) # a 3 by 3 square matrix
D = matrix(QQ, 2,3, [1,1,1,1,1,1]) # another 2 by 3 matrix
E = matrix(QQ, 3,2, [3,4,2,5,6,1]) # a 3 by 2 matrix
\end{lstlisting}
\par

      Let's see which of these Sage doesn't like. Can you predict, before
      evaluating the cells below, which of these will return an error?
    %
\begin{lstlisting}[style=sageinput]
A*B
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
B*A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 6 16 20]
[ 3  9 12]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A+B
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A*C
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 5  4  5]
[24 21 24]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
C*A
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A*D
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A+D
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsubsection  Sage and Matrix Addition}
\typeout{************************************************}
\subsubsection[Sage and Matrix Addition]{Sage and Matrix Addition}\label{subsubsection-17}
Matrix addition works a lot like addition of integers, as long as you fix
        a size first.%
\begin{itemize}[label=\textbullet]
\item{}There is a zero element.\item{}There are additive inverses (i.e. \emph{negatives}).\item{}The operation is commutative.\end{itemize}
\begin{lstlisting}[style=sageinput]
#This constructs the zero matrix
Z = zero_matrix(2,3); Z
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[0 0 0]
[0 0 0]
\end{lstlisting}
\par
Let us add \lstinline?A? and \lstinline?Z?:%
\begin{lstlisting}[style=sageinput]
A + Z
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[0 1 2]
[3 6 6]
\end{lstlisting}
\par
We can check that adding \lstinline?Z? doesn't change anything.%
\begin{lstlisting}[style=sageinput]
A + Z == A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\par
And we can do the natural thing to get an additive inverse.%
\begin{lstlisting}[style=sageinput]
L = -A; L
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 0 -1 -2]
[-3 -6 -6]
\end{lstlisting}
\par
Finally, this last thing should return zero.%
\begin{lstlisting}[style=sageinput]
A + L
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[0 0 0]
[0 0 0]
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsubsection  Sage and Matrix Multiplication}
\typeout{************************************************}
\subsubsection[Sage and Matrix Multiplication]{Sage and Matrix Multiplication}\label{subsubsection-18}

        Sage already has the structure of matrix multiplication built-in, and
        it can help with investigating the ways that matrix multiplicaiton
        is different from regular multiplication of numbers.
      %
\par

        We have seen above
        that Sage will not let us multiply matrices whose sizes do not match
        correctly.  Of course, one way around that trouble is to stick to square
        matrices. But even there we can have trouble with the fact that matrix
        multiplication might not commute. It is rarely the case that \(XY = YX\).
      %
\par

        For those of you who will eventually study Modern Algebra, the collection
        of all \(n\)-square matrices is an example of a non-commutative ring
        with unit.
      %
\begin{lstlisting}[style=sageinput]
A*E, E*A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(
         [12 27 30]
[14  7]  [15 32 34]
[57 48], [ 3 12 18]
)
\end{lstlisting}
\par
Sage knows about the ring structure. We can check for an inverse.%
\begin{lstlisting}[style=sageinput]
B.is_invertible()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
C.is_invertible()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
False
\end{lstlisting}
\par
And we can ask for the inverse in a couple of ways.%
\begin{lstlisting}[style=sageinput]
B.inverse()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[-1/2   1]
[ 3/2  -2]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
B^(-1)
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[-1/2   1]
[ 3/2  -2]
\end{lstlisting}
\par
One can even construct the whole ring of all \(n\times n\) matrices
      and play around inside it.%
\begin{lstlisting}[style=sageinput]
M = MatrixSpace(QQ, 2,2); M
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Full MatrixSpace of 2 by 2 dense matrices over Rational Field
\end{lstlisting}
\par
It is then not too hard to construct the identity element, which is
        the regular identity matrix of the correct size.
      %
\begin{lstlisting}[style=sageinput]
M(1)
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 0]
[0 1]
\end{lstlisting}
\par
Also, the zero matrix of the correct size is easy to make.%
\begin{lstlisting}[style=sageinput]
M(0)
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[0 0]
[0 0]
\end{lstlisting}
\par

        In fact, this allows you to short cut the construction of any matrix in
        \lstinline?M?. This can be really useful if you are going to work with a lot
        of matrices of the same shape.
      %
\begin{lstlisting}[style=sageinput]
H = M([2,1,1,1]); H
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[2 1]
[1 1]
\end{lstlisting}
\par
You can even use this to make complicated expressions out of matrix operations.
        As long as everything makes sense, Sage will do all the work.
      %
\begin{lstlisting}[style=sageinput]
H^2 + H
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[7 4]
[4 3]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
H * (H - 5*M(1) + 2*H^2)
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[21 14]
[14  7]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
# this one should turn out to be zero.
# I know this because of a theorem.
H^2 - 3*H + M(1)
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[0 0]
[0 0]
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsubsection  Sage and Block Matrices}
\typeout{************************************************}
\subsubsection[Sage and Block Matrices]{Sage and Block Matrices}\label{subsubsection-19}
Sage has a built-in method for constructing matrices out of blocks, too.
        It is easiest to see an example.
      %
\par

        To make this work, you have to tell Sage when you have a vector if you
        want it to be either a row or a column vector in the appropriate places.
      %
\begin{lstlisting}[style=sageinput]
X = matrix(QQ, 2,2, [0,-1,1,0])
v = vector([2,3])
w = matrix(QQ, 1,1, [1])
Blockey = block_matrix(QQ, 2,3,
                       [[X, X, v.column()],
                       [v.row(), v.row(), w]])
Blockey
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 0 -1| 0 -1| 2]
[ 1  0| 1  0| 3]
[-----+-----+--]
[ 2  3| 2  3| 1]
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-40}
\begin{task}
\label{task-55}

        Make an example of a \(2\times 3\) matrix and a \(3\times 3\)
        matrix, and use this to demonstrate the three different ways to multiply
        matrices.
      \end{task}
\begin{task}
\label{task-56}

        Give an example of a pair of \(2\times 2\) matrices \(A\) and
        \(B\) so that \(AB = 0\) but \(BA\neq 0\), or explain why
        this is  impossible.
      \end{task}
\begin{task}
\label{task-57}

        Give an example of a \(3\times 3\) matrix \(A\) such that neither
        \(A\) nor \(A^2\) is the zero matrix, but \(A^3=0\).
      \end{task}
\begin{task}
\label{task-58}

        Find all examples of matrices \(A\) which commute with both
        \(B = \left( \begin{smallmatrix} 1 & 0 \\ 0 & 0
          \end{smallmatrix}\right)\) and
        \(C = \left( \begin{smallmatrix} 0 & 1 \\ 0 & 0
          \end{smallmatrix}\right)\). That is, find all matrices \(A\)
        so that \(AB = BA\) and \(AC= CA\). How do you know you have
        all such matrices?
      \end{task}
\begin{task}
\label{task-59}

          Consider the matrix
          \[
            A = \begin{pmatrix} 2 & 1 & 0 \\ -2 & 0 & 1 \\ 8 & 5 & 3 \end{pmatrix}.
          \]
          Which elimination matrices \(E_{21}\) and \(E_{31}\) produce zeros in the
          \((2,1)\) and \((3,1)\) positions of \(E_{21}A\)
          and \(E_{31}A\)?
        %
\par

          Find a single matrix \(E\) which produces both zeros at once.
          Multiply \(EA\) to verify your result.
        %
\end{task}
\begin{task}
\label{task-60}

          Let's take a different view of the last computation.
          Block multiplication says that column 1 is eliminated by
          a step that looks like this one:
          \[
            EA = \begin{pmatrix} 1 & 0 \\ -c/a & I \end{pmatrix}
            \begin{pmatrix} a & b \\ c & D \end{pmatrix} =
            \begin{pmatrix} a & b \\ 0 & D - cb/a \end{pmatrix}.
          \]
          Here \(I\) is the \(2\times 2\) identity matrix, \(D\) is a
          \(2\times 2\) matrix, etc.
        %
\par

          So, in the last exercise, what are \(a\), \(b\), \(c\)
          and \(D\) and what
          is \(D-cb/a\)? Be sure to describe what shape each matrix has: the
          number of rows and columns.
        %
\end{task}
\begin{task}
\label{task-61}

        Suppose that we have already solved the equation \(Ax=b\) for the
        following three special choices of \(b\):
        \[
          Ax_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} \text{ , } Ax_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix} \text{, and } Ax_3 = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}.
        \]
        If the three solutions are called \(x_1\), \(x_2\) and \(x_3\)
        and then bundled together to make the columns of a matrix
        \[X =  \begin{pmatrix} | & | & | \\
          x_1 & x_2 & x_3 \\ | & | & |
          \end{pmatrix},
        \] what is the matrix \(AX\)? What does this mean about \(X\)?
      \end{task}
\typeout{************************************************}
\typeout{Section 2.6 Matrix Inverses}
\typeout{************************************************}
\section[Matrix Inverses]{Matrix Inverses}\label{inverse}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-41}
\begin{itemize}[label=\textbullet]
\item{}Read section 2.5 of \emph{Strang}.\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-42}

      Before class, a student should be able to:
    %
\begin{itemize}[label=\textbullet]
\item{}State the definition of \terminology{invertible} matrix.\item{}Solve an equation \(Ax = b\) using the inverse of \(A\)
        if it exists.
      \item{}State how inverses and multiplication interact.\item{}Use Gauss-Jordan elimination to compute the inverse of a matrix.\item{}State a test for invertibility of square matrices using pivots.\end{itemize}
\par

      Some time after class, a student should be able to:
    %
\begin{itemize}[label=\textbullet]
\item{}Describe the connection between Gauss-Jordan elimination and solving
        \(n\) different systems of equations.
      \item{}Describe the connection between Gauss-Jordan elimination, computing
        matrix inverses, and the process of elimination by matrix multiplication.
      \item{}State the definition of the determinant of a square matrix.\item{}State the connection between the determinant of a square matrix and
        invertibility.\item{}State the distinction between a matrix being \terminology{invertible}
        and a matrix being \terminology{singular}.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion}
\typeout{************************************************}
\subsection[Discussion]{Discussion}\label{subsection-43}
\typeout{************************************************}
\typeout{Subsubsection  Matrix Inverses}
\typeout{************************************************}
\subsubsection[Matrix Inverses]{Matrix Inverses}\label{subsubsection-20}

        The main point of this section is to start focusing on the first big
        problem in linear algebra. How can you tell, in advance, that a system
        of \(n\) equations in \(n\) unknowns will have a solution?
      %
\par

        Of course, like all things we have been studying, this will have several
        different faces, all of which are equivalent. The one front and center
        right now is this: When does an \(n \times n\) square matrix have an inverse?
      %
\typeout{************************************************}
\typeout{Subsubsection  Finding an Inverse: Gauss-Jordan Elimination}
\typeout{************************************************}
\subsubsection[Finding an Inverse: Gauss-Jordan Elimination]{Finding an Inverse: Gauss-Jordan Elimination}\label{subsubsection-21}

        There is an effective method for finding the inverse, and it is
        Gauss-Jordan elimination. (This is sometimes just called
        \emph{Gaussian elimination}.) Essentially, you wish to solve \(n\)
        different systems \(Ax= b\) of size \(n\times n\) all at the same
        time, with specially chosen right hand sides.
      %
\par

        The process is an algorithm, so it is very specific. If you do this some
        other way, you aren't doing Gauss-Jordan Elimination. The name is applied
        to the process.
      %
\par

        \emph{Gauss-Jordan Elimination}
      %
\begin{itemize}[label=\textbullet]
\item{}
          \emph{Augment:} Tack on a copy of the identity matrix of the same
          size to the right hand side of your matrix. It should now look like
          \((A \mid I)\).
        \item{}
          \emph{Forward Pass:} This is a nested procedure:
          \begin{itemize}[label=$\circ$]
\item{}
              \emph{preparation:} If necessary, use a row swap to make a
              non-zero entry in the upper left entry.
            \item{}
              \emph{make zeros:} The upper left entry is our first pivot. Use
              the operation of adding a multiple of the first row to the other
              rows to kill the entries below this first pivot.
            \item{}
              \emph{step down:} Step down to the second row and repeat the
              above, but ignoring rows and columns above and to the left. Repeat
              as necessary till you run out of rows.
            \end{itemize}

          If at any point in the process you get a row consisting of only zeros,
          perform a row switch to suffle it to the bottom. When the forward
          pass is complete, you should have an upper triangular matrix.
        \item{}
          \emph{Backward Pass:} This is also nested, like the forward pass,
          except that instead of working down and to the right, you begin at
          the lower right with the last pivot and work up and to the left.
          When complete, the matrix should have at most one non-zero entry in
          each row. This entry will be a pivot.
        \item{}
          \emph{Rescale:} rescale rows to make the pivots into \(1\)'s.
        \end{itemize}
\par

        At the end of the whole process, you should have something that looks
        like this: \((I \mid B)\). The wonderful part: \(B\) is the
        inverse of \(A\). Well, almost. The process can fail! If along the
        line you find that the left hand block of your big augmented matrix
        doesn't have \(n\) pivots in it, then your matrix was not invertible.
      %
\par

        What you have computed in the left hand block with the Gauss-Jordan
        elimination is the \emph{reduced row-echelon form} of your original matrix.
      %
\typeout{************************************************}
\typeout{Subsubsection  The Big Theorem: invertibility, singularity, and the determinant}
\typeout{************************************************}
\subsubsection[The Big Theorem: invertibility, singularity, and the determinant]{The Big Theorem: invertibility, singularity, and the determinant}\label{subsubsection-22}

        What is the key?
      %
\begin{theorem}\label{theorem-1}

          An \(n\times n\) matrix \(A\) is invertible exactly when it has
          \(n\) pivots. Equivalently, its reduced row-echelon form has \(n\)
          non-zero entries down the diagonal. The inverse will be computed by
          Gauss-Jordan elimination.
        \end{theorem}
\par

        This is huge. The algorithm is not difficult, and it answers an
        important question exactly.
      %
\par

        Note that we said a square matrix was \emph{singular} when it did not
        have enough pivots. So what the above says is that a matrix is invertible
        if and only if it is non-singular.
      %
\typeout{************************************************}
\typeout{Subsubsection  A simple test}
\typeout{************************************************}
\subsubsection[A simple test]{A simple test}\label{subsubsection-23}

        We can use the above to make a simple numerical test of when a matrix is
        invertible. First do the forward pass of elimination to obtain an upper
        triangular matrix. Take the product of the diagonal entries. This will
        be zero if and only if one of the diagonal entries is zero, which will
        only happen if there are fewer than \(n\) pivots. This product is then
        helpful enough to test for invertibility, and so it deserves its own
        name: the \terminology{determinant}. We shall learn more about this quantity later.
      %
\typeout{************************************************}
\typeout{Subsection  Sage and Gauss-Jordan Elimination}
\typeout{************************************************}
\subsection[Sage and Gauss-Jordan Elimination]{Sage and Gauss-Jordan Elimination}\label{subsection-44}

      We have already seen that Sage has commands for constructing matrices and
      performing row operations. Those are the operations used to perform
      Gauss-Jordan Elimination. But there are several interesting and useful
      commands in this neighborhood we have not yet discussed.
    %
\par

      Let us construct my favorite matrix so we have something to play with.
    %
\begin{lstlisting}[style=sageinput]
A = matrix(QQ, 2,2, [2,1,1,1])
\end{lstlisting}
\par

      We can use the \lstinline?.is_invertible()? method to check that \lstinline?A? is
      invertible. In general, this method returns \lstinline?True? or \lstinline?False?.
    %
\begin{lstlisting}[style=sageinput]
A.is_invertible()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\par

      And we can get Sage to just compute the inverse for us.
    %
\begin{lstlisting}[style=sageinput]
A.inverse()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1 -1]
[-1  2]
\end{lstlisting}
\par

      Just so we can see what happens if the matrix is not invertible, we try
      another matrix.
    %
\begin{lstlisting}[style=sageinput]
B = matrix(QQ, 2,2, [0,1,0,0])
B.is_invertible()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
False
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
B.inverse()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Error in lines 1-1
...
ZeroDivisionError: input matrix must be nonsingular
\end{lstlisting}
\par

      We can also ask Sage to compute determinants with the \lstinline?.determinant()?
      method.
    %
\begin{lstlisting}[style=sageinput]
A.determinant(), B.determinant()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(1,0)
\end{lstlisting}
\par

      Sage is also capable of computing the reduced row echelon form
      (the ``rref'') of a matrix with the appropriately named \lstinline?.rref()?
      method.
    %
\begin{lstlisting}[style=sageinput]
A.rref()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 0]
[0 1]
\end{lstlisting}
\par

      The method \lstinline?.rref()? does not change the matrix \lstinline?A?. There is
      another command which will work the same way for our purposes,
      \lstinline?.echelon_form()?.
    %
\begin{lstlisting}[style=sageinput]
A.echelon_form()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 0]
[0 1]
\end{lstlisting}
\par

      There is a related command which \emph{will find the rref and then update the
      matrix}. It is called \lstinline?.echelonize()?.
      Because I don't really want to mess with \lstinline?A?, we will
      make a copy first.
    %
\begin{lstlisting}[style=sageinput]
C = copy(A) # fancy Python trick! (not so fancy)
C.echelonize()
\end{lstlisting}
\par

      Now we ask sage to print those out for us.
    %
\begin{lstlisting}[style=sageinput]
print A
print '\n'
print C
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
        [2 1]
        [1 1]

        [1 0]
        [0 1]
\end{lstlisting}
\par

      Now, we can be just a bit more hands-on with Gauss-Jordan elimination
      if we do it this way. We will combine commands we have used before to
      do this.
    %
\begin{lstlisting}[style=sageinput]
M = MatrixSpace(QQ, 2,2)
M(1) # this is the 2x2 identity
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 0]
[0 1]
\end{lstlisting}
\par

      Now we do the algorithm.
    %
\begin{lstlisting}[style=sageinput]
D = A.augment(M(1))
D.rref()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1  0  1 -1]
[ 0  1 -1  2]
\end{lstlisting}
\par

      That was good. But we only need the right-hand submatrix. We can get Sage
      to report just that!
    %
\begin{lstlisting}[style=sageinput]
E = D.rref().matrix_from_columns([2,3]); E
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1 -1]
[-1  2]
\end{lstlisting}
\par

      It is often convenient to chain methods together like this. Then you can
      read what happens from left to right.
    %
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-45}

      Keep this in mind. The computations are simple, but tedious.
      Perhaps you want to use an appropriate tool.
    %
\begin{task}
\label{task-62}

        Use Gauss-Jordan elimination to find the inverse of the matrix \(A\) below.
        \[
        A = \begin{pmatrix} 3 & 17 \\ 1 & 6 \end{pmatrix}
        \]
        Be sure to clearly write down the operations you use and the matrices
        which perform the operations by left multiplication.
      \end{task}
\begin{task}
\label{task-63}

        Use Gauss-Jordan elimination to find the inverse of the matrix \(X\) below.
        \[
        X = \begin{pmatrix} a & b \\ c & d \end{pmatrix}
        \]
        Be sure to clearly write down the operations you use and the matrices
        which perform the operations by left multiplication.
      \end{task}
\begin{task}
\label{task-64}

        Use Gauss-Jordan elimination to find the inverse of the matrix \(B\) below.
        \[
        B = \begin{pmatrix} 3 & 4 & -1\\ 1 & 6 & 1 \\
        0 & 3 & -1 \end{pmatrix}
        \]
        Be sure to clearly write down the operations you use and the matrices
        which perform the operations by left multiplication.
      \end{task}
\begin{task}
\label{task-65}

        Use Gauss-Jordan elimination to find the inverse of the matrix \(B\) below.
        \[
        B = \begin{pmatrix}
        0 & 3 & 4 & -1\\
        0 & 1 & 6 & 1 \\
        2 & 0 & 3 & -1 \\
        5 & -1 & 1 & 3
        \end{pmatrix}
        \]
        Be sure to clearly write down the operations you use and the matrices
        which perform the operations by left multiplication.
      \end{task}
\begin{task}
\label{task-66}

        Use Gauss-Jordan elimination to find the inverse of the matrix \(D\) below.
        \[
        D = \begin{pmatrix}
        3 & 17 & -1 & 3 & 1 \\ 1 & 6 & -2 & 1 & 1 \\
        2 & 2 & 1 & -5 & 1 \\ 0 & 0 & 3 & 1 & -3 \\
        -2 & 3 & 4 & 1 & 1
        \end{pmatrix}
        \]\end{task}
\begin{task}
\label{task-67}

        Suppose that for the matrix \(D\) in the last exercise we imagine solving the matrix
        equation \(Dx = b\) for some vector \(b\) of the appropriate size. What might one mean
        by the row picture in this case? What might the column picture mean?
      \end{task}
\begin{task}
\label{task-68}

        Design a \(6 \times 6\) matrix which has the following properties:
        \begin{itemize}[label=\textbullet]
\item{} no entry equal to zero\item{} the reduced row echelon form should have exactly 5 pivots\item{} the 5 pivots should be different numbers\item{} no pair of rows should be scalar multiples of one another\end{itemize}

        Is your matrix invertible? How do you know? Does Sage say it is invertible?
      \end{task}
\typeout{************************************************}
\typeout{Section 2.7 The \(LU\) Decomposition}
\typeout{************************************************}
\section[The \(LU\) Decomposition]{The \(LU\) Decomposition}\label{lu-decomposition}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-46}
\begin{itemize}[label=\textbullet]
\item{}Read section 2.6 of \emph{Strang}.\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-47}

      Before class, a student should be able to:
    %
\begin{itemize}[label=\textbullet]
\item{}
        Use Gaussian Elimination to find the \(LU\) and \(LDU\)
        decompositions of a matrix.
      \item{}
        Describe when the process of Gaussian Elimination will fail to
        produce an \(LU\) decomposition.
      \end{itemize}
\par

      Sometime after class, a student should be able to:
    %
\begin{itemize}[label=\textbullet]
\item{}
        Solve a system of equations by using the \(LU\) decomposition and
        two triangular systems.
      \item{}
        Explain the connection between matrix elimination and the \(LU\)
        or \(LDU\) factorization of a matrix.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion: The \(LU\) Decomposition of a Matrix}
\typeout{************************************************}
\subsection[Discussion: The \(LU\) Decomposition of a Matrix]{Discussion: The \(LU\) Decomposition of a Matrix}\label{subsection-48}

      We now look at the ideas behind elimination from a more advanced
      perspective. If we think about the matrix multiplication form of the
      forward pass, we can realize it a \emph{matrix decomposition theorem}:
    %
\begin{theorem}\label{theorem-2}

        Any square matrix \(A\) can be written as a product \(A = LU\)
        where \(L\)
        is a lower triangular matrix and \(U\) is an upper triangular matrix.
        Moreover, the matrix \(L\) will have \(1\)'s down its diagonal.
      \end{theorem}
\par

      There are three key observations that make this work:
    %
\begin{itemize}[label=\textbullet]
\item{}
        Each of the matrices \(E_{ij}\) that affects a row operation of the
        form \emph{add a multiple of row \(i\) to row \(j\)} is an invertible
        matrix, with an easy to find inverse.
      \item{}
        If we make a sequence of row operations in the forward pass using
        matrices \(E_k\), then we are essentially computing a big product
        \[
        E_k \dots E_1 A = U
        \]
        where each of the \(E_i\)'s is a lower triangular matrix and the matrix
        \(U\) is upper triangular. This can the be rewritten as
        \[
        A = \left(E_1^{-1} \dots E_k^{-1} \right) U .
        \]
        Note that the inverses have to be done \emph{in reverse order} for things
        to cancel out properly.
      \item{}
        Finally, the product \(L = E_1^{-1} \dots E_k^{-1}\) is really easy to
        compute, because its entries are simply the negatives of the multipliers
        we used to do the operations in the forward pass.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsubsection  A Nice Computational Result}
\typeout{************************************************}
\subsubsection[A Nice Computational Result]{A Nice Computational Result}\label{subsubsection-24}

        One important output of this comes into play when we want to compute
        solutions to equations like \(Ax = b\). Since we can write \(A = LU\),
        then our equation can be split into two (big) steps:
        \begin{enumerate}
\item{} First find the solution to the equation \(Ly = b\).\item{} Then find the solution to the equation \(Ux = y\).\end{enumerate}

      %
\par

        First, note that this is a good thing because both of the systems
        \(Ly = b\) and \(Ux = y\) are triangular. They can be solved by back
        substitution. In \(Ly = b\) you work from the top down, and in \(Ux=y\)
        you work from the bottom up.
      %
\par

        Second, this works because following this process gives us a
        vector \(x\) which will satisfy this:
        \[
        Ax = (LU)x = L (Ux) = Ly = b.
        \]
      %
\par

        Third, this doesn't really save time when you only want to solve
        one equation \(Ax= b\). But if you have lots of different values
        of \(b_i\), and you want to solve all of the equations \(Ax = b_i\),
        it becomes a lot faster to factor the matrix \(A= LU\) once and do
        two back substitutions for each value of \(b_i\).
      %
\typeout{************************************************}
\typeout{Subsection  Sage and The \(LU\) Decomposition}
\typeout{************************************************}
\subsection[Sage and The \(LU\) Decomposition]{Sage and The \(LU\) Decomposition}\label{subsection-49}

      A neat feature of linear algebra is that simple facts about solving equations
      have several different incarnations. This section contains the first big
      example: Gaussian Elimination leads to a multiplicative decomposition (a
      factorization) for matrices.
    %
\par

      Each step of Gaussian elimination is a simple row operation, and if we do
      the process in the standard order, then the \(LU\) decomposition can
      be read out directly, without any extra computation.
    %
\par

      First, let us recall how Sage can help us check bits of
      the three key observations above.
    %
\begin{lstlisting}[style=sageinput]
M = MatrixSpace(QQ,3,3)
One = M(1); One
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 0 0]
[0 1 0]
[0 0 1]
\end{lstlisting}
\par

      Consider a matrix which performs an elementary row operation of the
      form ``add a multiple of one row to another''. The matrix \(E\)
      below performs the operations \emph{add \(-4\) times row 2 to row 3}.
    %
\begin{lstlisting}[style=sageinput]
E = One.with_added_multiple_of_row(2,1,-4); E
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1  0  0]
[0  1  0]
[0 -4  1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
E.is_invertible()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
E.inverse()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 0 0]
[0 1 0]
[0 4 1]
\end{lstlisting}
\par

      Note that the inverse just came from changing the sign of that one entry.
      This makes sense for the following reason: the opposite operation to
      ``add \(-4\) times row 2 to row 3'' should be ``Add \(4\) times
      row 2 to row 3''. That is the way you undo the operation!
    %
\typeout{************************************************}
\typeout{Subsubsection  Study Break: Try it yourself}
\typeout{************************************************}
\subsubsection[Study Break: Try it yourself]{Study Break: Try it yourself}\label{subsubsection-25}

        Make your own \(3\times 3\) matrix and check the whole procedure.
      %
\typeout{************************************************}
\typeout{Subsubsection  Sage Commands to short-cut the process}
\typeout{************************************************}
\subsubsection[Sage Commands to short-cut the process]{Sage Commands to short-cut the process}\label{subsubsection-26}

        Here is the basic command for getting Sage to compute the \(LU\)
        decomposition directly.
      %
\begin{lstlisting}[style=sageinput]
A = M([2,3,1,-1,3,5,6,5,4]); A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 2  3  1]
[-1  3  5]
[ 6  5  4]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.LU()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(
[0 0 1]  [   1    0    0]  [     6      5      4]
[0 1 0]  [-1/6    1    0]  [     0   23/6   17/3]
[1 0 0], [ 1/3 8/23    1], [     0      0 -53/23]
)
\end{lstlisting}
\par

        Hold on, the output is three matrices. Not two, but three. One is upper
        triangular, one is lower triangular, but the first one is a
        \terminology{permutation matrix}. (It switches rows 1 and 3.) What is
        going on? If you perform a search in the Sage documentation, you find
        \href{http://www.sagemath.org/doc/reference/matrices/sage/matrix/matrix2.html}{this page}.
        There is a description of the command, and the first bit is something about
        a ``pivoting strategy'' and row swaps. But we don't want row swaps.
      %
\par

        By reading carefully, we can see what the way through is, too. We can
        specify our pivoting strategy by adding the keyword argument
        \lstinline?pivot="nonzero"? inside the parentheses. Then the algorithm used
        will match the one Strang describes.
      %
\par
(If you are using SMC, you can access the help using many other ways.
        But a Google search for \lstinline?Sage math "topic"? will hit the documentation
        pretty reliably.)
      %
\begin{lstlisting}[style=sageinput]
A.LU(pivot='nonzero')
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(
[1 0 0]  [   1    0    0]  [   2    3    1]
[0 1 0]  [-1/2    1    0]  [   0  9/2 11/2]
[0 0 1], [   3 -8/9    1], [   0    0 53/9]
)
\end{lstlisting}
\par

        Aaah! There we go, now the permutation part is the identity. Note that
        the command returns a ``tuple''. This is a collection of things,
        kind of like a list. (Technical Python details omitted here.)
        To grab the information out, we assign the parts of that output to different
        names so we can use them.
      %
\begin{lstlisting}[style=sageinput]
P, L, U = A.LU(pivot='nonzero')
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
L # this is the lower triangular part
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[   1    0    0]
[-1/2    1    0]
[   3 -8/9    1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
U # this is the upper triangular part
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[   2    3    1]
[   0  9/2 11/2]
[   0    0 53/9]
\end{lstlisting}
\par

        Those parts should be factors of \(A\). We can check:
      %
\begin{lstlisting}[style=sageinput]
L*U # this should multiply to A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 2  3  1]
[-1  3  5]
[ 6  5  4]
\end{lstlisting}
\par

        And we can have Sage check if they are really equal.
      %
\begin{lstlisting}[style=sageinput]
L*U == A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsubsection  What about the \(LDU\) decomposition?}
\typeout{************************************************}
\subsubsection[What about the \(LDU\) decomposition?]{What about the \(LDU\) decomposition?}\label{subsubsection-27}
For now, Sage has no built-in \(LDU\) decomposition.%
\typeout{************************************************}
\typeout{Subsubsection  An insurmountable obstacle}
\typeout{************************************************}
\subsubsection[An insurmountable obstacle]{An insurmountable obstacle}\label{subsubsection-28}

        Some matrices \emph{require} permutations of rows. In these cases, we
        have to have some pivoting strategy \emph{must} be employed. Consider this
        example.
      %
\begin{lstlisting}[style=sageinput]
B = M([0,2,2,1,3,1,1,1,1]); B
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 0  2  2]
[ 1  3 -1]
[ 1  1  1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
B.LU(pivot='nonzero')
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(
[0 1 0]  [ 1  0  0]  [ 1  3 -1]
[1 0 0]  [ 0  1  0]  [ 0  2  2]
[0 0 1], [ 1 -1  1], [ 0  0  4]
)
\end{lstlisting}
\par

        This has a row-swap permutation matrix, and \emph{it must}. Since the
        (1,1) entry of \lstinline?B? is zero, but numbers below that are not zero,
        we cannot use zero as a pivot. We'll sort out how to handle this in the next
        section.
      %
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-50}

    Keep this in mind. The computations are simple, but tedious.
    Perhaps you want to use an appropriate tool.
  %
\begin{task}
\label{task-69}

        Consider the following system of 3 linear equations in 3 unknowns.
        \[\left\{
        \begin{array}{rrrrrrr}
        x & + & y & + & z & = & 5 \\
        x & + & 2y & + & 3z & = & 7 \\
        x & + & 3y & + & 6z & = & 11
        \end{array}\right.
        \]
        Perform the forward pass of elimination to find an equivalent upper
        triangular system. Write down this upper triangular system. What three
        row operations do you need to perform to make this work?
      %
\par

        Use the information you just found to write a matrix
        decomposition \(A = LU\) for the coefficient matrix \(A\) for this
        system of equations. (Be sure to multiply the matrices \(L\) and \(U\)
        to check your work.)
      %
\end{task}
\begin{task}
\label{task-70}

        Solve the two systems \(Ly = b\) and \(Ux=y\) obtained in the last
        exercise.
      %
\par

        Solve the system \(Ax=b\) directly using Gauss-Jordan
        elimination (hint: use Sage) and make sure that the results
        are the same.
      %
\end{task}
\begin{task}
\label{task-71}

      Consider the matrix \(A\) below. Find the matrix \(E\) which
      transforms \(A\) into an upper triangular matrix \(EA = U\).
      Find \(L = E^{-1}\). Use this to write down the \(LU\) decomposition
      \(A= LU\) of \(A\).
      \[
        A =
        \begin{pmatrix}
        2 & 1 & 0 \\
        0 & 4 & 2 \\
        6 & 3 & 5
        \end{pmatrix}
      \]\end{task}
\begin{task}
\label{task-72}

      The matrix below is \terminology{symmetric}, because if you flip it across
      its main diagonal you get the same thing. Find the \(LDU\) triple
      decomposition of this symmetric matrix.
      \[
        B =
        \begin{pmatrix}
        2 & 4 \\
        4 & 11
        \end{pmatrix}
      \]\end{task}
\begin{task}
\label{task-73}

      The matrix below is \terminology{symmetric}, because if you flip it
      across its main diagonal you get the same thing. Find the \(LDU\)
      triple decomposition of this symmetric matrix.
      \[
        C =
        \begin{pmatrix}
        1 & 4 & 0 \\
        4 & 12 & 4 \\
        0 & 4 & 0
        \end{pmatrix}
      \]\end{task}
\begin{task}
\label{task-74}

      The matrix below is \terminology{symmetric}, because if you flip it
      across its main diagonal you get the same thing. Find the \(LU\)
      decomposition of this symmetric matrix.
      \[
        D =
        \begin{pmatrix}
        a & a & a & a \\
        a & b & b & b \\
        a & b & c & c \\
        a & b & c & d
        \end{pmatrix}
      \]
      What conditions on the variables \(a\), \(b\), \(c\),
      and \(d\) will guarantee that this matrix has four pivots?
    \end{task}
\begin{task}
\label{task-75}

      Find an example of a \(3\times 3\) matrix \(A\) which has all of
      its entries non-zero, so that the \(LU\) decomposition has
      \(U = I\), where \(I\) is the identity matrix, or explain why no
      such example exists.
    \end{task}
\typeout{************************************************}
\typeout{Section 2.8 Permutation Matrices}
\typeout{************************************************}
\section[Permutation Matrices]{Permutation Matrices}\label{permutations}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-51}
\begin{itemize}[label=\textbullet]
\item{}Read section 2.7 of \emph{Strang}\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-52}
Before class, a student should be able to:%
\begin{itemize}[label=\textbullet]
\item{}
        Compute the transpose of a matrix.
      \item{}
        Correctly perform calculations where the transpose interacts with the
        operations of matrix sum, matrix product, and matrix inverse.
      \item{}
        Compute inner and outer products using the transpose.
      \item{}
        Decide if a matrix is symmetric or not.
      \item{}
        Recognize permutation matrices, and design permutation matrices which
        correspond to given row swaps.
      \end{itemize}
\par

      Some time after class, a student should be able to:
    %
\begin{itemize}[label=\textbullet]
\item{}
        Find the \(LDL^T\) decomposition for symmetric matrices.
      \item{}
        Explain how the necessity of permuting rows during Gaussian elimination
        leads to the decomposition \(PA = LU\).
      \item{}
        Explain why \(P^T = P^{-1}\) for permutation matrices.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion: Transposes, Symmetric Matrices, and Permutations}
\typeout{************************************************}
\subsection[Discussion: Transposes, Symmetric Matrices, and Permutations]{Discussion: Transposes, Symmetric Matrices, and Permutations}\label{subsection-53}

      An important operation on matrices we have yet to encounter is called the
      \terminology{transpose}. If \(A\) is an \(m\times n\) matrix, the
      transpose \(A^T\) of
      \(A\) is made by changing the roles of the rows and the columns. The result
      \(A^T\) will be an \(n \times m\) matrix, because of this switch.
    %
\par

      For now, the transpose will feel like some random thing, but its primary
      importance comes from its connection with the dot product. If we think of
      column vectors \(u\) and \(v\) of size \(n\) as if they are \(n \times 1\)
      matrices, then the dot product \(u \cdot v\) can be computed with a nice
      combination of matrix multiplication and the transpose:
      \[
      u \cdot v = u^T v .
      \]
      On the right, this is matrix multiplication! That makes sense because
      \(u^T\) is \(1 \times n\) and \(v\) is \(n \times 1\). This
      means that the result
      is a \(1\times 1\) matrix, i.e. a number.
    %
\par

      Since the dot product contains all of the geometry of Euclidean space in
      it, the transpose becomes an important operation. I know that sounds weird,
      but the dot product contains all of the information we need to measure
      lengths and angles, so basically all of the \emph{metric} information in
      Euclidean geometry is there.
    %
\typeout{************************************************}
\typeout{Subsubsection  Algebraic results about the transpose}
\typeout{************************************************}
\subsubsection[Algebraic results about the transpose]{Algebraic results about the transpose}\label{subsubsection-29}

        There are some key results about the way the transpose interacts with
        other matrix operations, each of these can be checked with some tedious
        computation:
      %
\begin{itemize}[label=\textbullet]
\item{}
          If \(A\) and \(B\) are matrices of the same shape, then
          \((A+B)^T = A^T + B^T\).
        \item{}
          If \(A\) and \(B\) are of sizes so that \(AB\) is defined,
          then \((AB)^T = B^T A^T\).
        \item{}
          If \(A\) is an invertible matrix, with inverse \(A^{-1}\),
          then \(A^T\) is also invertible and it has inverse
          \(\left(A^T\right)^{-1} = \left(A^{-1}\right)^T \).
        \end{itemize}
\typeout{************************************************}
\typeout{Subsubsection  Symmetric Matrices}
\typeout{************************************************}
\subsubsection[Symmetric Matrices]{Symmetric Matrices}\label{subsubsection-30}

        A matrix \(A\) is called \emph{symmetric} when \(A^T = A\). These
        pop up in lots of interesting places in linear algebra. A neat result
        is that a symmetric matrix has a symmetric looking \(LDU\) decomposition:
        \[
        \text{if } A^T=A\text{, then } A = LDL^T .
        \]
        That is, in the LDU decomposition, \(U = L^T\).
      %
\par

        There are several ways to get symmetric matrices. For example, if
        \(A\) is any matrix, the new matrix \(B = A^T A\) will be symmetric.
        (Check this.) Also, the matrix \(S = A^T + A\) will be symmetric.
      %
\typeout{************************************************}
\typeout{Subsubsection  Permutation Matrices and Pivoting strategies in Gauss-Jordan Elimination}
\typeout{************************************************}
\subsubsection[Permutation Matrices and Pivoting strategies in Gauss-Jordan Elimination]{Permutation Matrices and Pivoting strategies in Gauss-Jordan Elimination}\label{subsubsection-31}

        It is sometimes the case that Gauss-Jordan elimination requires a row
        swap. As we have seen, the operation of swapping a row can be achieved
        by left multiplying by a matrix of a special type. If we take a bunch of
        those and multiply them together, we still get a matrix which is in a
        special class: \emph{the permutation matrices}.
      %
\par

        A permutation matrix is square matrix having a single \(1\) in each column
        and in each row. A helpful property of permutation matrices is that they
        are invertible, and their inverses are the same as their transposes:
        \[
        P^{-1} = P^T .
        \]
      %
\par

        Gauss-Jordan elimination is easy enough to understand, now. It is time
        to let go of performing all those arithmetic operations by hand. So,
        permutation matrices become important for a different reason! Even if
        Gauss-Jordan elimination can be done without a row swap, it may be
        numerically better for a computer to swap out for a larger number as a
        pivot, so a row swap is used anyway. This partial pivoting strategy is
        encapsulated in most computer algebra algorithms in some way, and is
        part of the computation involved in computing a PLU decomposition.
        Strang has a decent discussion of the choices, below we will
        discuss how Sage handles this.
      %
\typeout{************************************************}
\typeout{Subsection  Sage and Transposes, Symmetry, Permutations, and Pivots}
\typeout{************************************************}
\subsection[Sage and Transposes, Symmetry, Permutations, and Pivots]{Sage and Transposes, Symmetry, Permutations, and Pivots}\label{subsection-54}

      There is a lot going on in this little section. At first glance, it is a
      bit intimidating. But we have seen most of the ideas before.
    %
\typeout{************************************************}
\typeout{Subsubsection  The Transpose}
\typeout{************************************************}
\subsubsection[The Transpose]{The Transpose}\label{subsubsection-32}

        The transpose of a matrix is what you get by switching the roles of rows
        and columns. Sage has a simple method for this.
      %
\begin{lstlisting}[style=sageinput]
M = MatrixSpace(QQ, 3,3)
A = M([1,2,3,4,5,6,7,8,9]); A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 2 3]
[4 5 6]
[7 8 9]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.transpose()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 4 7]
[2 5 8]
[3 6 9]
\end{lstlisting}
\par

        One place that the transpose is useful is in describing the dot product.
        Check this out.
      %
\begin{lstlisting}[style=sageinput]
u = vector([1,2,3])
v = vector([4,5,6])
u.dot_product(v)
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
32
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
U = u.column(); U # this puts u into a column matrix
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1]
[2]
[3]
\end{lstlisting}
\par

        To be sure, we check what the ``parent'' of \lstinline?U? is.
      %
\begin{lstlisting}[style=sageinput]
U.parent()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Full MatrixSpace of 3 by 1 dense matrices over Integer Ring
\end{lstlisting}
\par

        See! Sage thinks of \lstinline?U? as a matrix with 3 rows and 1 column.
      %
\par

        Now we do the same with \lstinline?v?
      %
\begin{lstlisting}[style=sageinput]
V = v.column()
V
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[4]
[5]
[6]
\end{lstlisting}
\par

        Now the magic.
      %
\begin{lstlisting}[style=sageinput]
U.transpose()*V
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
32
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
V.transpose()*U
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
32
\end{lstlisting}
\par

        That is the dot product, but stuffed into a \(1\times 1\) matrix!
      %
\typeout{************************************************}
\typeout{Subsubsection  Other Properties}
\typeout{************************************************}
\subsubsection[Other Properties]{Other Properties}\label{subsubsection-33}

        The transpose has other useful properties. Strang lists the big ones,
        including how the transpose interacts with matrix multiplication and
        matrix inverses.
      %
\typeout{************************************************}
\typeout{Subsubsection  Symmetry}
\typeout{************************************************}
\subsubsection[Symmetry]{Symmetry}\label{subsubsection-34}

        A matrix is called \terminology{symmetric} when it is equal to its
        transpose. Sage has some built-in commands for this.
      %
\begin{lstlisting}[style=sageinput]
B = M([2,1,0,1,1,0,0,0,1])
B
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[2 1 0]
[1 1 0]
[0 0 1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
B.transpose()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[2 1 0]
[1 1 0]
[0 0 1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
B.is_symmetric()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
C = M([1,0,1,1,1,1,0,0,0]); C
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 0 1]
[1 1 1]
[0 0 0]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
C.is_symmetric()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
False
\end{lstlisting}
\par

        Strang notes a really neat property of symmetric matrices. Their
        \(LDU\) decompostions are nicer than average.
      %
\begin{lstlisting}[style=sageinput]
P, L, U = B.LU(pivot='nonzero')
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
P # here, things are good and no row swaps are needed
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 0 0]
[0 1 0]
[0 0 1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
L
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[  1   0   0]
[1/2   1   0]
[  0   0   1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
U
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[  2   1   0]
[  0 1/2   0]
[  0   0   1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
D = M([2,0,0,0,1/2,0,0,0,1])
Uprime = D.inverse()*U
Uprime
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[  1 1/2   0]
[  0   1   0]
[  0   0   1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
B == L*D*Uprime
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
L.transpose() # this is the neat part
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[  1 1/2   0]
[  0   1   0]
[  0   0   1]
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsubsection  Permutations and Pivots}
\typeout{************************************************}
\subsubsection[Permutations and Pivots]{Permutations and Pivots}\label{subsubsection-35}

        We have seen that elimination sometimes requires us to perform a row
        operation of swapping the position of two rows to put a pivot in a good
        place. At first, we want to do this to avoid a zero. But for computational
        reasons, a machine really likes to have a \emph{big} number as a pivot.
        So software often uses rows swaps even when not strictly needed.
      %
\par

        If all we care about is finding the reduced row echelon form (rref),
        then this won't worry us. You do whatever operations you want, and the
        rref is always the same thing. But if we want to keep track with matrices,
        things get a little complicated.
      %
\par

        Here is the important stuff to remember:
        \begin{enumerate}
\item{}A row swap is performed by a permutation matrix. A permutation matrix
            is a matrix with exactly one \(1\) in each column and in each row.
            These matrices have the important property that their transposes and
            their inverses are equal. That is, if \(P\) is a permutation matrix,
            then \(P^T\) is equal to \(P^{-1}\). (Not every matrix with
            this extra property is a permutation matrix. Be careful.)
          \item{}
            It is possible to figure out what all of the row swaps should be, and
            then rearrange all of the amtrices in an LU decomposition routine.
            If you do it correctly, you get:
            \[
              P'A = LU
            \]
            or
            \[
              A = PLU
            \]
            where \(P'\) and \(P\) are permutation matrices.
          \end{enumerate}

      %
\par

        Note: Strang prefers to write things as \(P'A = LU\), but Sage writes
        \(A = PLU\). Fortunately, there is a simple relationship here. Strang's
        \(P'\) is the transpose (and hence the inverse!) of Sage's \(P\).
      %
\par

        If you haven't figured it out by now, I think that row reduction by hand
        is really for chumps. Sage (or whatever computational tool you use) makes
        it waaaaaaaaay easier.
      %
\begin{lstlisting}[style=sageinput]
# using 'partial pivoting' where we get "big pivots"
P, L, U = A.LU()
x = '{0!r}\n\n{1!r}\n\n{2!r}'.format(P,L,U)
print x # fancy python tricks for readable display
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
          [0 1 0]
          [0 0 1]
          [1 0 0]

          [  1   0   0]
          [1/7   1   0]
          [4/7 1/2   1]

          [   7    8    9]
          [   0  6/7 12/7]
          [   0    0    0]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
P*L*U
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 2 3]
[4 5 6]
[7 8 9]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A == P*L*U
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
P.transpose()*A == L*U
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
P.transpose()*A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[7 8 9]
[1 2 3]
[4 5 6]
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-55}

      Keep this in mind. The computations are simple, but tedious.
      Perhaps you want to use an appropriate tool.
    %
\begin{task}
\label{task-76}

        Find an example of a matrix \(A\) such that \(A^T A = 0\),
        but \(A \neq 0\).
      \end{task}
\begin{task}
\label{task-77}

        These are true or false questions. If the statement is true, explain why
        you know it is true. If the statement is false, give an example that
        shows it is false.
        \begin{enumerate}
\item{}
            The block matrix \(\left( \begin{smallmatrix} A & 0 \\
            0 & A \end{smallmatrix}\right)\) is automatically symmetric.
          \item{}
            If \(A\) and \(B\) are symmetric, then their product \(AB\)
            is symmetric.
          \item{}
            If \(A\) is not symmetric, then \(A^{-1}\) is not symmetric.
          \end{enumerate}
\end{task}
\begin{task}
\label{task-78}

        If \(P_1\) and \(P_2\) are permuation matrices, then so is
        \(P_1P_2\). Give examples with \(P_1P_2 \neq P_2P_1\) and
        \(P_3P_4 = P_4P_3\).
      \end{task}
\begin{task}
\label{task-79}

        Explain the following phenomena in terms of row operations.
        \begin{enumerate}
\item{}
            For any permutation matrix \(P\), it is the case that
            \(P^T P = I\).
          \item{}
            All row exchange matrices are symmetric: \(P^T = P\).
            (other permutation matrices may or may not be symmetric.)
          \item{}
            If \(P\) is a row exchange matrix, then \(P^2 = I\).
          \end{enumerate}
\end{task}
\begin{task}
\label{task-80}

        For each of the following, find an example of a \(2\times 2\)
        symmetric matrix with the given property:
        \begin{enumerate}
\item{} \(A\) is not invertible.\item{} \(A\) is invertible but cannot be factored into \(LU\).\item{}
            \(A\) can be factored into \(LDL^T\), but not into
            \(LL^T\) because \(D\) has negative entries.
          \end{enumerate}
\end{task}
\begin{task}
\label{task-81}

          This is a new factorization of \(A\) into \emph{triangular
          times symmetric}:
        %
\par

          Start with \(A = LDU\). Then \(A = B S\),
          where \(B = L\left(U^T\right)^{-1}\) and \(S = U^T D U\).
        %
\par

          Explain why this choice of \(B\) is lower triangular with
          \(1\)'s on the diagonal. Expain why \(S\) is symmetric.
        %
\end{task}
\typeout{************************************************}
\typeout{Section 2.9 Matrix Algebra: Going Further}
\typeout{************************************************}
\section[Matrix Algebra: Going Further]{Matrix Algebra: Going Further}\label{lin-eq-gf2}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-56}
\begin{itemize}[label=\textbullet]
\item{}
      Go back through the exercises in the last four sections of this chapter.
      Complete any items you
      did not complete the first time through. Prepare any that we have not
      discussed in class so that you will be ready to present them.
    \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion}
\typeout{************************************************}
\subsection[Discussion]{Discussion}\label{subsection-57}
Now we take a short break to revisit and consolidate the learning you
    have done so far. Revisit the reading and the exercises you have done in
    Chapter Two: Linear Equations. The important feature of this work should be
    learning to think about your own thinking. This sort of \terminology{meta-cognition}
    characterizes expert learners. Eventually, you want to be able to monitor
    your work at all times and recognize when you understand deeply and when
    you do not. This will allow you to self-correct.
  %
\par
To help you get started with meta-cognition, I listed learning goals in
    each section. To go further, you need to explicitly go through the process
    of reviewing what you can do and what you cannot. Here are some prompts to
    help you get started with this process.
    \begin{itemize}[label=\textbullet]
\item{}
        Review the learning goals from each section. Can you do the things
        described? Can you do them sometimes, or have you mastered them so you
        can do them consistently?
      \item{}
        Look through all of the tasks and go deeper into them. Can you
        connect each exercise to one of our pictures? Try to build a mental
        model of how the exercise and its solution work.
      \item{}
        If your first solution to an exercise involve a ``guess-and-check''
        approach, can you now complete the exercise in a \emph{purposeful}
        and systematic manner?
      \item{}
        Make a list of concepts or exercises that are not clear to you. Phrase
        each item in your list as a question, and make each question as
        specific as possible. Talk with fellow students or your
        instructor until you can answer your own questions.
      \end{itemize}

  %
\typeout{************************************************}
\typeout{Chapter 3 Vector Spaces and Subspaces}
\typeout{************************************************}
\chapter[Vector Spaces and Subspaces]{Vector Spaces and Subspaces}\label{chapter-subspaces}
\typeout{************************************************}
\typeout{Introduction  }
\typeout{************************************************}

      In the last chapter we took on the central problem of linear algebra: solving
      systems of linear equations. We built a lot of useful tools, but we only
      answered the questions in the case of \emph{square systems} where the
      number of variables is equal to the number of equations.
    %
\par

      Now it is time to solve the problem in general. As a reminder, the set of
      questions is this:
    %
\begin{itemize}[label=\textbullet]
\item{}
        What is the set of solutions to a given system of linear equations?
      \item{}
        When does a given system have solution, and when does it not?
      \item{}
        If there is a solution, how many solutions are there?
      \item{}
        What ways do we have of describing the collection of solutions?
      \item{}
        Is there a computationally effective way to find those solutions?
      \end{itemize}
\par

      We will run into the need for a few new tools, each of which has been
      hiding in the background in our work so far. For square systems, we could
      get away without this level of detail, but now we will need these concepts
      to clarify our work: \terminology{subspaces}, \terminology{rank} and
      \terminology{reduced row echelon form}, and \terminology{bases}.
    %
\par

      This chapter concludes with a description of the \emph{four fundamental subspaces}
      associated to a matrix. These will help us put together an understanding of
      the transformational model of an \(m\times n\) matrix as a function with
      domain \(\mathbb{R}^n\) and target \(\mathbb{R}^m\).
    %
\typeout{************************************************}
\typeout{Section 3.1 Subspaces of \(\mathbb{R}^n\)}
\typeout{************************************************}
\section[Subspaces of \(\mathbb{R}^n\)]{Subspaces of \(\mathbb{R}^n\)}\label{vector-spaces}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-58}
\begin{itemize}[label=\textbullet]
\item{}Read Chapter 3 section 1 of \emph{Strang}.\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-59}
Before class, a student should be able to:%
\begin{itemize}[label=\textbullet]
\item{}Identify the column space of a matrix.\item{}Decide if a set in \(\mathbb{R}^n\) is a subspace or not.\end{itemize}
\par
Some time after class, a student should be able to:%
\begin{itemize}[label=\textbullet]
\item{}
        Use the notion of column space to decide if a given linear system has a
        solution or not.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion: Vector Spaces and Subspaces}
\typeout{************************************************}
\subsection[Discussion: Vector Spaces and Subspaces]{Discussion: Vector Spaces and Subspaces}\label{subsection-60}

      The key concepts in this section are those of a \terminology{vector space}
      and of a \terminology{subspace}. The basic idea is that a vector space is
      a kind of place where the basic operations involved in a linear
      combination make sense. There is a set of rules for being a vector space,
      but they are all aimed at the fact that there are two operations
      (addition and scalar multiplication) and we can form linear combinations
      with them that make sense.
    %
\par

      The biggest thing is that we possibly enlarge the kinds of things we
      call ``vectors.'' To a professional mathematician, a vector is
      anything that is an element of some vector space under consideration.
      My favorites are things like this:
    %
\begin{itemize}[label=\textbullet]
\item{}
        The set \(M_{2,2}\) of \(2\times 2\) matrices is a vector space.
        But now the things we call vectors are actually matrices.
      \item{}
        The set \(\mathcal{C}(\mathbb{R})\) of continuous functions with
        domain and range both equal to the set of real numbers is a vector
        space. But now the things we call vectors are actually functions.
      \item{}
        The set \(\ell(\mathbb{R})\) of sequences
        \((x_1, x_2, x_3, x_4, \ldots)\) of real numbers is a vector space.
        But now the things we call vectors are actually whole infinite sequences.
      \end{itemize}
\par

      In more advanced mathematics, vector spaces are extremely important. They
      show up everywhere. But for us, we will mostly stick to the family of vector
      spaces \(\mathbb{R}^n\). It will be much more important for us to understand
      subspaces.
    %
\par

      The idea of a subspace is some subset, some part, of a vector space which
      is a vector space in its own right. The prototype is the \(xy\)-plane
      inside of \(\mathbb{R}^3\).
    %
\par

      For now, the most important subspaces we see will be derived from
      individual matrices. Our first example is the column space of a matrix
      \(A\). If \(A\) is an \(m\times n\) matrix, then the column
      space \(\mathrm{col}(A)\) of \(A\) is the collection of
      \(n\)-vectors which can be expressed as linear combinations of the
      columns of \(A\). This is our first exposure to the idea of a
      \terminology{span}. The column space of \(A\) is the subspace of
      \(\mathbb{R}\) spanned by the columns of \(A\).
    %
\typeout{************************************************}
\typeout{Subsection  Sage and subspaces}
\typeout{************************************************}
\subsection[Sage and subspaces]{Sage and subspaces}\label{subsection-61}
Sage allows you to construct different kinds of vector spaces. The most
      important is the standard vector space \(\mathbb{R}^n\).
    %
\begin{lstlisting}[style=sageinput]
V = VectorSpace(QQ, 3)
V
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of dimension 3 over Rational Field
\end{lstlisting}
\par

      This is like the \lstinline?MatrixSpace? construction we have seen before.
      \lstinline?V? is now the collection of all vectors of size \(3\) with
      rational numbers as entries.
    %
\par

      More important for us is the fact that Sage knows how to find the subspaces
      associated to matrices.
    %
\begin{lstlisting}[style=sageinput]
A = matrix(QQ, 2,3, [4,5,3,0,1,1])
A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[4 5 3]
[0 1 1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.column_space()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 2 and dimension 2 over Rational Field
Basis matrix:
[1 0]
[0 1]
\end{lstlisting}
\par

      Note that Sage gives information in terms of a basis. We will talk about
      this concept soon.
    %
\begin{lstlisting}[style=sageinput]
B = matrix(QQ, 3,3 , [1,2,3,4,5,6,7,8,9])
B
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 2 3]
[4 5 6]
[7 8 9]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
B.column_space()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 3 and dimension 2 over Rational Field
Basis matrix:
[ 1  0 -1]
[ 0  1  2]
\end{lstlisting}
\par

      This last bit here is interesting.
      Our original matrix has size \(2\times 3\), so the columns are \(3\)-vectors.
      But this basis is displayed as a matrix where the columns are size \(2\).
      What is going on?
    %
\par

      Sage prefers to display vectors as rows! This is a big interal preference.
      We just have to get over it. Well, we have to \emph{remember it} and get over it.
      To access the basis vectors and display them as columns,
      we will use the following:
    %
\begin{lstlisting}[style=sageinput]
B.column_space().basis()[0].column() # first basis vector
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1]
[ 0]
[-1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
B.column_space().basis()[1].column() # second basis vector
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[0]
[1]
[2]
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-62}
\begin{task}
\label{task-82}

        Find an example of a vector which is not in the column space of the
        matrix \[
        A = \begin{pmatrix} 2 & 1  \\ 1 & 1 \end{pmatrix},
        \] or explain why that is not possible.
      \end{task}
\begin{task}
\label{task-83}

        Find an example of a vector which is not in the column space of the
        matrix \[
        B = \begin{pmatrix} 3 & 2 \\ 6 & 4 \end{pmatrix},
        \] or explain why it is not possible.
      \end{task}
\begin{task}
\label{task-84}

        Let \(\mathcal{P}\) be the set of polynomials of degree 3 or less.
        Explain why \(\mathcal{P}\) is a vector space, or explain why it is not.
      \end{task}
\begin{task}
\label{task-85}

        Consider the vector space \(\mathbb{R}^2\). Explain why the following
        are not subspaces:
        \begin{itemize}[label=\textbullet]
\item{}The unit circle.\item{}The line \(x+y = 4\).\item{}The union of lines \(2x+3y = 0\) and \(x-y=0\).\item{}The first quadrant where \(x\geq 0\) and \(y\geq 0\).\end{itemize}
\end{task}
\begin{task}
\label{task-86}

        Let \(W\) be the set of functions \(f\) in
        \(\mathcal{C}(\mathbb{R})\) which satisfy the differential equation
        \(f''(x) = 0\). Decide if \(W\) is a subspace of
        \(\mathcal{C}(\mathbb{R})\), and explain your thinking.
      \end{task}
\begin{task}
\label{task-87}

        Design a \(2\times 3\) matrix whose column space does not contain the
        vector \(\begin{pmatrix} 4 \\ 4 \end{pmatrix}\), or explain why this is
        not possible.
      \end{task}
\typeout{************************************************}
\typeout{Section 3.2 The Nullspace}
\typeout{************************************************}
\section[The Nullspace]{The Nullspace}\label{nullspace}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-63}
\begin{itemize}[label=\textbullet]
\item{}Read chapter 3 section 2 of \emph{Strang}.\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-64}
Before coming to class, a student should be able to:%
\begin{itemize}[label=\textbullet]
\item{}Find the special solutions associated to a matrix.\item{}Compute the \terminology{reduced row echelon form} of a matrix.\item{}Identify the free variables and the pivot variables associated to
        a matrix.
      \item{}
        Describe the nullspace of a matrix with the ``simplest'' set of
        equations possible.
      \end{itemize}
\par
Some time after class, a student should be able to:%
\begin{itemize}[label=\textbullet]
\item{}Describe the complete solution to a homogeneous system of equations
        \(Ax = 0\).
      \item{}Give an argument for why the nullspace is a vector subspace.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion: The Nullspace, RREF, and Special Solutions}
\typeout{************************************************}
\subsection[Discussion: The Nullspace, RREF, and Special Solutions]{Discussion: The Nullspace, RREF, and Special Solutions}\label{subsection-65}

      Strang aims right at the heart of things in this section, and does not
      waste any space. Let me highlight a few things:
    %
\par

      Let \(A\) be an \(m\times n\) matrix. We do not require that \(A\) is a square
      matrix, that is, it may not be the case that \(m=n\). What we are most
      interested in is solving the equation \(Ax = 0\). Note that if \(A\) is not
      square, then the vectors \(x\) and \(0\) have different sizes.
    %
\par

      By the way, an equation like \(Ax=0\) where the right hand side is zero
      is called a \terminology{homogeneous equation}.
    %
\begin{itemize}[label=\textbullet]
\item{}The nullspace of \(A\) is the set of vectors \(x\) such that \(Ax=0\). It
        is a theorem that this is a vector subspace of \(\mathbb{R}^n\). It is a
        common to use the synonym \terminology{the kernel of \(A\)} in place of the
        terminology \terminology{the nullspace of \(A\).}
      \item{}The key to everything is to not mind that your matrix isn't square.
        Just do Gauss-Jordan elimination anyway. The end result is called the
        \terminology{reduced row echelon form} of the matrix, or RREF for short.
      \item{}Note that there is no discussion of using an augmented matrix in this
        section, even though we are solving systems of equations \(Ax=0\). This
        is because the vector on the right hand side is zero and will stay
        zero. There is nothing interesting to track!
      \item{}The RREF is good for several things. One that is often overlooked is
        that one can use it to rewrite the system of equations in a ``simplest
        form.'' I mean that the equations left over in the RREF are somehow
        the easiest equations to use to cut out the solution as an intersection
        of hyperplanes.
      \item{}In the RREF, all of the structure can be inferred from the splitting
        of the columns into two types: the pivot columns and the free columns.
        Since each column is associated to a variable in our system of linear
        equations (the columns hold coefficients!), it is also common to refer
        to pivot variables and free variables.
      \item{}The number of free columns basically determines the ``size'' of the
        nullspace. This is an entry point to the concept of the
        \terminology{dimension} of a vector space. We shall see this in more detail
        later.
      \item{}Strang points out an easy way to find some individual vectors in the
        null space: he calls these the \terminology{special solutions}. This is
        because they are solutions to the equation \(Ax=0\).
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Sage and the Nullspace}
\typeout{************************************************}
\subsection[Sage and the Nullspace]{Sage and the Nullspace}\label{subsection-66}

      The main idea in this section is to get solutions to homogeneous
      systems \(Ax=0\) by using the reduced row echelon form. This will
      compute the nullspace of the matrix \(A\). Sage has some useful built
      in commands for this. Let us explore an example.
    %
\begin{lstlisting}[style=sageinput]
entries = [1,2,3,4,5,6,7,8,9,10,11,12]
A = matrix(QQ, 3,4, entries); A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1  2  3  4]
[ 5  6  7  8]
[ 9 10 11 12]
\end{lstlisting}
\par

      The most direct way is to ask Sage for the nullspace. But Sage doesn't
      call it that. It uses the synonym \terminology{kernel}. Also, because we
      do matrix vector multiplication with vectors on the right, we ahve to
      tell Sage to do it that way.
    %
\begin{lstlisting}[style=sageinput]
A.right_kernel()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 4 and dimension 2 over Rational Field
Basis matrix:
[ 1  0 -3  2]
[ 0  1 -2  1]
\end{lstlisting}
\par

      Notice that Sage returns the nullspace by specifying a basis. This is
      a complete set of special solutions! How can we be sure?
    %
\par

      Let's check that those vectors are actually in the nullspace. This command
      should return two zero vectors in \(\mathbb{R}^3\).
    %
\begin{lstlisting}[style=sageinput]
sage1 = vector([1,0,-3,2])
sage2 = vector([0,1,-2,1])
A*sage1, A*sage2
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
((0, 0, 0), (0, 0, 0))
\end{lstlisting}
\par

      Remember that the method to find the reduced row
      echelon form is .rref().
    %
\begin{lstlisting}[style=sageinput]
A.rref()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1  0 -1 -2]
[ 0  1  2  3]
[ 0  0  0  0]
\end{lstlisting}
\par

      What this means is that the null space is described by the two equations
      \[
        \left\{
        \begin{array}{rrrrrrrrr}
        v & & & - & y & - & 2 z & = & 0 \\
          & & x & + & 2y & + & 3 z & = & 0
        \end{array}\right.
      \]
      That is, the nullspace is the intersection of these two hyperplanes in
      \(\mathbb{R}^4\).
    %
\begin{lstlisting}[style=sageinput]
# Let's see what Sage's solve command tells us:
v,x,y,z = var('v x y z')
expr1 = v - y - 2*z == 0
expr2 = x + 2*y + 3*z == 0
solve([expr1, expr2], [v,x,y,z])
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[[v == 2*r5 + r6, x == -3*r5 - 2*r6, y == r6, z == r5]]
\end{lstlisting}
\par

      Aha! Sage is trying to tell us that the pivot variables \(v\)
      and \(x\) should be
      written in terms of the free variables \(y\) and \(z\).
      Strang's "easy way out" is to choose the free variables to be
      \(0\)'s and \(1\)'s in
      combination. Here we have two of them, so we will alternate.
      One special solution will be to choose \(y = 1\) and \(z=0\).
      Then we solve to get \(v = 1\) and \(x = -2\).
    %
\begin{lstlisting}[style=sageinput]
special1 = vector([1,-2,1,0])

# let's check
A*special1
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(0, 0, 0)
(0, 0, 0)
\end{lstlisting}
\par

      Similarly, we can choose \(y=0\) and
      \(z=1\) to get a second special solution. This leads to
      \(v=2\) and \(x = -3\).
    %
\begin{lstlisting}[style=sageinput]
special2 = vector([2,-3,0,1])

#let's check
A*special2
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(0, 0, 0)
\end{lstlisting}
\par

      These two vectors form a \terminology{basis} for the nullspace. Every
      vector in the nullspace can be written as a linear combination of
      these two vectors.
    %
\par

      What is weird is that Sage always wants its basis to look
      like \(1\)'s and \(0\)'s at the beginning, and our process makes
      them look like that at the end!
    %
\par

      How can we be sure everything lines up?  Well, it is possible to
      express the two vectors that Sage gives us as a linear combination
      of the ones we just found, and vice versa. So we are getting \emph{two}
      descriptions of the same space.
    %
\begin{lstlisting}[style=sageinput]
sage1 == -3* special1 + 2* special2
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
sage2 == -2* special1 + special2
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
special1 ==  sage1 - 2* sage2
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
special2 == 2* sage1 - 3* sage2
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\par

      Can you see the matrix and its inverse hiding behind that? I found those
      relationships using row operations and a matrix inverse.
    %
\par

      Anyway, what has happened is that Sage has performed our calculations above,
      and then taken the extra steps of putting the matrix of basis vectors into
      reduced row echelon form, too.
    %
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-67}

      For the first three exercises, your job is to find both
      \begin{itemize}[label=\textbullet]
\item{}The minimal set of equations which cuts out the nullspace of the
          given matrix as an intersection of hyperplanes; and
        \item{}the size of the nullspace, expressed through the number
          ``independent directions'' it contains.
        \end{itemize}

      As always, it will be crucial to explain how you know you are correct.
    %
\begin{task}
\label{task-88}

        Consider the matrix
        \[
          A = \begin{pmatrix}
          1 & 2 & 3 \\
          4 & 8 & 5 \\
          -1 & -2 & 0
          \end{pmatrix}
        \]\end{task}
\begin{task}
\label{task-89}

      Consider the matrix
      \[
        A = \begin{pmatrix}
        2 & 1 & 3 & 7 \\
        1 & 1 & 1 & -3
        \end{pmatrix}
      \]\end{task}
\begin{task}
\label{task-90}

        Consider the matrix
        \[
          A = \begin{pmatrix}
          2 & 3 & 5 & 6 \\
          1 & 1 & 2 & 3 \\
          0 & 1 & 1 & 0 \\
          -1 & -1 & -2 & -3 \\
          1 & 1 & 2 & 3
          \end{pmatrix}
        \]\end{task}
\par

      For the next three exercises, your job is to find a complete set of
      special solutions to the homogeneous equation \(Ax=0\).
      By a ``complete set,'' we mean ``enough special solutions so that
      any vector in the nullspace of \(A\) can be expressed as a linear
      combination of your vectors.''
    %
\par

      As always, it will be crucial to explain how you know you are correct.
    %
\begin{task}
\label{task-91}

        Consider the matrix
        \[
          A = \begin{pmatrix}
          6 & 7 \\
          7 & 8 \\
          1 & 0 \\
          4 & 5
          \end{pmatrix}
        \]\end{task}
\begin{task}
\label{task-92}

        Consider the matrix
        \[
          A = \begin{pmatrix}
          23 & 17 & 9 & 2 \\
          1 & -2 & 4 & 1 \\
          22 & 19 & 5 & 1
          \end{pmatrix}
        \]\end{task}
\begin{task}
\label{task-93}

        Consider the matrix
        \[
          A = \begin{pmatrix}
          -3 & -6 & -9 & -12 \\
          1 & 2 & 3 & 4 \\
          1 & 4 & 9 & 16 \\
          1 & 8 & 27 & 64 \\
          -1 & -1 & -1 & -1 \\
          0 & 0 & 1 & 0
          \end{pmatrix}
        \]\end{task}
\typeout{************************************************}
\typeout{Section 3.3 Rank and the RREF}
\typeout{************************************************}
\section[Rank and the RREF]{Rank and the RREF}\label{rank}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-68}
\begin{itemize}[label=\textbullet]
\item{}Read chapter 3 section 3 of \emph{Strang}.\item{}Read the following and do the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-69}
Before class, a student should be able to:%
\begin{itemize}[label=\textbullet]
\item{}State a definition of the rank of a matrix.\item{}Compute the rank of a matrix.\item{}Describe the relation between the rank and the size of the column
        space.
      \item{}Construct the nullspace matrix associated to a matrix.\end{itemize}
\par
Some time after class, a student should be able to:%
\begin{itemize}[label=\textbullet]
\item{}Use the RREF and special solutions associated to a matrix to describe
        how free columns are linear combinations of earlier pivot columns.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion: The Rank of a Matrix}
\typeout{************************************************}
\subsection[Discussion: The Rank of a Matrix]{Discussion: The Rank of a Matrix}\label{subsection-70}

      The new concept for this section is the \terminology{rank} of a matrix.
      There are \emph{three} definitions of rank given in the first two
      pages of this section. It is important to know them and see that they
      are equivalent.
    %
\par

      Another important part of what happens here is the realization that
      putting the matrix into reduced row echelon form, which uses \emph{row}
      operations, actually tells us something about the \emph{columns} of the
      matrix. This is kind of amazing.
    %
\par

      Take a matrix \(A\) and put it into reduced row echelon form
      \(R = \mathrm{rref}(A)\). Then \(A\) and \(R\) have the same shape.
      In particular, they have the same number of columns. But in \(R\)
      it is easy to see the location of the pivots. This divides up the
      columns of \(R\) into those columns having a pivot, the \terminology{pivot columns},
      and those which do not, the \terminology{free columns}.
      We will label a column of \(A\) as a pivot column or a free column in a way
      that corresponds. Here is the startling fact:
    %
\begin{theorem}\label{theorem-3}

        Let \(A\) be a matrix.
        The pivot columns of \(A\) are those which cannot be expressed as linear
        combinations of the columns to their left.
        The free columns of \(A\) are those which can be expressed as linear
        combinations of the columns to their left.
      \end{theorem}
\par

      How exactly can we see this? Strang uses the construction of a
      \terminology{nullspace matrix}. The special solutions we learned about
      in the last section can be bundled together as the columns of a matrix
      \(N\), called the nullspace matrix. This matrix has two important
      properties:
    %
\begin{itemize}[label=\textbullet]
\item{} \(AN = 0\); and\item{} Each column of \(N\) (i.e. each special solution) holds the
          coefficients needed to write down a interesting linear combination
          equation on the columns of \(A\).
        \end{itemize}
\par

        Finally, please use some caution when reading through the area
        with blue boxes and equations (4) and (5) on page 147. Note that
        Strang introduces a big simplifying assumption that makes his work
        easier. The general principles will hold, but those nice, neat
        equations won't always look so good for an arbitrary matrix.
      %
\typeout{************************************************}
\typeout{Subsection  Sage and the Rank}
\typeout{************************************************}
\subsection[Sage and the Rank]{Sage and the Rank}\label{subsection-71}

      Sage has a built-in command for the rank of a matrix. It is called
      \lstinline?.rank()?, of course.
    %
\begin{lstlisting}[style=sageinput]
A = matrix(QQ, 3,4, [1,2,3,4,5,6,7,8,9,10,11,12])
A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1  2  3  4]
[ 5  6  7  8]
[ 9 10 11 12]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.rank()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
2
\end{lstlisting}
\par

      Sage knows how to do the underlying computations, too. Let's have Sage
      compute the reduced row echelon form of \lstinline?A?:
    %
\begin{lstlisting}[style=sageinput]
A.rref()
\end{lstlisting}
\par

      Nice! Note that this fits the ``special case'' form that Strang uses on
      page 147. Let's see what else it can do.
    %
\begin{lstlisting}[style=sageinput]
A.right_kernel()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 4 and dimension 2 over Rational Field
Basis matrix:
[ 1  0 -3  2]
[ 0  1 -2  1]
\end{lstlisting}
\par

      We can make Sage give us just the matrix in that last response.
    %
\begin{lstlisting}[style=sageinput]
A.right_kernel().basis_matrix()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1  0 -3  2]
[ 0  1 -2  1]
\end{lstlisting}
\par

      That is \emph{almost} Strang's version of a nullspace matrix in this case.
      Part of what we need to do is make the rows into columns. The transpose command is
      the way to do that:
    %
\begin{lstlisting}[style=sageinput]
N = A.right_kernel().basis_matrix().transpose()
N
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1  0]
[ 0  1]
[-3 -2]
[ 2  1]
\end{lstlisting}
\par

      That is \emph{NOT} exactly Strang's nullspace matrix. But it is very close.
      In fact, it still has this crucial property:
    %
\begin{lstlisting}[style=sageinput]
A*N
\end{lstlisting}
\par

      What has happened here is that Sage constructs a \emph{different} version
      of the nullspace matrix than Strang does. Strang's version is easier to
      find by hand and tends to have \(1\)'s and \(0\)'s and the end.
      The Sage version comes from a routine that strongly prefers to
      start with \(1\)'s and \(0\)'s.
    %
\par

      The two version of nullspace matrix are related, of course. Let's see how:
    %
\begin{lstlisting}[style=sageinput]
E = N.matrix_from_rows([2,3]); E
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[-3 -2]
[ 2  1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
F = N*E.inverse(); F
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1  2]
[-2 -3]
[ 1  0]
[ 0  1]
\end{lstlisting}
\par

      \emph{That} is Strang's nullspace matrix.
    %
\begin{lstlisting}[style=sageinput]
A*F
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[0 0]
[0 0]
[0 0]
[0 0]
\end{lstlisting}
\par

      Where does this relationship come from? I have a hint: \emph{Column Operations}.
      How can we use the structure of matrix multiplication to see why left multiplication
      by \lstinline?E.inverse()? performs some column operations?
    %
\par

      Here is another thing to think about: where does that \lstinline?E? come from?
      Compare \lstinline?A? and \lstinline?N?. Notice anything? Remember that Strang wants
      the nullspace matrix to have \(1\)'s and \(0\)'s in the free columns.
    %
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-72}
\begin{task}
\label{task-94}

        Give an example of a \(5 \times 3\) matrix which has
        \begin{itemize}[label=\textbullet]
\item{}rank equal to 3, and\item{}all non-zero entries,\end{itemize}

        or explain why no such example is possible.
      \end{task}
\begin{task}
\label{task-95}

        Give an example of a \(3 \times 3\) matrix which has
        \begin{itemize}[label=\textbullet]
\item{}rank equal to 2,\item{}a nullspace of \(\{0\}\), and\item{}all non-zero entries,\end{itemize}

        or explain why no such example is possible.
      \end{task}
\begin{task}
\label{task-96}

        Consider the matrix \[
        A = \left( \begin{smallmatrix} 2 & 1 & 5 \\ 1 & 1
        & 7 \end{smallmatrix}\right).\]
        Find the reduced row echelon form \(R = \mathrm{rref}(A)\)
        of \(A\). Track the row operations you use, and use them to
        find an invertible matrix \(E\) so that \(EA = R\).
      \end{task}
\begin{task}
\label{task-97}

        Contine with the matrix of the last exercise. Find the null space matrix of
        \(A\). Use the information contained in the nullspace matrix to
        write down a linear combination equation on the columns of
        \(R = \mathrm{rref}(A)\) of the form
        \[
          a * \text{(column1)} + b * \text{(column2)} + c * \text{(column3)} = 0.
        \]
        Explain why the matrix \(E\) allows us to translate this equation
        into this one on the columns of \(A\):
        \[
          a\begin{pmatrix} 2 \\ 1 \end{pmatrix} +
          b \begin{pmatrix} 1 \\ 1 \end{pmatrix} +
          c \begin{pmatrix} 5 \\ 7 \end{pmatrix} =
          \begin{pmatrix}0\\ 0 \end{pmatrix}.
        \]\end{task}
\begin{task}
\label{task-98}

        Consider the matrix \(N\) given below. Make an example of two
        different matrices \(A\) and \(B\) which have different shapes
        and each have \(N\) as nullspace matrix, or explain why such an
        example is not possible.
        \[
        N = \begin{pmatrix}
        -3 & 2 & -1 \\
         1 & 0 &  0 \\
         0 & 1 &  0 \\
         2 & 6 &  1 \\
         0 & 0 &  1
        \end{pmatrix}
        \]\end{task}
\begin{task}
\label{task-99}

        Consider the matrix \(T = \left(\begin{smallmatrix} 2 &
        1 \\ 1 & 1 \end{smallmatrix}\right)\). What is the nullspace
        matrix of \(T\)?
      \end{task}
\typeout{************************************************}
\typeout{Section 3.4 Solving a System}
\typeout{************************************************}
\section[Solving a System]{Solving a System}\label{complete-solution}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-73}
\begin{itemize}[label=\textbullet]
\item{}Read chapter 3 section 4 of Strang.\item{}Read the following and complete the tasks below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-74}

      Before class, a student should be able to:
    %
\begin{itemize}[label=\textbullet]
\item{}
        Identify a particular solution to a matrix-vector equation \(Ax=b\).
        (Provided there is one.)
      \item{}
        Find the complete solution to a matrix-vector equation \(Ax=b\)
        as a parametrized object. (Provided there is one.)
      \end{itemize}
\par

      After class, a student should be able to:
    %
\begin{itemize}[label=\textbullet]
\item{}
        Describe the complete solution to a matrix-vector equation \(Ax=b\)
        as an implicit object, cut out by equations.
      \item{}
        Describe the possibilities for the number of solutions to a matrix-vector
        equation \(Ax=b\) in terms of the shape of the matrix.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  
      Discussion: The Complete Solution to a System of Equations
    }
\typeout{************************************************}
\subsection[
      Discussion: The Complete Solution to a System of Equations
    ]{
      Discussion: The Complete Solution to a System of Equations
    }\label{subsection-75}

      This is the big day! We finally learn how to write out the general
      solution to a system of linear equations. We have spent so much time
      understanding things related to this, that it should go pretty quickly.
    %
\par

      The tiny little facts underneath the analysis for this section are
      these: For a matrix \(A\), vectors \(v\) and \(w\) and a scalar
      \(\lambda\), all chosen so that the equations make any sense,
      \[
        \begin{array}{rcl}
        A(v+w) &= &Av + Aw \\
        A(\lambda v) &= &\lambda ( Av )
        \end{array}
      \]
    %
\par

      The first is a kind of \terminology{distributive property}, and the second
      is a kind of \terminology{commutative property}. When taken together, these
      things say that the operation of ``left-multiply by the matrix \(A\)''
      is a special kind of function. The kind of function here is important
      enough that we have a special word for this combined property: it is
      called \terminology{linearity}. That is, left-multiplication by \(A\) is a
      \terminology{linear operation} or a \terminology{linear transformation}.
    %
\par

      The linearity property makes it possible to check the following two results.
    %
\begin{theorem}\label{theorem-4}

        Let \(Ax=b\) be a system of linear equations, and let \(Ax=0\) be
        the associated homogeneous system. If \(x_p\) and \(x_p'\) are
        two particular solutions to \(Ax=b\), then \(x_p - x_p'\) is a
        solution to the homogeneous system \(Ax=0\).
      \end{theorem}
\begin{theorem}\label{theorem-5}

        Let \(Ax=b\) be a system of linear equations, and let \(Ax=0\)
        be the associated homogeneous system.
        If \(x_p\) is some particular solution to \(Ax=b\) and
        \(x_n\) is some solution to \(Ax=0\), then \(x_p + x_n\) is
        another solution to \(Ax=b\).
      \end{theorem}
\par

      And if we put these two theorems together, we find this result which
      sounds fancier, but has exactly the same content.
    %
\begin{theorem}\label{theorem-6}

        The complete set of solutions to the system \(Ax=b\) is the set
        \[
          \left\{ x_p + x_n \mid x_n \in \mathrm{null}(A) \right\},
        \]
        where \(x_p\) is any one particular solution to \(Ax=b\).
      \end{theorem}
\par

      This leads us to Strang's very sensible advice about finding the
      complete solution:
    %
\begin{itemize}[label=\textbullet]
\item{}
        Form the augmented matrix \(\left( A \mid b \right)\) and use
        Gauss-Jordan elimination to put it in reduced row echelon form
        \(\left( R \mid d \right)\).
      \item{}
        Use the information from the RREF to find a particular solution
        \(x_p\) by solving for the pivot variables from the vector
        \(d\) and setting the free variables to zero.
      \item{}
        Use the special solutions \(s_1, s_2, \dots, s_k\)
        (if any exist!) to describe the nullspace \(\mathrm{null}(A)\).
      \item{}
        Write down the resulting general solution:
        \[
          x = x_p + a_1 s_1 + a_2 s_2 + \dots + a_k s_k,
          \quad \text{for any scalars } a_i \in \mathbb{R}.
        \]
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Sage and Solving General Systems}
\typeout{************************************************}
\subsection[Sage and Solving General Systems]{Sage and Solving General Systems}\label{subsection-76}

      Sage has many built-in methods for solving systems of linear equations.
      We will investigate three common ones with a single example considered
      several times.
    %
\begin{lstlisting}[style=sageinput]
A = matrix(QQ, 3,4, [1,0,2,3, 1,3,2,0, 2,0,4,9])
b = vector([2,5,10])
print A
print b
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 0 2 3]
[1 3 2 0]
[2 0 4 9]
(2, 5, 10)
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsubsection  Method One: RREF and the Nullspace}
\typeout{************************************************}
\subsubsection[Method One: RREF and the Nullspace]{Method One: RREF and the Nullspace}\label{subsubsection-36}

        First we find a particular solution.
      %
\begin{lstlisting}[style=sageinput]
X = A.augment(b, subdivide=True).rref()
X  # this subdivide thing is pretty handy!
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1  0  2  0|-4]
[ 0  1  0  0| 3]
[ 0  0  0  1| 2]
\end{lstlisting}
\par

        This clearly has three pivots, and all belong in the original matrix.
        So there will be a solution. We pull out the particular solution.
      %
\begin{lstlisting}[style=sageinput]
xp = vector([-4, 3, 0, 2])
xp
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(-4, 3, 0, 2)
\end{lstlisting}
\par

        Since we typed that in by hand, we should check our work.
      %
\begin{lstlisting}[style=sageinput]
A*xp == b
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\par

        Now we need to find the nullspace and the special solutions.
      %
\begin{lstlisting}[style=sageinput]
A.right_kernel()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 4 and dimension 1 over Rational Field
Basis matrix:
[   1    0 -1/2    0]
\end{lstlisting}
\par

        The basis has only one row, so there is only one special solution.
        This matches our expectation. Our system is \(3\times 4\) and has
        rank \(3\). So there is only one free column, and hence only one
        special solution.
      %
\begin{lstlisting}[style=sageinput]
s1 = vector([-2, 0, 1, 0])
A*s1 == 0
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\par

        Now we can check the ``general solution''.
      %
\begin{lstlisting}[style=sageinput]
t = var('t')
gensol = xp + t * s1
gensol
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(-2*t - 4, 3, t, 2)
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A*gensol == b
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsubsection  Method Two: A Sage built-in}
\typeout{************************************************}
\subsubsection[Method Two: A Sage built-in]{Method Two: A Sage built-in}\label{subsubsection-37}

        Sage has a built-in method that looks like ``Matrix division''. Here
        we ``left divide'' by the matrix. This is odd notation, and is just
        something Sage allows.
      %
\begin{lstlisting}[style=sageinput]
A\b
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(-4, 3, 0, 2)
\end{lstlisting}
\par

        It is weird, but this works even if \lstinline?A? is not invertible, like now.
      %
\begin{lstlisting}[style=sageinput]
A.inverse()*b
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Error in lines 2-2
Traceback (most recent call last):
...
ArithmeticError: self must be a square matrix
\end{lstlisting}
\par

        The downside to this particular method is that in only gives you one
        particular solution. It does not produce the complete solution. You have
        to do that bit for yourself, maybe like the above.
      %
\typeout{************************************************}
\typeout{Subsubsection  Method Three: Another Sage Built-in}
\typeout{************************************************}
\subsubsection[Method Three: Another Sage Built-in]{Method Three: Another Sage Built-in}\label{subsubsection-38}

        Finally, Sage will also try to solve the system if you apply the
        \lstinline?.solve_right()? method to \lstinline?A?. You have to supply the vector
        \lstinline?b? as an argument to the command.
      %
\begin{lstlisting}[style=sageinput]
A.solve_right(b)
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
(-4, 3, 0, 2)
\end{lstlisting}
\par

        Again, this only pulls out a single particular solution. It is up to you
        to figure out the rest.
      %
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-77}
\begin{task}
\label{task-100}
(Strang ex 3.4.4)
        Find the complete solution (also called the general solution) to
        \[
          \begin{pmatrix} 1 & 3 & 1 & 2 \\
          2 & 6 & 4 & 8 \\
          0 & 0 & 2 & 4 \end{pmatrix}
          \begin{pmatrix} x \\ y \\ z \\ t \end{pmatrix} =
          \begin{pmatrix} 1 \\ 3 \\ 1 \end{pmatrix}.
        \]\end{task}
\begin{task}
\label{task-101}
(Strang ex 3.4.6)
        What conditions on \(b_1\), \(b_2\), \(b_3\) and \(b_4\) make
        each of these systems solvable? Find a solution in those cases.
        \begin{enumerate}
\item{}
            \[
              \begin{pmatrix} 1 & 2 \\ 2 & 4 \\ 2 & 5 \\ 3 & 9 \end{pmatrix}
              \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} =
              \begin{pmatrix} b_1 \\ b_2 \\ b_3 \\ b_4 \end{pmatrix}.
            \]
          \item{}
            \[
              \begin{pmatrix} 1 & 2 & 3\\ 2 & 4 & 6\\
              2 & 5 & 7\\ 3 & 9 & 12\end{pmatrix}
              \begin{pmatrix} x_1 \\ x_2 \\ x_3\end{pmatrix} =
              \begin{pmatrix} b_1 \\ b_2 \\ b_3 \\ b_4 \end{pmatrix}.
            \]
          \end{enumerate}
\end{task}
\begin{task}
\label{task-102}
(Strang ex 3.4.11)
        It is impossible for a \(1 \times 3\) system of equations to have
        \(x_p = (2,4,0)\) and \(x_n = \text{ any multiple of } (1,1,1)\).
        Explain why.
      \end{task}
\begin{task}
\label{task-103}
(Strang ex 3.4.13)
        Each of the statments below is false. Find a \(2\times 2\) counterexample
        to each one.
        \begin{enumerate}
\item{}
            The complete solution is any linear combination of \(x_p\) and
            \(X_n\).
          \item{}
            A system \(Ax=b\) has at most one particular solution.
          \item{}
            The solution \(x_p\) with all free variables zero is the shortest
            solution, in that it has the minimum norm \(||x_p||\).
          \item{}
            If \(A\) is an invertible matrix, there is no solution \(x_n\)
            in the nullspace.
          \end{enumerate}
\end{task}
\begin{task}
\label{task-104}
(Strang ex 3.4.21)
        Find the complete solution in the form \(x_p + x_n\) to these full
        rank systems.
        \begin{enumerate}
\item{}
            \[ x+y+z = 4\]
          \item{}
            \[
              \begin{array}{ccccccc}
              x & + & y & + & z & = & 4 \\
              x & - & y & + & z & = & 4
              \end{array}
            \]
          \end{enumerate}
\end{task}
\begin{task}
\label{task-105}
(Strang ex 3.4.24)
        Give examples of matrices \(A\) for which the number of solutions
        to \(Ax = b\) is
        \begin{enumerate}
\item{}\(0\) or \(1\), depending on \(b\);\item{}\(\infty\), regardless of \(b\);\item{}\(0\) or \(\infty\), depending on \(b\);\item{}\(1\), regardless of \(b\).\end{enumerate}
\end{task}
\begin{task}
\label{task-106}
(Strang ex 3.4.31)
        Find examples of matrices with the given property, or explain why it is
        impossible:
        \begin{enumerate}
\item{}
            The only solution of \(Ax = \left(\begin{smallmatrix}
            1 \\ 2 \\ 3 \end{smallmatrix}\right)\) is \(x = \left(\begin{smallmatrix}
            0 \\ 1 \end{smallmatrix}\right)\).
          \item{}
            The only solution of \(Bx = \left(\begin{smallmatrix}
            0 \\ 1 \end{smallmatrix}\right)\) is
            \(x = \left(\begin{smallmatrix}
            1 \\ 2 \\ 3 \end{smallmatrix}\right)\).
          \end{enumerate}
\end{task}
\begin{task}
\label{task-107}
(Strang ex 3.4.33)
        The complete solution to the equation \(Ax = \left(\begin{smallmatrix}
        1 \\ 3\end{smallmatrix}\right)\) is \(x = \left(\begin{smallmatrix}
        1 \\ 0\end{smallmatrix}\right) +
        c\left(\begin{smallmatrix}0\\ 1 \end{smallmatrix}\right)\).
        Find the matrix \(A\). Write the set of equations that corresponds to
        \(Ax = b\). (This is the \emph{implicit} description of this set!)
      \end{task}
\typeout{************************************************}
\typeout{Section 3.5 Going Further with Solving Systems}
\typeout{************************************************}
\section[Going Further with Solving Systems]{Going Further with Solving Systems}\label{soln-gf}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-78}
\begin{itemize}[label=\textbullet]
\item{}
        Go back through the exercises in the first four sections of this chapter.
        Complete any items you
        did not complete the first time through. Prepare any that we have not
        discussed in class so that you will be ready to present them.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion}
\typeout{************************************************}
\subsection[Discussion]{Discussion}\label{subsection-79}
Now we take a short break to revisit and consolidate the learning you
      have done so far. Revisit the reading and the exercises you have done in
      the first four sections of Chapter Three: Vector Space and Subspaces. The
      important feature of this work should be
      learning to think about your own thinking. This sort of \terminology{meta-cognition}
      characterizes expert learners. Eventually, you want to be able to monitor
      your work at all times and recognize when you understand deeply and when
      you do not. This will allow you to self-correct.
    %
\par
To help you get started with meta-cognition, I listed learning goals in
      each section. To go further, you need to explicitly go through the process
      of reviewing what you can do and what you cannot. Here are some prompts to
      help you get started with this process.
      \begin{itemize}[label=\textbullet]
\item{}
          Review the learning goals from each section. Can you do the things
          described? Can you do them sometimes, or have you mastered them so you
          can do them consistently?
        \item{}
          Look through all of the tasks and go deeper into them. Can you
          connect each exercise to one of our pictures? Try to build a mental
          model of how the exercise and its solution work.
        \item{}
          If your first solution to an exercise involve a ``guess-and-check''
          approach, can you now complete the exercise in a \emph{purposeful}
          and systematic manner?
        \item{}
          Make a list of concepts or exercises that are not clear to you. Phrase
          each item in your list as a question, and make each question as
          specific as possible. Talk with fellow students or your
          instructor until you can answer your own questions.
        \end{itemize}

    %
\typeout{************************************************}
\typeout{Section 3.6 Bases}
\typeout{************************************************}
\section[Bases]{Bases}\label{basis}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-80}
\begin{itemize}[label=\textbullet]
\item{}Read \emph{Strang} chapter 3 section 5.\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-81}
Before class, a student should be able to:%
\begin{itemize}[label=\textbullet]
\item{}Use the row space algorithm to decide if a set of vectors is
        linearly independent or linearly dependent.\item{}Use the column space algorithm to decide if a set of vectors is
        linearly independent or linearly dependent\item{}Compute the dimension of a subspace.\end{itemize}
\par
Sometime after class, a student should be able to:%
\begin{itemize}[label=\textbullet]
\item{}Explain the connection between a set of vectors being
      linearly independent, a spanning set, and a basis.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion: Linear Independence, Spanning, Basis, and Dimension}
\typeout{************************************************}
\subsection[Discussion: Linear Independence, Spanning, Basis, and Dimension]{Discussion: Linear Independence, Spanning, Basis, and Dimension}\label{subsection-82}

      The purpose of this lesson is to introduce specific terms for several
      concepts we have been dancing around. This is a spot that sometimes gives
      students difficulty because they are unused to the way mathematicians
      talk. So, here is a big warning:
    %
\begin{quote}
      
        ``When I use a word,'' Humpty Dumpty said in rather a scornful tone,
        ``it means just what I choose it to mean -- neither more nor less.''
      %

      \par

      --\emph{Through the Looking Glass}, Lewis Carroll%

    \end{quote}\par

      That is the essence of it. We have several new words, and they will
      mean \emph{exactly} what we declare them to mean, no more, no less.
    %
\par

      What are the new terms?
    %
\begin{itemize}[label=\textbullet]
\item{}
        A set \(\{v_1, v_2, \ldots, v_k\}\) of vectors is \terminology{linearly
        independent}, or \terminology{linearly dependent}.
      \item{}
        A set of vectors \terminology{spans} a vector space or a subspace.
      \item{}
        A set of vectors is a \terminology{basis} or not.
      \item{}
        The \terminology{dimension} of a vector space, or of a vector subspace.
      \end{itemize}
\par

      You should take away from this reading what those four terms are, how
      to check them, and some examples and non-examples.
    %
\par

      A bit of notation: If we have a set \(\{ v_1, v_2, \dots, v_k\}\) of
      vectors in some vector space, then we denote the subspace which they
      span by \(\mathrm{span}(\{v_1, v_2, \ldots, v_k\})\). (This has to
      be the easiest possible notational choice ever.)
    %
\typeout{************************************************}
\typeout{Subsubsection  Other Vector Spaces}
\typeout{************************************************}
\subsubsection[Other Vector Spaces]{Other Vector Spaces}\label{subsubsection-39}

        Once you understand these terms as they apply to the Euclidean
        spaces \(\mathbb{R}^n\) and their subspaces (especially those
        associated to matrices), you should pause to admire your achievement.
      %
\par

        But next, realize that those terms apply generally to all sorts of
        vector spaces! Can you make examples and non-examples in some of
        these other situtations?
      %
\begin{itemize}[label=\textbullet]
\item{}
          The set \(\mathcal{P}_3\) of polynomials in \(x\) of degree
          no more than 3?
        \item{}
          The set \(M_{m,n}\) of \(m\times n\) matrices.
        \item{}
          The set \(\mathcal{C}(\mathbb{R})\) of continuous functions.
        \item{}
          The set of all functions which are solutions to the differential
          equation \(y'' = y\).
        \end{itemize}
\typeout{************************************************}
\typeout{Subsubsection  Two Methods of Sorting out Linear Independence}
\typeout{************************************************}
\subsubsection[Two Methods of Sorting out Linear Independence]{Two Methods of Sorting out Linear Independence}\label{subsubsection-40}

        We have enough information to collect two ways to answer the question:
        ``Is this set linearly independent?''
        Well, at least when working with vectors in some Euclidean space
        \(\mathbb{R}^n\).
      %

        The Column Space Algorithm
        
          Given a set of vectors \(\{v_1, v_2, \ldots, v_k\}\) from
          \(\mathbb{R}^n\):
        %

        \begin{itemize}[label=\textbullet]
\item{}Form the \(n\times k\) matrix
            \(A = \left( \begin{smallmatrix} v_1 & v_2 & \dots &
            v_k\end{smallmatrix}\right)\).
          \item{}Put \(A\) into reduced row echelon form
            \(R = \mathrm{rref}(A)\). (Really, you only need to go to
            echelon form, here.)
          \item{}Read out pivot columns and free columns of \(R\).
            Those columns of \(A\) which are free columns are linear
            combinations of previous columns to the left! So, if any column
            of \(R\) (and \(A\)) is a free columns, the set of vectors
            is linearly dependent. If all of the columns of \(R\)
            (and \(A\)) are pivot columns, then the set is linearly
            independent.
          \end{itemize}

        \par

          This method is particularly good for identifying which subset of
          our original set of vectors would form a basis of the vector
          subspace \(\mathrm{span}(\{v_1, v_2, \ldots, v_k\})\).
        %

      
        The Row Space Algorithm
        (We will see the reason for the name soon.)
          Given a set of vectors \(\{v_1, v_2, \ldots, v_k\}\) from
          \(\mathbb{R}^n\):
        %

        \begin{itemize}[label=\textbullet]
\item{}Form the \(k\times n\) matrix
          \(A = \left( \begin{smallmatrix} v_1^T \\ v_2^T \\ \vdots
            \\ v_k^T \end{smallmatrix}\right)\).
          \item{}Put \(A\) into reduced row echelon form
            \(R = \mathrm{rref}(A)\).
          \item{}The rows of \(R\) will contain a basis for the vector
            subspace \(\mathrm{span}(\{v_1, v_2, \ldots, v_k\})\) (written as
            rows instead of columns, of course).
            If \(R\) has any zero rows, the original set was linearly
            dependent, otherwise it was linearly independent.
          \end{itemize}

        \par

          This method is good at picking out a simple basis of the vector
          subspace \(\mathrm{span}(\{v_1, v_2, \ldots, v_k\})\), but the
          resulting vectors \emph{probably won't come from your original set}.
        %

      \typeout{************************************************}
\typeout{Subsection  Sage and Bases}
\typeout{************************************************}
\subsection[Sage and Bases]{Sage and Bases}\label{subsection-83}

      Sage has several commands which are useful for dealing with the concpets
      of this section.
    %
\typeout{************************************************}
\typeout{Subsubsection  Commands for span, dimension, and basis}
\typeout{************************************************}
\subsubsection[Commands for span, dimension, and basis]{Commands for span, dimension, and basis}\label{subsubsection-41}

        This first command will
        construct a vector subspace of \(3\)-space which is spanned by the two
        vectors we pass in as arguments.
      %
\begin{lstlisting}[style=sageinput]
v1 = vector([1,0,2])
v2 = vector([1,1,3])

V = span([v1,v2], QQ)
V
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 3 and dimension 2 over Rational Field
Basis matrix:
[1 0 2]
[0 1 1]
\end{lstlisting}
\par

        Note that Sage already gives us some information:
        \begin{itemize}[label=\textbullet]
\item{}dimension\item{}a basis\end{itemize}

        Sage has chosen its preferred basis, as usual.
      %
\begin{lstlisting}[style=sageinput]
V.dimension()
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
V.basis()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[
(1, 0, 2),
(0, 1, 1)
]
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsubsection  The Algorithms for Checking Linear Independence}
\typeout{************************************************}
\subsubsection[The Algorithms for Checking Linear Independence]{The Algorithms for Checking Linear Independence}\label{subsubsection-42}

        Sage does not have built-in commands with names for checking linear independence
        or linear dependence. Instead, you have to just use the algorithms.
      %

        The Row Space Algorithm
        \begin{lstlisting}[style=sageinput]
row_matrix = matrix(QQ, 2,3, [v1,v2])
row_matrix
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 0 2]
[1 1 3]
\end{lstlisting}

        \begin{lstlisting}[style=sageinput]
R = row_matrix.rref()
R
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 0 2]
[0 1 1]
\end{lstlisting}

        
          (Where have we seen that before?) From this, it is clear that \(\left\{
          v_1, v_2 \right\}\) is linearly dependent because both rows have pivots.
        %

      
        The Column Space Algorithm
        
          We work much the same way here.
        %

        \begin{lstlisting}[style=sageinput]
col_matrix = matrix(2,3, [v1,v2]).transpose()
col_matrix
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 1]
[0 1]
[2 3]
\end{lstlisting}

        \begin{lstlisting}[style=sageinput]
col_matrix.rref()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 0]
[0 1]
[0 0]
\end{lstlisting}

        \par

          This tells us that our first two columns are pivot columns, so we
          should
          keep those as part of our basis for
          \(\mathrm{span}\left(v_1, v_2\right)\).
        %

      \typeout{************************************************}
\typeout{Subsubsection  Spaces Associated to a Matrix}
\typeout{************************************************}
\subsubsection[Spaces Associated to a Matrix]{Spaces Associated to a Matrix}\label{subsubsection-43}

        Sage knows about the column space and row space associated to a
        matrix. For this next example, we will work over the ring \lstinline?AA?
        of ``algebraic numbers'', so we can include \(\sqrt{2}\).
      %
\begin{lstlisting}[style=sageinput]
entries = [453, 1/3, 34, 2.sqrt(),
           9, 11/9, -3, 8, 98, 10,
           21, -4]
A = matrix(AA, 3,4, entries); A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[               453                1/3                 34 1.414213562373095?]
[                 9               11/9                 -3                  8]
[                98                 10                 21                 -4]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.column_space()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 3 and dimension 3 over Algebraic Real Field
Basis matrix:
[1 0 0]
[0 1 0]
[0 0 1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.column_space().dimension()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
3
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.column_space().basis()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[
(1, 0, 0),
(0, 1, 1),
(0, 0, 1)
]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.row_space()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 4 and dimension 3 over Algebraic Real Field
Basis matrix:
[                   1                    0                    0 0.12115291228302765?]
[                   0                    1                    0   1.751194192226904?]
[                   0                    0                    1  -1.589758444095512?]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.row_space().dimension()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
3
\end{lstlisting}
\par

        Or we could do this in other ways.
      %
\begin{lstlisting}[style=sageinput]
span(A.columns()).dimension()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
3
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
span(A.rows()).dimension()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
3
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.right_kernel().dimension()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
1
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-84}

      Note: We may not present all of these in class.
    %
\begin{task}
\label{task-108}

        Make an example of a vector \(v \in \mathbb{R}^4\) so that the set
        \[
          \left\{
          \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \end{pmatrix},
          \begin{pmatrix} 0 \\ 1 \\ 2 \\ 1 \end{pmatrix},
          \begin{pmatrix} -2\\ 1 \\ 0 \\ -5 \end{pmatrix},
          v
          \right\}
        \]
        is a linearly independent set, or explain why it is impossible to find
        such an example.
        Is your resulting set a basis?
      \end{task}
\begin{task}
\label{task-109}

        Make an example of a vector \(w \in \mathbb{R}^3\) so that the set
        \[
          \left\{
          \begin{pmatrix} 2 \\ 3 \\ 4 \end{pmatrix},
          \begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix},
          \begin{pmatrix} -2\\ -4 \\ -5 \end{pmatrix},
          w
          \right\}
        \]
        is a spanning set, or explain why it is impossible to find such an
        example. Is your resulting set a basis?
      \end{task}
\begin{task}
\label{task-110}
(Strang 3.5.10)
        Find two independent vectors on the plane \(x+2y-3z-t=0\) in
        \(\mathbb{R}^4\). Then find three independent vectors on this plane.
        Why not four? Find a matrix for which this plane is the nullspace.
      \end{task}
\begin{task}
\label{task-111}
(Strang 3.5.21)
        Suppose that the columns of a \(5 \times 5 \) matrix \(A\) are a
        basis for \(\mathbb{R}^5\).
        \begin{enumerate}
\item{}
            Explain why the equation \(Ax = 0\) has only the solution
            \(x=0\).
          \item{}
            What fact about the column vectors guarantees that for any \(b\)
            in \(\mathbb{R}^5\) the equation \(Ax =b\) is solvable?
          \end{enumerate}

        Conclusion: \(A\) is invertible. Its rank is \(5\). Its rows are
        also a basis for \(\mathbb{R}^5\).
      \end{task}
\begin{task}
\label{task-112}
(Strang 3.5.22)
        Suppose that \(S\) is a \(5\)-dimensional subspace of
        \(\mathbb{R}^6\).
        Determine if the following statements are true or false. If true, give
        an explanation for why it is true. If false, give a counterexample.
        \begin{enumerate}
\item{}
            Every basis for \(S\) can be extended to a basis for
            \(\mathbb{R}^6\) by adding one more vector.
          \item{}
            Every basis for \(\mathbb{R}^6\) can be reduced to a basis for
            \(S\) by removing one vector.
          \end{enumerate}
\end{task}
\begin{task}
\label{task-113}
(Strang 3.5.26)
        Find a basis (and the dimension) for each of these subspaces of the vector
        space of \(3\times 3\) matrices:
        \begin{enumerate}
\item{}
            All diagonal matrices.
          \item{}
            All symmetric matrices. (\(A^T = A\))
          \item{}
            All skew-symmetric matrices. (\(A^T = -A\))
          \end{enumerate}
\end{task}
\begin{task}
\label{task-114}
(Strang 3.5.35)
        Find a basis for the space of polynomials \(p(x)\) of degree less than
        or equal to \(3\). Find a basis for the subspace with \(p(1)=0\).
      \end{task}
\typeout{************************************************}
\typeout{Section 3.7 The Four Subspaces}
\typeout{************************************************}
\section[The Four Subspaces]{The Four Subspaces}\label{four-subspaces}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-85}
\begin{itemize}[label=\textbullet]
\item{}Read chapter 3 section 6 of \emph{Strang}.\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-86}
Before class, a student should be able to:%
\begin{itemize}[label=\textbullet]
\item{}Identify the four subspaces associated to a matrix by giving a basis
        of each.
      \item{}Determine the dimension of each of the four subspaces associated to a
        matrix.
      \end{itemize}
\par
Some time after class, a student should be able to:%
\begin{itemize}[label=\textbullet]
\item{}Describe each of the four subspaces associated to a matrix by
        giving minimal sets of equations which ``cut them out.''
      \item{}State and use the Fundamental Theorem of Linear Algebra (FTLA)
        to reason about matrices.
      \item{}Use the RREF of a matrix to explain why the FTLA is true.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion: The Four Subspaces}
\typeout{************************************************}
\subsection[Discussion: The Four Subspaces]{Discussion: The Four Subspaces}\label{subsection-87}

      This section summarizes a big tool for understanding the behavior of a
      matrix as a function.
      Recall that if \(A\) is an \(m\times n\) matrix, then we can
      think of it as defining a function
      \[
        \begin{array}{rcl}
        T_A: \mathbb{R}^n & \rightarrow & \mathbb{R}^m \\
        v\phantom{R} & \mapsto & Av
        \end{array}
      \]
      which takes as inputs vectors from \(\mathbb{R}^n\) and has as
      outputs vectors in \(\mathbb{R}^m\). We have also seen that
      properties of matrix multiplication translate into properties that
      make into a \terminology{linear transformation}.
    %
\par

      We now have four fundamental subspaces associated to the matrix \(A\).
    %
\begin{itemize}[label=\textbullet]
\item{}The column space, \(\mathrm{col}(A)\), spanned by all of the
        columns of \(A\). This is a subspace of \(\mathbb{R}^m\).
      \item{}The row space, \(\mathrm{row}(A)\), spanned by all of the rows
        of \(A\). This is a subspace of \(\mathbb{R}^n\). This also
        happens to be the column space of \(A^T\).
      \item{}The nullspace (or kernel), \(\mathrm{null}(A)\), consisting of
        all those vectors \(x\) for which \(Ax=0\). This is a subspace
        of \(\mathbb{R}^n\).
      \item{}The left nullspace, which is just the nullspace of \(A^T\). This
        is a subspace of \(\mathbb{R}^m\).
      \end{itemize}
\par

      And we have a big result:
    %
\begin{theorem}\label{theorem-7}

        If \(A\) is an \(m\times n\) matrix with rank
        \(\mathrm{rank}(A) = r\), then
        \begin{itemize}[label=\textbullet]
\item{}\(\dim(\mathrm{col}(A)) = \dim(\mathrm{row}(A)) = r\),\item{}\(\dim(\mathrm{null}(A)) = n-r\), and\item{}\(\dim(\mathrm{null}(A^T))= m-r\).\end{itemize}
\end{theorem}
\par

      (\emph{Study Hint: Write that out in English, with no notation.
      It will help you remember it.})
    %
\par

      We will have more to say about these spaces when we reconsider the uses
      of the dot product in chapter 4.
    %
\typeout{************************************************}
\typeout{Subsection  Sage and the Four Subspaces}
\typeout{************************************************}
\subsection[Sage and the Four Subspaces]{Sage and the Four Subspaces}\label{subsection-88}

      We have already seen enough Sage commands to work with the four subspaces:
      \lstinline?.row_space()?, \lstinline?.column_space()?, \lstinline?.left_kernel()?, and
      \lstinline?.right_kernel()? all work.
      Alternatively, we need only remember that the left nullspace and the row
      space are just the nullspace and column space of the transpose. Let us
      take a look at some of the options.
    %
\begin{lstlisting}[style=sageinput]
A = matrix(AA, 2,5, [3,4,5,-1,-1, 1,1,2,-1,1])
A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 3  4  5 -1 -1]
[ 1  1  2 -1  1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.column_space()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 2 and dimension 2 over Algebraic Real Field
Basis matrix:
[1 0]
[0 1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.right_kernel()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 5 and dimension 3 over Algebraic Real Field
Basis matrix:
[  1   0   0   2   1]
[  0   1   0 5/2 3/2]
[  0   0   1 7/2 3/2]
\end{lstlisting}
\par

      We have computed column spaces and nullspaces before. What about our
      new friends?
    %
\begin{lstlisting}[style=sageinput]
A.row_space()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 5 and dimension 2 over Algebraic Real Field
Basis matrix:
[ 1  0  3 -3  5]
[ 0  1 -1  2 -4]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.transpose().column_space()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 5 and dimension 2 over Algebraic Real Field
Basis matrix:
[ 1  0  3 -3  5]
[ 0  1 -1  2 -4]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.transpose().column_space() == A.row_space()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\par

      Since Sage prefers rows under the hood, the left nullspace is easy to find.
    %
\begin{lstlisting}[style=sageinput]
A.left_kernel()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 2 and dimension 0 over Algebraic Real Field
Basis matrix:
[]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.transpose().right_kernel()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 2 and dimension 0 over Algebraic Real Field
Basis matrix:
[]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.left_kernel() == A.transpose().right_kernel()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\par

      And there you have it. Sage can construct all four fundamental subspaces,
      and each comes with a basis computed by Sage. (Using the row algorithm!)
      Note that the FTLA works in this case.
    %
\typeout{************************************************}
\typeout{Subsection  Questions for Section 3.6}
\typeout{************************************************}
\subsection[Questions for Section 3.6]{Questions for Section 3.6}\label{subsection-89}
\begin{task}
\label{task-115}

        Find the four subspaces, including a basis of each, for the matrix
        \[
          A = \begin{pmatrix}
          7 & -1 & 3 \\
          -2 & 4 & -5 \\
          1 & 11 & -12
          \end{pmatrix}.
        \]\end{task}
\begin{task}
\label{task-116}

        Find the four subspaces, including a basis of each, for the matrix
        \[
          B = \begin{pmatrix}
          1 & 3 & -2 & 0 & 2 & 0 \\
          2 & 6 & -5 & -2 & 4 & -3 \\
          0 & 0 & 5 & 10 & 0 & 15 \\
          2 & 6 & 0 & 8 & 4 & 18
          \end{pmatrix}.
        \]\end{task}
\begin{task}
\label{task-117}
(Strang 3.6.12)
        
          Find an example of a matrix which has \((1,0,1)\) and \((1,2,0)\)
          as a basis for its row space and its column space.
        %
\par

          Why can't this be a basis for the row space and the nullspace?
        %
\end{task}
\begin{task}
\label{task-118}
(Strang 3.6.14)
        Without computing \(A\), find bases for its four fundamental subspaces:
        \[
          A = \begin{pmatrix} 1 & 0 & 0 \\
          6 & 1 & 0 \\ 9 & 8 & 1 \end{pmatrix}
          \begin{pmatrix} 1 & 2 & 3 & 4 \\
          0 & 1 & 2 & 3 \\ 0 & 0 & 1 & 2 \end{pmatrix}.
        \]\end{task}
\begin{task}
\label{task-119}
(Strang 3.6.16)
        Explain why the vector \(v = (1, 0, -1)\) cannot be a row of \(A\)
        and also in the nullspace of \(A\).
      \end{task}
\begin{task}
\label{task-120}
(Strang 3.6.24)
        
          The equation \(A^Ty = d\) is solvable exactly when \(d\) lies in
          one of the four subspaces associated to \(A\). Which is it?
        %
\par

          Which subspace can you use to determine if the solution to that
          equation is unique? How do you use that subspace?
        %
\end{task}
\typeout{************************************************}
\typeout{Section 3.8 Going Further with the Fundamental Theorem}
\typeout{************************************************}
\section[Going Further with the Fundamental Theorem]{Going Further with the Fundamental Theorem}\label{ftla-gf.xml}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-90}
\begin{itemize}[label=\textbullet]
\item{}
        Go back through the exercises in the last two sections of this chapter.
        Complete any items you
        did not complete the first time through. Prepare any that we have not
        discussed in class so that you will be ready to present them.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion}
\typeout{************************************************}
\subsection[Discussion]{Discussion}\label{subsection-91}
Now we take a short break to revisit and consolidate the learning you
      have done so far. Revisit the reading and the exercises you have done in
      the first four sections of Chapter Three: Vector Space and Subspaces. The
      important feature of this work should be
      learning to think about your own thinking. This sort of \terminology{meta-cognition}
      characterizes expert learners. Eventually, you want to be able to monitor
      your work at all times and recognize when you understand deeply and when
      you do not. This will allow you to self-correct.
    %
\par
To help you get started with meta-cognition, I listed learning goals in
      each section. To go further, you need to explicitly go through the process
      of reviewing what you can do and what you cannot. Here are some prompts to
      help you get started with this process.
      \begin{itemize}[label=\textbullet]
\item{}
          Review the learning goals from each section. Can you do the things
          described? Can you do them sometimes, or have you mastered them so you
          can do them consistently?
        \item{}
          Look through all of the tasks and go deeper into them. Can you
          connect each exercise to one of our pictures? Try to build a mental
          model of how the exercise and its solution work.
        \item{}
          If your first solution to an exercise involve a ``guess-and-check''
          approach, can you now complete the exercise in a \emph{purposeful}
          and systematic manner?
        \item{}
          Make a list of concepts or exercises that are not clear to you. Phrase
          each item in your list as a question, and make each question as
          specific as possible. Talk with fellow students or your
          instructor until you can answer your own questions.
        \end{itemize}

    %
\typeout{************************************************}
\typeout{Chapter 4 Orthogonality}
\typeout{************************************************}
\chapter[Orthogonality]{Orthogonality}\label{chapter-orthogonality}
\typeout{************************************************}
\typeout{Introduction  }
\typeout{************************************************}

      We introduced the dot product in Chapter One, but we have not found a lot
      of use for it so far. In this chapter, we pick it back up and see what
      we can do with it. Of course, the dot product hides a lot of geometry
      in its mysteries, but for us the key is the notion of \emph{orthogonality}.
    %
\par

      In this chapter we will address two important questions:
      \begin{enumerate}
\item{}
          How can we find \emph{approximate solutions} to equations \(Ax=b\)
          when we know (or suspect) that finding a true solutions is impossible?
        \item{}
          How can we use geometry to find a \emph{good} basis for a subspace?
        \end{enumerate}

      The answers to both of these questions will involve using the dot product
      to check for orthogonality. This will be leveraged into the technique of
      \terminology{orthogonal projection.}
    %
\par

      Now, to begin, we shall explore the idea of \emph{subspaces} being orthogonal,
      rather than just vectors, and strengthen the Fundamental Theorem of Linear
      Algebra.
    %
\typeout{************************************************}
\typeout{Section 4.1 Orthogonality and the Four Subspaces}
\typeout{************************************************}
\section[Orthogonality and the Four Subspaces]{Orthogonality and the Four Subspaces}\label{section-ftla2}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-92}
\begin{itemize}[label=\textbullet]
\item{}Read section 4.1 of \emph{Strang}.\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-93}

      Before class, a student should be able to:
    %
\begin{itemize}[label=\textbullet]
\item{}
        Find the four subspaces associated to a matrix and verify that they
        are orthogonal subspaces.
      \item{}
        Draw the ``Big Picture'' associated to a matrix as a schematic drawing,
        with the four subspaces properly located and their dimensions identified.
      \end{itemize}
\par

      Some time after class, a student should be able to:
    %
\begin{itemize}[label=\textbullet]
\item{}Find the orthogonal complement to a subspace.\item{}
        Reason about the structure of a matrix as a transformation using information
        about its four subspaces.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion: Orthogonality for subspaces}
\typeout{************************************************}
\subsection[Discussion: Orthogonality for subspaces]{Discussion: Orthogonality for subspaces}\label{subsection-94}

      Previously, we had the notion of orthogonality for two vectors in
      Euclidean space. In this section, the concept gets extended to subspaces.
    %
\begin{definition}\label{definition-1}

        Let \(V\) and \(W\) be subspaces of \(\mathbb{R}^n\). We say that
        \(\mathbb{R}^n\) and \(\mathbb{R}^m\) are \terminology{orthogonal}
        when for each vector \(v \in V\) and each vector \(w \in W\) we
        have \(v \cdot w =0\).
      \end{definition}
\par

      Two orthogonal subspaces always have as intersection the trivial subspace
      \(\{ 0\}\). The reason for this is that if some vector \(x\) lay in
      both \(V\) and \(W\), then we must have that \(x \cdot x =0\).
      (Think of the first \(x\) as lying in \(V\), and the second in
      \(W\).) But the properties of the dot product then mean that \(x\)
      is the zero vector.
    %
\par

      There is a further concept:
    %
\begin{definition}\label{definition-2}

        Let \(V\) be a vector subspace of \(\mathbb{R}^n\). The
        \terminology{orthogonal complement} of \(V\) is the set
        \[
          V^{\perp} = \{ w \in \mathbb{R}^n \mid w \cdot v = 0 \text{ for all } v \in V \}.
        \]\end{definition}
\par

      The basic idea is that two spaces are orthogonal complements if they are
      orthogonal, and together they contain enough vectors to span the entire
      space. The definition looks like it is a one-directional thing: for a
      subspace, you find its orthogonal complement. But really it is a
      \emph{complementary} relationship. If \(W\) is the orthogonal
      complement to \(V\), then \(V\) is the orthogonal complement to
      \(W\).
    %
\par

      Recall the four fundamental subspaces associated to an \(m\times n\)
      matrix \(A\).
      
        
          The column space, \(\mathrm{col}(A)\), spanned by all of the
          columns of \(A\). This is a subspace of \(\mathbb{R}^m\).
        
        
          The row space, \(\mathrm{row}(A)\), spanned by all of the rows of
          \(A\). This is a subspace of \(\mathbb{R}^n\). This also happens
          to be the column space of \(A^T\).
        
        
          The nullspace (or kernel), \(\mathrm{null}(A)\), consisting of all
          those vectors \(x\) for which \(Ax=0\). This is a subspace of
          \(\mathbb{R}^n\).
        
        
          The left nullspace, which is just the nullspace of \(A^T\). This is
          a subspace of \(\mathbb{R}^m\).
        
      
    %
\par

      And we have another big result, which is a sharpening of the Fundamental
      Theorem of Linear Algebra from the end of Chapter Three.
    %
\begin{theorem}\label{theorem-8}

        If \(A\) is an \(m\times n\) matrix, then
        \begin{itemize}[label=\textbullet]
\item{}
            The nullspace of \(A\) and the row space of \(A\) are
            orthogonal complements of one another.
          \item{}
            The column space of \(A\) and the left nullspace of \(A\)
            are orthogonal complements of one another.
          \end{itemize}
\end{theorem}
\typeout{************************************************}
\typeout{Subsection  Sage and the Orthogonal Complement}
\typeout{************************************************}
\subsection[Sage and the Orthogonal Complement]{Sage and the Orthogonal Complement}\label{subsection-95}

      It is not hard to find the four subspaces associated to a matrix with Sage's
      built-in commands. But Sage also has a general purpose \lstinline?.complement()?
      method available for vector subspaces which can be used.
    %
\begin{lstlisting}[style=sageinput]
A = matrix(QQ, 3,2, [12,3,4,5,6,7])
A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[12 3]
[ 4 5]
[ 6 7]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.right_kernel() # the nullspace
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 2 and dimension 0 over Rational Field
Basis matrix:
[]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.column_space()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 3 and dimension 2 over Rational Field
Basis matrix:
[   1    0 1/24]
[   0    1 11/8]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.row_space()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 2 and dimension 2 over Rational Field
Basis matrix:
[1 0]
[0 1]
\end{lstlisting}
\par

      Those are the easy ones to find. The left nullspace is just a touch trickier.
      What is the deal? It is just the nullspace of the matrix \(A^T\), of course.
      But by the Fundamental Theorem of Linear Algebra, the left nullspace is the
      orthogonal complement of the column space. Let's see if they agree.
    %
\begin{lstlisting}[style=sageinput]
A.column_space().complement()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 3 and dimension 1 over Rational Field
Basis matrix:
[  1  33 -24]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.transpose().right_kernel()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 3 and dimension 1 over Rational Field
Basis matrix:
[  1  33 -24]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.left_kernel()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Vector space of degree 3 and dimension 1 over Rational Field
Basis matrix:
[  1  33 -24]
\end{lstlisting}
\par

      That is good news! It looks like the three ways we have of computing the
      left nullspace agree. As a check for understanding, you should be able to
      ask Sage if the rowspace of \(A\) is the orthogonal complement to the
      nullspace of \(A\).
    %
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-96}
\begin{task}
\label{task-121}
(Strang ex. 4.1.2)
        Draw the ``Big Picture'' for a \(3\times 2\) matrix of rank \(2\).
        Which subspace has to be the zero subspace?
      \end{task}
\begin{task}
\label{task-122}
(Strang ex. 4.1.3)
        For each of the following, give an example of a matrix with the required
        properties, or explain why that is impossible.
        \begin{enumerate}
\item{}
            The column space contains \(\begin{pmatrix} 1 \\ 2 \\ -3\end{pmatrix}\)
            and \(\begin{pmatrix} 2 \\ -3 \\ 5 \end{pmatrix}\), and the nullspace
            contains \(\begin{pmatrix} 1\\1\\1 \end{pmatrix}\).
          \item{}
            the row space contains \(\begin{pmatrix} 1 \\ 2 \\ -3\end{pmatrix}\)
            and \(\begin{pmatrix} 2 \\ -3 \\ 5 \end{pmatrix}\), and the nullspace
            contains \(\begin{pmatrix} 1\\1\\1 \end{pmatrix}\).
          \item{}
            \(Ax = \begin{pmatrix}1 \\ 1\\ 1\end{pmatrix}\) has a solution, and
            \(A^T\begin{pmatrix}1 \\ 0 \\ 0 \end{pmatrix} =
              \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}\).
          \item{}
            Every row is orthogonal to every column, but \(A\) is not the zero matrix.
          \item{}
            The columns add up to a column of zeros, and the rows add up to a row of \(1\)'s.
          \end{enumerate}
\end{task}
\begin{task}
\label{task-123}
(Strang ex 4.1.4)
        If \(AB = 0\), how do the columns of \(B\) relate to the subspaces
        of \(A\)? How do the rows of \(A\) relate to the subspaces of \(B\)?
        Why can't we make an example of this where \(A\) and \(B\) are both
        \(3 \times 3\) matrices of rank \(2\)?
      \end{task}
\begin{task}
\label{task-124}
(Strang ex 4.1.9)
        Use the four subspace of \(A\) to explain why this is always true:
        \[\text{If } A^T Ax = 0, \text{then } Ax = 0.\]
        (This fact will be useful later! It will help us see that \(A^TA\) and
        \(A\) have the same nullspace.)
      \end{task}
\begin{task}
\label{task-125}
(Strang ex 4.1.12)
        Find the pieces \(x_r\) and \(x_n\) and draw the ``Big Picture''
        carefully when:
        \[ A = \begin{pmatrix} 1 & -1 \\ 0 & 0 \\ 0 & 0 \end{pmatrix}
          \qquad \text{and } x = \begin{pmatrix} 2 \\ 0 \end{pmatrix}.
        \]\end{task}
\begin{task}
\label{task-126}
(Strang ex. 4.1.22)
        Let \(P\) be the hyperplane of vectors in \(\mathbb{R}^4\) which
        satisfy the equation
        \[ x_1 + x_2 + x_3 + x_4 = 0 .\]
        Write down a basis for \(P^{\perp}\). Construct a matrix \(X\) which
        has \(P\) as its nullspace.
      \end{task}
\typeout{************************************************}
\typeout{Section 4.2 Projections Onto Subspaces}
\typeout{************************************************}
\section[Projections Onto Subspaces]{Projections Onto Subspaces}\label{section-projections}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-97}
\begin{itemize}[label=\textbullet]
\item{}Read Chapter 4 section 2 of \emph{Strang}.\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-98}
Before class, a student should be able to:%
\begin{itemize}[label=\textbullet]
\item{}Compute the projection of a vector onto a line.\item{}Find the projection matrix which computes the projections of vectors onto a given line.\item{}Draw the schematic picture of a projection: the line, the vector, the projected vector, and the difference.\end{itemize}
\par
Sometime after class, a student should be able to:%
\begin{itemize}[label=\textbullet]
\item{}Compute the projection of a vector onto a subspace.\item{}Find the projection matrix which computes the projections of vectors onto a given subspace.\item{}
            Explain the process for finding the equations which determine the projection matrix, and say
            why the transpose makes an appearance.
        \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion: Orthogonal Projections}
\typeout{************************************************}
\subsection[Discussion: Orthogonal Projections]{Discussion: Orthogonal Projections}\label{subsection-99}

        One good use of the geometry in \(\mathbb{R}^n\) is the concept of
        orthogonal projection. The basic idea is to mimic the behavior of
        shadows under sunlight. Our everyday experience leads us to thinking
        about the projection of a vector onto a plane (the ground--its roughly
        a plane), but if you imagine holding out a pencil you can summon up the
        visual of projection onto a line, too.
      %
\par

        The key concept is to use the basic condition of orthogonality
        (\(u \cdot v = 0\)) to figure things out.
      %
\par

        Note that everything in this section is done by projecting onto
        \emph{subspaces}! This is a bit of a restriction. In practice, this
        restriction can be removed by translating your whole problem to have a
        new origin.
      %
\typeout{************************************************}
\typeout{Subsection  Sage and Orthogonal Projection}
\typeout{************************************************}
\subsection[Sage and Orthogonal Projection]{Sage and Orthogonal Projection}\label{subsection-100}

        Sage has no built-in commands for orthogonal projections. But let us recall
        those parts of Sage that will be useful right now:
      %
\begin{lstlisting}[style=sageinput]
A = matrix(QQ, 3,2, [1,2,3,4,5,6])
A
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.transpose()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 3 5]
[2 4 6]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.inverse()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
Error in lines 1-1
...
ArithmeticError: self must be a square matrix
\end{lstlisting}
\par

        Sorry, that matrix isn't even square, so it can't be invertible. But
        this will be:
      %
\begin{lstlisting}[style=sageinput]
(A.transpose()*A).inverse()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[  7/3 -11/6]
[-11/6 35/24]
\end{lstlisting}
\par

        Finally, this makes sense:
      %
\begin{lstlisting}[style=sageinput]
P = A * (A.transpose()*A).inverse() * A.transpose()
P
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 5/6  1/3 -1/6]
[ 1/3  1/3  1/3]
[-1/6  1/3  5/6]
\end{lstlisting}
\par

        This process should have some basic properties. Let's check them.
      %
\begin{lstlisting}[style=sageinput]
B = A.transpose()*A
print B.is_invertible()
print B.is_symmetric()
print B.parent()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
True
Full MatrixSpace of 2 by 2 dense matrices over Rational Field
\end{lstlisting}
\par

        So \(A^TA\) is square, symmetric, and invertible.
      %
\begin{lstlisting}[style=sageinput]
print P.transpose() == P
print P.is_symmetric()
print P*P == P
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
True
True
\end{lstlisting}
\par
Also as expected.%
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-101}
\begin{task}
\label{task-127}

            Find the projection matrix which computes projections of vectors in
            \(\mathbb{R}^2\) onto the line \(3x+2y=0\).
            (Since it goes through zero, it is a subspace.)
          %
\par

            Find the orthogonal projection of the vector
            \(\left( 17,3 \right)\) onto this line.
          %
\end{task}
\begin{task}
\label{task-128}

            Find the projection matrix which computes projections of vectors in
            \(\mathbb{R}^3\) onto the line which is the intersection of the
            planes \(x-2y+3z = 0\) and \(y+2z=0\). (Again, that is a subspace.)
          %
\par

            Find the orthogonal projection of the vector \(\left(1,1,1\right)\)
            onto this line.
          %
\end{task}
\begin{task}
\label{task-129}

            Find the projection matrix which computes projections of vectors in
            \(\mathbb{R}^3\) onto the plane \(-2x + y +3z = 0\).
          %
\par

            Find the orthogonal projection of the vector \(\left( 9,7,-5\right)\)
            onto this plane.
          %
\end{task}
\begin{task}
\label{task-130}

            Find the projection matrix which computes projections of vectors in
            \(\mathbb{R}^4\) onto the plane which is the intersectoin of
            \(5x+y +w=0\) and \(z+y+z+w=0\). (This subspace is the 2
            dimensional plane where these two 3-dimensional hyperplanes meet.)
          %
\par

            Find the orthogonal projection of the vector
            \(\left(-3,1,-3,1\right)\) on this plane.
          %
\end{task}
\typeout{************************************************}
\typeout{Section 4.3 Going Further: Projections}
\typeout{************************************************}
\section[Going Further: Projections]{Going Further: Projections}\label{section-gf-proj}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-102}
\begin{itemize}[label=\textbullet]
\item{}
        Go back through the exercises in this chapter. Complete any items you
        did not complete the first time through. Prepare any that we have not
        discussed in class so that you will be ready to present them.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion}
\typeout{************************************************}
\subsection[Discussion]{Discussion}\label{subsection-103}
Now we take a short break to revisit and consolidate the learning you
      have done so far. Revisit the reading and the exercises you have done in
      the first two sections of Chapter Four. The important feature of this work should be
      learning to think about your own thinking. This sort of \terminology{meta-cognition}
      characterizes expert learners. Eventually, you want to be able to monitor
      your work at all times and recognize when you understand deeply and when
      you do not. This will allow you to self-correct.
    %
\par
To help you get started with meta-cognition, I listed learning goals in
      each section. To go further, you need to explicitly go through the process
      of reviewing what you can do and what you cannot. Here are some prompts to
      help you get started with this process.
      \begin{itemize}[label=\textbullet]
\item{}
          Review the learning goals from each section. Can you do the things
          described? Can you do them sometimes, or have you mastered them so you
          can do them consistently?
        \item{}
          Look through all of the tasks and go deeper into them. Can you
          connect each exercise to one of our pictures? Try to build a mental
          model of how the exercise and its solution work.
        \item{}
          If your first solution to an exercise involve a ``guess-and-check''
          approach, can you now complete the exercise in a \emph{purposeful}
          and systematic manner?
        \item{}
          Make a list of concepts or exercises that are not clear to you. Phrase
          each item in your list as a question, and make each question as
          specific as possible. Talk with fellow students or your
          instructor until you can answer your own questions.
        \end{itemize}

    %
\typeout{************************************************}
\typeout{Section 4.4 Approximate Solutions: Least Squares}
\typeout{************************************************}
\section[Approximate Solutions: Least Squares]{Approximate Solutions: Least Squares}\label{section-least-squares}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-104}
\begin{itemize}[label=\textbullet]
\item{}Read section 4.3 of Strang.\item{}Read the discussion below.\item{}Complete exercises 1-11 from section 4.3 in Strang.\item{}Prepare the items in the exercises for presentation.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion: Least Squares Approximation}
\typeout{************************************************}
\subsection[Discussion: Least Squares Approximation]{Discussion: Least Squares Approximation}\label{subsection-105}

      (It is probably best to read this after you read the section in Strang.)
    %
\typeout{************************************************}
\typeout{Subsubsection  Some Perspective}
\typeout{************************************************}
\subsubsection[Some Perspective]{Some Perspective}\label{subsubsection-44}

        Scientific problems often come down to something as simple as this: make
        a bunch of observations, and then try to fit those observations with
        some sort of model for greater understanding.
      %
\par

        But data found in scientific problems is often noisy, or infected with
        error in some way. This leads researchers to gather \emph{more} data
        so that chance variations and small errors might get smoothed out.  How
        might we fit a curve to a lot of data? Lots of data points means that we
        likely have too many points to have a curve of our specified model type
        actually hit all of those points.
      %
\par

        For example, fitting a line to five points is already problematic: any
        two points gives us a line, and there is no reason to believe that the
        other three points will all sit on that line.
      %
\par

        If we set the problem up as a system of equations, things go like this:
        We have a bunch of data set up as input-output pairs \(\{ (a_i, y_i) \}\);
        we are looking for a function \(f\) which has a specified type (linear,
        quadratic, exponential, etc.) that passes through those points.
      %
\begin{itemize}[label=\textbullet]
\item{}Each data point leads us to an equation \(f(a_i) = y_i\).\item{}
          The modelling function $f$ has some parameters in it, and we want to
          find the best value of those parameters so that the curve ``fits''
          the data well. These parameters are the unknowns in our equations.
        \end{itemize}
\par

        This is generally a challenging problem. The method of least squares is
        a technique for solving it when the resulting equations make a linear
        system.
      %
\typeout{************************************************}
\typeout{Subsubsection  Some History}
\typeout{************************************************}
\subsubsection[Some History]{Some History}\label{subsubsection-45}

        Gauss discovered the technique described in this section in the late
        1790's. In 1801 he used it to help astronomers calculate the orbit of
        the newly discovered asteroid Ceres, and thus find it after it re-emerged
        from behind the sun.
      %
\par

        See how the pattern fits? Several weeks worth of data about the position
        of Ceres was known, but it surely had measurement errors in it. Since
        the time of Kepler (Newton), we have known that the motion of the
        asteroid must be an ellipse. This is a simple equation with only a few
        parameters (the coefficients of the equation defining the ellipse). So,
        the question confronting Gauss was this: find the ellipse which best
        fits the data.
      %
\par

        But plugging all the data into the correct model shape (a conic!) leads
        to a rather large system of linear equations where the unknowns are the
        coefficients we seek.
      %
\typeout{************************************************}
\typeout{Subsubsection  So, what is really happening here?}
\typeout{************************************************}
\subsubsection[So, what is really happening here?]{So, what is really happening here?}\label{subsubsection-46}

        In the end, we get a system of the form \(Ax = y\). Here \(A\) is
        an \(m\times n\) matrix and \(ym\) is an \(n\)-vector, where
        \(m\) is the number of equations and \(n\) is the number of
        parameters we must find.
        Typically, \(m\) is much larger than \(n\), so the matrix \(A\)
        is tall and skinny.
      %
\par

        So the system likely has no solution. Instead, we will find the orthogonal
        projection \(\hat{y}\) of \(y\) onto the column space
        \(\mathrm{col}(A)\) of \(A\), and then solve \(Ax = \hat{y}\).
        That's the secret. Since we have already mastered projections, this is
        no big deal.
      %
\typeout{************************************************}
\typeout{Subsection  Sage instructions}
\typeout{************************************************}
\subsection[Sage instructions]{Sage instructions}\label{subsection-106}

      There are no new commands for dealing with matrices here, as we already
      have all that we need. If you are interested, Sage does have a built-in
      function called \lstinline?find_fit?.
    %
\begin{lstlisting}[style=sageinput]
Data = [[1,2], [-1,3], [4,1],[2,1],[1,.5]]
a, b, x = var('a b x')
model(x) = a*x + b

best = find_fit(Data, model, solution_dict=True)
best
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
{b: 2.030303030305277, a: -0.37878787879088605}
\end{lstlisting}
\par
Just to check that, let's plot the data and the curve.%
\begin{lstlisting}[style=sageinput]
curve = model.subs(best)
plot(curve, (x,-1,6)) + points(Data, color='red', size=20)
\end{lstlisting}
\par

      That is not so terrible. Keep in mind that, from a linear algebra perspective,
      we just found the projection of \(y = (2,3,1,1,.5) \in \mathbb{R}^5\) on
      the column space of the matrix
      \[
        A = \begin{pmatrix} 1 & 1 \\ -1 & 1\\ 4 & 1 \\ 2 & 1 \\
        1 & 1 \end{pmatrix},
      \]
      which is a \(2\)-dimensional plane in that \(5\)-dimensional space.
      If I could, I would draw that picture, but five dimensions in challenging.
    %
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-107}

      The best thing you can do to understand this is work some examples. Do
      Strang 1 - 11 from section 4.3. We will present these:
    %
\begin{task}
\label{task-131}

        Exercise 1 from section 4.3 of Strang.
      \end{task}
\begin{task}
\label{task-132}

        Exercise 5 from section 4.3 of Strang.
      \end{task}
\begin{task}
\label{task-133}

        Exercise 6 from section 4.3 of Strang.
      \end{task}
\begin{task}
\label{task-134}

        Exercise 7 from section 4.3 of Strang.
      \end{task}
\begin{task}
\label{task-135}

        Exercise 8 from section 4.3 of Strang.
      \end{task}
\begin{task}
\label{task-136}

        Exercise 9 from section 4.3 of Strang.
      \end{task}
\begin{task}
\label{task-137}

        Exercise 10 from section 4.3 of Strang.
      \end{task}
\begin{task}
\label{task-138}

        Exercise 11 from section 4.3 of Strang.
      \end{task}
\typeout{************************************************}
\typeout{Section 4.5 Orthonormal Bases and Gram-Schmidt}
\typeout{************************************************}
\section[Orthonormal Bases and Gram-Schmidt]{Orthonormal Bases and Gram-Schmidt}\label{section-gram-schmidt}
\typeout{************************************************}
\typeout{Subsection  The assignment}
\typeout{************************************************}
\subsection[The assignment]{The assignment}\label{subsection-108}
\begin{itemize}[label=\textbullet]
\item{}Read section 4.4 of \emph{Strang}.\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-109}
Before class, a student should be able to:%
\begin{itemize}[label=\textbullet]
\item{}Correctly decide if a matrix is orthogonal or not.\item{}
        Use the Gram-Schmidt algorithm to turn a basis into an
        orthonormal basis.
      \end{itemize}
\par
Some time after class, a student should be able to:%
\begin{itemize}[label=\textbullet]
\item{}Find the QR decomposition of a matrix.\item{}Describe the Gram-Schmidt process geometrically using orthogonal
        projections.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion: Gram-Schmidt}
\typeout{************************************************}
\subsection[Discussion: Gram-Schmidt]{Discussion: Gram-Schmidt}\label{subsection-110}

      There are four main points to take away from this section:
      \begin{itemize}[label=\textbullet]
\item{}The idea of an orthonormal basis.\item{}The idea of an orthogonal matrix. The special property that
          \(Q^T = Q^{-1}\) for an orthogonal matrix \(Q\).
        \item{}The Gram-Schimdt algorithm for constructing an orthonormal basis.\item{}The \(QR\) decomposition of a matrix \(A\).\end{itemize}

    %
\typeout{************************************************}
\typeout{Subsection  Sage and the QR decomposition}
\typeout{************************************************}
\subsection[Sage and the QR decomposition]{Sage and the QR decomposition}\label{subsection-111}

      Sage has a built-in command to find the QR decomposistion of a matrix.
      Essentially, it does the Gram-Schmidt algorithm under the hood.
    %
\par

      If you check the documentation, you will see that the matrix has to be defined
      over a special type of ring, so use \lstinline?QQbar?.
    %
\begin{lstlisting}[style=sageinput]
A = matrix(QQbar, 4,4,
           [1,-1,0,0, 0,1,-1,0,
           0,0,1,-1, 1,1,1,1])
A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1 -1  0  0]
[ 0  1 -1  0]
[ 0  0  1 -1]
[ 1  1  1  1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
Q, R = A.QR()
Q
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 0.7071067811865475? -0.5773502691896258? -0.3162277660168379? -0.2581988897471611?]
[                   0  0.5773502691896258? -0.6324555320336758?  -0.516397779494323?]
[                   0                    0  0.6324555320336758?  -0.774596669241484?]
[ 0.7071067811865475?  0.5773502691896258?  0.3162277660168379?  0.2581988897471611?]
\end{lstlisting}
\par

      This matrix should be an orthogonal matrix. Let's check that.
    %
\begin{lstlisting}[style=sageinput]
Q*Q.transpose()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1.000000000000000?            0.?e-17            0.?e-16            0.?e-17]
[           0.?e-17 1.000000000000000?            0.?e-16            0.?e-17]
[           0.?e-16            0.?e-16 1.000000000000000?            0.?e-16]
[           0.?e-17            0.?e-17            0.?e-16 1.000000000000000?]
\end{lstlisting}
\par

      That is machine language for ``I am pretty sure that's the identity.''
      Those question marks are for machine precision representation of exact numbers.
    %
\begin{lstlisting}[style=sageinput]
R
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[  1.414213562373095?              0.?e-18  0.7071067811865475?  0.7071067811865475?]
[                   0   1.732050807568878?              0.?e-17  0.5773502691896258?]
[                   0                    0   1.581138830084190? -0.3162277660168379?]
[                   0                    0                    0   1.032795558988645?]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
Q*R
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1.000000000000000? -1.000000000000000?             0.?e-17             0.?e-16]
[                  0  1.000000000000000? -1.000000000000000?             0.?e-16]
[                  0                   0  1.000000000000000? -1.000000000000000?]
[ 1.000000000000000?  1.000000000000000?  1.000000000000000?  1.000000000000000?]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
Q*R == A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-112}
\begin{task}
\label{task-139}
(Strang ex. 4.4.12)
        If \(a_1\), \(a_2\), \(a_3\) is a basis for \(\mathbb{R}^3\),
        then any vector \(b\) can be written as
        \[
          b = x_1 a_1 + x_2 a_2 + x_3 a_3
        \]
        or
        \[
          \begin{pmatrix} | & | & | \\ a_1 & a_2 & a_3 \\
          | & | & | \end{pmatrix}\begin{pmatrix} x_1 \\ x_2 \\ x_3
          \end{pmatrix} = b.
        \]\begin{enumerate}
\item{}
            Suppose the \(a_i\)'s are orthonormal.
            Show that \(x_1 = a_1^T b\).
          \item{}
            Suppose the \(a_i\)'s are orthogonal.
            Show that \(x_1 = a_1^T b / a_1^T a_1\).
          \item{}
            If the \(a_i\)'s are independent, \(x_1\) is the first component of
            something times b. What is the something?
          \end{enumerate}
\end{task}
\begin{task}
\label{task-140}
(Strang ex. 4.4.18)
        Find orthogonal vectors \(A\), \(B\), \(C\) by Gram-Schmidt from
        \(a\), \(b\), \(c\):
        \[
          a = (1,-1,0,0), \quad b = (0,1,-1,0), \quad c = (0,0,1,-1).
        \]
        Note that both of these collections are bases for the hyperplane perpendicular
        to \((1,1,1,1)\).
      \end{task}
\begin{task}
\label{task-141}
(Strang ex. 4.4.19)
        If \(A = QR\), then check that \(A^TA\) is the same thing as \(R^TR\).
        What can we say about the shape of the matrix \(R\)? Note that this means
        that Gram-Schmidt on \(A\) corresponds to \emph{elimination on \(A^TA\)}!
        The pivots for \(A^TA\) must be the squares of the diagonal entries of
        \(R\). Find \(Q\) and \(R\) by Gram-Schmidt for this \(A\):
        \[
          A = \begin{pmatrix} -1 & 1 \\ 2 & 1 \\ 2 & 4 \end{pmatrix}.
        \]
        Compare with the structure of this matrix:
        \[
          A^TA = \begin{pmatrix} 9 & 9 \\ 9 & 18 \end{pmatrix}
          = \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix}
          \begin{pmatrix} 9 & 0 \\ 0 & 9 \end{pmatrix}
          \begin{pmatrix}  1 & 1 \\ 0 & 1 \end{pmatrix}.
        \]\end{task}
\begin{task}
\label{task-142}
(Strang ex. 4.4.21)
        Find an orthonormal basis for the column space of \(A\), and then compute
        the projection of \(b\) onto that column space.
        \[
          A = \begin{pmatrix} 1 & -2 \\ 1 & 0 \\ 1 & 1 \\ 1 & 3\end{pmatrix}
          \qquad \text{ and } \qquad b = \begin{pmatrix} -4 \\ -3 \\ 3 \\ 0 \end{pmatrix}.
        \]\end{task}
\begin{task}
\label{task-143}
(Strang ex. 4.4.23)
        Let \(a\), \(b\), \(c\) be the columns of the matrix
        \[
          A = \begin{pmatrix} 1 & 2 & 4 \\ 0 & 0 & 5 \\
          0 & 3 & 6 \end{pmatrix}
        \]
        Note that these vectors are linearly independent, so make a basis for
        \(\mathbb{R}^3\).
        Find an orthonormal basis \(q_1\), \(q_2\), \(q_3\) and express
        the vectors as linear combinations of \(a\), \(b\), \(c\).
        Finally, write \(A\) as \(QR\).
      \end{task}
\begin{task}
\label{task-144}
(Strang ex. 4.4.24)
        \begin{enumerate}
\item{}
            Find a basis for the subspace \(S\) in \(\mathbb{R}^4\)
            spanned by all solutions of
            \[
              x_1 + x_2 + x_3 - x_4 = 0.
            \]
          \item{}
            Find a basis for the orthogonal complement \(S^{\perp}\).
          \item{}
            Find \(b_1\) in \(S\) and \(b_2\) in \(S^{\perp}\) so that
            \(b_1 + b_2 = b = (1,1,1,1)\).
          \end{enumerate}
\end{task}
\typeout{************************************************}
\typeout{Section 4.6 Going Further: Least Squares and Gram-Schmidt}
\typeout{************************************************}
\section[Going Further: Least Squares and Gram-Schmidt]{Going Further: Least Squares and Gram-Schmidt}\label{section-gf-ls-gs}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-113}
\begin{itemize}[label=\textbullet]
\item{}
        Go back through the exercises in this chapter. Complete any items you
        did not complete the first time through. Prepare any that we have not
        discussed in class so that you will be ready to present them.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion}
\typeout{************************************************}
\subsection[Discussion]{Discussion}\label{subsection-114}
Now we take a short break to revisit and consolidate the learning you
      have done so far. Revisit the reading and the exercises you have done in
      the last two sections of Chapter Four. The important feature of this work should be
      learning to think about your own thinking. This sort of \terminology{meta-cognition}
      characterizes expert learners. Eventually, you want to be able to monitor
      your work at all times and recognize when you understand deeply and when
      you do not. This will allow you to self-correct.
    %
\par
To help you get started with meta-cognition, I listed learning goals in
      each section. To go further, you need to explicitly go through the process
      of reviewing what you can do and what you cannot. Here are some prompts to
      help you get started with this process.
      \begin{itemize}[label=\textbullet]
\item{}
          Review the learning goals from each section. Can you do the things
          described? Can you do them sometimes, or have you mastered them so you
          can do them consistently?
        \item{}
          Look through all of the tasks and go deeper into them. Can you
          connect each exercise to one of our pictures? Try to build a mental
          model of how the exercise and its solution work.
        \item{}
          If your first solution to an exercise involve a ``guess-and-check''
          approach, can you now complete the exercise in a \emph{purposeful}
          and systematic manner?
        \item{}
          Make a list of concepts or exercises that are not clear to you. Phrase
          each item in your list as a question, and make each question as
          specific as possible. Talk with fellow students or your
          instructor until you can answer your own questions.
        \end{itemize}

    %
\typeout{************************************************}
\typeout{Chapter 5 Determinants}
\typeout{************************************************}
\chapter[Determinants]{Determinants}\label{chapter-determinants}
\typeout{************************************************}
\typeout{Introduction  }
\typeout{************************************************}

      Our next goal is to find a better understanding of the \terminology{determinant}
      of a square matrix. We have seen this come up before as a measurement that allows
      us to ``determine'' if a square matrix is invertible. But it has some more
      information in it than that, and it has some interesting properties.
    %
\par

      Our first section will take up the interesting properties, with special attention
      to those properies which make the determinant easier to compute. Then we will
      look at a few alternate methods of computation.
    %
\typeout{************************************************}
\typeout{Section 5.1 Properties of the Determinant}
\typeout{************************************************}
\section[Properties of the Determinant]{Properties of the Determinant}\label{section-det-properties}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-115}
\begin{itemize}[label=\textbullet]
\item{}
        Read section chapter 5 section 1 of Strang.
      \item{}
        Read the following and complete the exercises below.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-116}
At some point, a student should be able to compute the determinant of a
      square matrix using the properties outlined here.
    %
\typeout{************************************************}
\typeout{Subsection  Discussion: The Determinant}
\typeout{************************************************}
\subsection[Discussion: The Determinant]{Discussion: The Determinant}\label{subsection-117}

      Generally, the most interesting matrices to look at are the square ones.
      For square matrices, there is an important number called the
      \terminology{determinant} which helps us determine if the matrix is
      invertible or not.
    %
\par

      Strang lists 10 important properties of determinants in this section,
      and verifies them for \(2\times 2\) matrices. The verifications for
      general matrices aren't any harder, but they sure are longer,
      so I am glad he skipped them. Anyway, these properties are enough to get
      by when it is time to compute. In fact, clever use of these properties
      can save you a lot of time.
    %
\begin{enumerate}
\item{}The determinant of the identity matrix is always one: \(\det(I)=1\).\item{}If we exchange two rows of a matrix, the determinant changes sign.\item{}The determinant is linear in each row separately. (The fancy new word
        here is that \(\det\) is a \terminology{multilinear} function.)\item{}If two rows are equal, then the determinant is zero.\item{}The row operation of ``add a multiple of one row to another row''
        does not change the determinant of a matrix.\item{}If a matrix has a row of zeros, then its determinant is zero.\item{}If \(A\) is triangular, then \(\det(A)\) is the product of the
        diagonal entries of \(A\).\item{}A matrix is singular if its deterimant is zero. A matrix is invertible
        exactly when its determinant is non-zero.\item{}The determinant is a \terminology{multiplicative} function at the level
      of matrices: \(\det(AB) = \det(A)\det(B)\).\item{}The determinant of a matrix and its transpose are the same.\end{enumerate}
\par

      That last property can be helpful in a variety of ways: it allows us to translate
      all of those statements about rows into statements about columns!
    %
\typeout{************************************************}
\typeout{Subsubsection  Interpretation}
\typeout{************************************************}
\subsubsection[Interpretation]{Interpretation}\label{subsubsection-47}

        Suppose that an \(n\times n\) matrix \(A\) is represented as a
        collection of its column vectors:
        \[
        A = \begin{pmatrix}
        | & | &  & | \\
        v_1 & v_2 & \dots & v_n \\
        | & | &  & |
        \end{pmatrix} .
        \]
        Then the geometric significance of the determinant is this: The number
        \(\det(A)\) represents the signed \(n\)-dimensional
        volume of the \(n\)-dimensional box in \(\mathbb{R}^n\) with sides
        \(v_1, v_2, \ldots, v_n\).
      %
\par

        This takes a bit of getting used to, and the hardest part is the choice
        of signs. We choose a positive sign if the vectors
        \(v_1, v_2, \ldots, v_n\) have the same orientation as the standard
        basis.
      %
\typeout{************************************************}
\typeout{Subsection  Sage and the Determinant}
\typeout{************************************************}
\subsection[Sage and the Determinant]{Sage and the Determinant}\label{subsection-118}
Sage has a built-in command for the determinant of a square matrix. It is
      just what you expect: \lstinline?A.determinant()?.
    %
\begin{lstlisting}[style=sageinput]
A = matrix(QQ, 3,3, [4,3,5, 2,-1,0, -5,2,10])
A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 4  3  5]
[ 2 -1  0]
[-5  2 10]
\end{lstlisting}
\par
To be sure that this works properly, we can do it the old way, too:%
\begin{lstlisting}[style=sageinput]
P, L, U = A.LU(pivot="nonzero")
print(P)
print(L)
print(U)
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 0 0]
[0 1 0]
[0 0 1]
[     1      0      0]
[   1/2      1      0]
[  -5/4 -23/10      1]
[   4    3    5]
[   0 -5/2 -5/2]
[   0    0 21/2]
\end{lstlisting}
\par

      Clearly, there are no row swaps used, so it is easy to see that
      \(\det(A) = 4*(-5/2)*(21/2) = -105\).
    %
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-119}
\begin{task}
\label{task-145}
(Strang Ex 5.1.1)
        If a \(4 \times 4\) matrix \(A\) has \(\det(A) = 1/2\), find
        \begin{enumerate}
\item{}\(\det(2A)\),\item{}\(\det(-A)\), and\item{}\(\det(A^2)\).\end{enumerate}

        Explain which properties of determinants you need to make your deductions.
      \end{task}
\begin{task}
\label{task-146}
(Strang Ex 5.1.2)
        If a \(3 \times 3\) matrix has \(\det(A) = -1\), find
        \begin{enumerate}
\item{}\(\det(\frac{1}{2}A)\),\item{}\(\det(-A)\),\item{}\(\det(A^2)\), and \item{}\(\det(A^{-1})\)\end{enumerate}
\end{task}
\begin{task}
\label{task-147}
(Strang Ex 5.1.10)
        
          Suppose that \(A\) is a matrix with the property that the entries in
          each of its rows add to zero. Solve the equation \(Ax = 0\) to prove
          that \(\det(A)=0\).
        %
\par

          Then suppose that instead the entries in each row add to one. Show that
          \(\det(A-I)=0\). Does this mean that \(\det(A) = 1\)? Give an example
          or an argument.
        %
\end{task}
\begin{task}
\label{task-148}
(Strang Ex 5.1.12)
        The inverse of a \(2 \times 2\) matrix seems to have determinant \(1\)
        all the time:
        \[
          \det(A^{-1}) = \det \frac{1}{ad-bc}\begin{pmatrix} d & -b \\
          -c & a \end{pmatrix} = \frac{ad-bc}{ad-bc} = 1.
        \]
        What is wrong with this argument? What should the value of \(\det(A^{-1})\)
        be?
      \end{task}
\begin{task}
\label{task-149}
(Strang Ex 5.1.14)
        Use row operations to reduce these matrices to upper triangular ones, then find
        the determinants:
        \[
          \det\begin{pmatrix} 1 & 2 & 3 & 0 \\ 2 & 6 & 6 & 1 \\ -1 & 0 & 0 & 3 \\
          0 & 2 & 0 & 7 \end{pmatrix} \quad \text{ and } \quad
          \det\begin{pmatrix} 2 & -1 & 0 & 0 \\ -1 & 2 & -1 & 0 \\ 0 & -1 & 2 & -1 \\
          0 & 0 & -1 & 2 \end{pmatrix}.
        \]\end{task}
\begin{task}
\label{task-150}
(Strang Ex 5.1.15)
        Use row operations to simplify and compute these determinants:
        \[
          \det\begin{pmatrix} 101 & 201 & 301 \\
          102 & 202 & 302  \\ 103 & 203 & 303
          \end{pmatrix} \quad \text{ and } \quad
          \det\begin{pmatrix} 1 & t & t^2 \\ t & 1 & t \\
          t^2 & t & 1 \end{pmatrix}.
        \]\end{task}
\begin{task}
\label{task-151}
(Strang Ex 5.1.23)
        
          Suppose that \(A = \left( \begin{smallmatrix} 4 & 1 \\ 2 & 3
          \end{smallmatrix} \right)\) and that \(\lambda\) is some number.
          Find \(A^2\), \(A^{-1}\) and \(A-\lambda I\) and their determinants.
        %
\par

          Which two numbers \(\lambda\) lead to \(\det(A-\lambda I) = 0\)?
        %
\end{task}
\typeout{************************************************}
\typeout{Section 5.2 Computing Determinants}
\typeout{************************************************}
\section[Computing Determinants]{Computing Determinants}\label{section-det-computation}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-120}
\begin{itemize}[label=\textbullet]
\item{}Read Chapter 5 section 2 of Strang.\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Learning Goals}
\typeout{************************************************}
\subsection[Learning Goals]{Learning Goals}\label{subsection-121}
Before class, a student should be able to compute the determinant by using
      cofactors. A student should also be able to compute a determiant using the
      ``big formula'' for matrices of size 2 or 3.
    %
\par

      Some time after class, a student should be comfortable with the different parts
      of the invertible matrix theorem.
    %
\typeout{************************************************}
\typeout{Subsection  Discussion: The Importance of the Determinant}
\typeout{************************************************}
\subsection[Discussion: The Importance of the Determinant]{Discussion: The Importance of the Determinant}\label{subsection-122}
Strang devotes all of his energy in this section to the different ways to
      compute the determinant. I don't have much to add to that.
    %
\par

      The real importance of the determinant is described in the following
      theorem. Note that this is a special result for square matrices.
      The shape is crucial for this result.
    %
\begin{theorem}\label{theorem-9}
(The Invertible Matrix Theorem)
        
          Let \(A\) be an \(n\times n\) matrix. Then the following conditions
          are equivalent:
        %
\begin{itemize}[label=\textbullet]
\item{}The columns of \(A\) are linearly independent.\item{}The columns of \(A\) are a spanning set for \(\mathbb{R}^n\).\item{}The colums of \(A\) are a basis for \(\mathbb{R}^n\).\item{}The rows of \(A\) are linearly independent.\item{}The rows of \(A\) are a spanning set for \(\mathbb{R}^n\).\item{}The rows of \(A\) are a basis for \(\mathbb{R}^n\).\item{}For any choice of vector \(b \in \mathbb{R}^n\), the system of linear
            equations \(Ax = b\) has a unique solution.\item{}\(A\) is invertible.\item{}The transpose \(A^T\) is invertible.\item{}\(\det(A) \neq 0\).\item{}\(\det(A^T) \neq 0\).\end{itemize}
\end{theorem}
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-123}
\begin{task}
\label{task-152}
 (Strang 5.2.2)
        Compute determinants of the following matrices using the big formula. Are
        the columns of these matrices linearly independent?
        \[
          A = \begin{pmatrix} 1 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 1 \end{pmatrix},
          B = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{pmatrix},
          C = \begin{pmatrix} A & 0 \\ 0 & A \end{pmatrix},
          D = \begin{pmatrix} A & 0 \\ 0 & B \end{pmatrix}.
        \]\end{task}
\begin{task}
\label{task-153}
 (Strang 5.2.3)
        Show that \(\det(A)=0\), no matter what values are used to fill in the
        five unknowns marked with dots. What are the cofactors of row 1? What is
        the rank of \(A\)? What are the six terms in the big formula?
        \[
          A = \begin{pmatrix} \bullet & \bullet & \bullet \\
          0 & 0 & \bullet \\ 0 & 0 & \bullet \end{pmatrix}.
        \]\end{task}
\begin{task}
\label{task-154}
 (Strang 5.2.4)
        Use cofactors to compute the determinants below:
        \[
          \det \begin{pmatrix} 1 & 0 & 0 & 1 \\ 0 & 1 & 1 & 1 \\
          1 & 1 & 0 & 1 \\ 1 & 0 & 0 & 1 \end{pmatrix}, \qquad
          \det \begin{pmatrix} 1 & 0 & 0 & 2 \\ 0 & 3 & 4 & 5 \\
          5 & 4 & 0 & 3 \\ 2 & 0 & 0 & 1 \end{pmatrix}.
        \]\end{task}
\begin{task}
\label{task-155}
 (Strang 5.2.5)
        What is the smallest arrangement of zeros you can place in a \(4 \times 4\)
        matrix to guarantee that its determinant is zero? Try to place as many non-zero
        entries as you can while keeping \(\det A \neq 0\).
      \end{task}
\begin{task}
\label{task-156}

        Decide if the columns of this matrix are linearly dependent without doing any
        row operations:
        \[
          A = \begin{pmatrix} 4 & 21\\ 3 & 16 \end{pmatrix}.
        \]\end{task}
\begin{task}
\label{task-157}

        Complete this matrix to one with determinant zero in four genuinely different
        ways. How did you make that happen?
        \[
          X = \begin{pmatrix} 2 & 1 & \bullet \\ 1 & 1 & \bullet
          \\ -1 & 1 & \bullet \end{pmatrix}.
        \]\end{task}
\typeout{************************************************}
\typeout{Section 5.3 Going Further: The Determinant}
\typeout{************************************************}
\section[Going Further: The Determinant]{Going Further: The Determinant}\label{section-det-gf}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-124}
\begin{itemize}[label=\textbullet]
\item{}
        Go back through the exercises in this chapter. Complete any items you
        did not complete the first time through. Prepare any that we have not
        discussed in class so that you will be ready to present them.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion}
\typeout{************************************************}
\subsection[Discussion]{Discussion}\label{subsection-125}
Now we take a short break to revisit and consolidate the learning you
      have done so far. Revisit the reading and the exercises you have done in
      the last two sections of Chapter Five. The important feature of this work should be
      learning to think about your own thinking. This sort of \terminology{meta-cognition}
      characterizes expert learners. Eventually, you want to be able to monitor
      your work at all times and recognize when you understand deeply and when
      you do not. This will allow you to self-correct.
    %
\par
To help you get started with meta-cognition, I listed learning goals in
      each section. To go further, you need to explicitly go through the process
      of reviewing what you can do and what you cannot. Here are some prompts to
      help you get started with this process.
      \begin{itemize}[label=\textbullet]
\item{}
          Review the learning goals from each section. Can you do the things
          described? Can you do them sometimes, or have you mastered them so you
          can do them consistently?
        \item{}
          Look through all of the tasks and go deeper into them. Can you
          connect each exercise to one of our pictures? Try to build a mental
          model of how the exercise and its solution work.
        \item{}
          If your first solution to an exercise involve a ``guess-and-check''
          approach, can you now complete the exercise in a \emph{purposeful}
          and systematic manner?
        \item{}
          Make a list of concepts or exercises that are not clear to you. Phrase
          each item in your list as a question, and make each question as
          specific as possible. Talk with fellow students or your
          instructor until you can answer your own questions.
        \end{itemize}

    %
\typeout{************************************************}
\typeout{Chapter 6 Eigendata and the Singular Value decomposition}
\typeout{************************************************}
\chapter[Eigendata and the Singular Value decomposition]{Eigendata and the Singular Value decomposition}\label{chapter-eigendata-and-svd}
\typeout{************************************************}
\typeout{Introduction  }
\typeout{************************************************}

      We have studied the algebraic structure of matrices, and we have studied the
      way that these algebraic properties are reflected in a geometric model where
      a matrix represents a transformation from one Euclidean space to another.
    %
\par

      In this chapter we shall study how to do this in a different way that cares
      more about the geometry adapted to the specific matrix. First we shall take
      up square matrices, where we will study \terminology{eigenvalues} and
      \terminology{eigenvectors}. We shall see that some matrices can be reimagined
      as if they are diagonal matrices. This idea of \terminology{diagonalization}
      is powerful, but it doesn't always work. We will study one situation where
      we know it will always work in the \terminology{spectral theorem} for symmetric
      matrices.
    %
\par

      Then we shall apply all that we have learned this term and put it together
      to study the \terminology{singular value decomposition} of a general rectangular
      matrix. This will be a good geometric understanding of how a matrix behaves
      as a function.
    %
\typeout{************************************************}
\typeout{Section 6.1 Eigenvectors and Eigenvalues}
\typeout{************************************************}
\section[Eigenvectors and Eigenvalues]{Eigenvectors and Eigenvalues}\label{section-eigenvectors}
\typeout{************************************************}
\typeout{Subsection  The assignment}
\typeout{************************************************}
\subsection[The assignment]{The assignment}\label{subsection-126}
\begin{itemize}[label=\textbullet]
\item{}Read section 6.1 of Strang (pages 283-292).\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion: Eigenvalues and Eigenvectors}
\typeout{************************************************}
\subsection[Discussion: Eigenvalues and Eigenvectors]{Discussion: Eigenvalues and Eigenvectors}\label{subsection-127}

      We have discussed the ``transformational view'' of the geometry of a
      system of \(m\) linear equations \(Ax = b\) in \(n\) unknowns
      \(x\), where we view the \(m \times n\) matrix \(A\) as defining
      a function from \(\mathbb{R}^m\) to \(\mathbb{R}^n\). In the case of
      a square matrix \(m=n\), the domain and the target are the same space
      \(\mathbb{R}^n\). So we can think of \(A\) as making a function from
      one space to itself.
    %
\par

      This means it might be interesting to think about how \(A\) moves a
      vector about inside of \(\mathbb{R}^n\). Usually, the vector \(v\)
      will get turned into a vector \(Av\) which has a different length and
      points in a completely different direction. But sometimes, sometimes,
      \(v\) and \(Av\) will point in the same direction. This is an eigenvector.
    %
\par

      A number \(\lambda\) is called an \terminology{eigenvalue} of the matrix
      \(A\) when the matrix \(A-\lambda I\) is singular. A vector \(v\)
      is called an \terminology{eigenvector} of \(A\) corresponding to \(\lambda\)
      when \(v\) is not zero but still lies in the null space of \(A-\lambda I\).
      We exclude \(0\) from being an eigenvector because it is boring. The
      zero vector lies in every subspace, including the nullspace of any matrix.
    %
\par

      As Strang discusses, the eigenvalues are found as roots of the
      \terminology{characteristic polynomial} \(\det(A-\lambda \cdot I) = 0\).
      That's right, we only need to find the roots of a polynomial! Sounds great,
      but as a general thing this is pretty hard. Don't get too excited. Have you
      heard this fact before? It is both depressing and interesting: there is no
      general formula to find the roots of a polynomial of degree 5 or more.
    %
\typeout{************************************************}
\typeout{Subsection  Sage and Eigenvectors}
\typeout{************************************************}
\subsection[Sage and Eigenvectors]{Sage and Eigenvectors}\label{subsection-128}

      Since eigenvalues and eigenvectors are found using standard techniques, we
      can use Sage to compute them without any new techniques.
    %
\typeout{************************************************}
\typeout{Subsubsection  Using Nullspaces and root finding commands}
\typeout{************************************************}
\subsubsection[Using Nullspaces and root finding commands]{Using Nullspaces and root finding commands}\label{subsubsection-48}

        Let's use basic sage commands we have seen before to compute eigenvalues
        and eigenvectors. We start by finding the characteristic polynomial of
        the mundane example matrix \(X\) below.
      %
\begin{lstlisting}[style=sageinput]
X = matrix(QQ, 3,3, [1,2,3,4,5,6,7,8,9])
X
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1 2 3]
[4 5 6]
[7 8 9]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
t = var('t')
poly = (X - t*identity_matrix(3)).determinant()
poly
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
-((t - 5)*(t - 9) - 48)*(t - 1) + 29*t + 3
\end{lstlisting}
\par

        Now we need the roots of that polynomial. Sage has a simple built-in for
        that, which returns a list of pairs: (root, multiplicity).
      %
\begin{lstlisting}[style=sageinput]
poly.roots()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[(-3/2*sqrt(33) + 15/2, 1), (3/2*sqrt(33) + 15/2, 1), (0, 1)]
\end{lstlisting}
\par

        In this case each of the three roots has (algebraic) multiplicity equal
        to one. For now, we will look at just the first one. Let's pull it out of
        this list, give it a more convenient name, and use it to find a corresponding
        eigenvector.
      %
\begin{lstlisting}[style=sageinput]
lam = poly.roots()[0][0]
V = (X - lam * identity_matrix(3)).right_kernel()
V
\end{lstlisting}
\par

        So, that looks like a mess. We can get Sage to display the basis vector
        more nicely. This is our eigenvector.
      %
\begin{lstlisting}[style=sageinput]
show(V.basis()[0])
\end{lstlisting}
\par
Well, that probably needs a simplification or two. But there it is!%
\typeout{************************************************}
\typeout{Subsubsection  Built-in Sage Commands}
\typeout{************************************************}
\subsubsection[Built-in Sage Commands]{Built-in Sage Commands}\label{subsubsection-49}

        Sage has useful built-in commands that get at the same computations. But
        for them to work, your matrix must be defined over a set of numbers that
        is big enough to take roots of polynomials. We will use \lstinline?AA?, which
        stands for the real algebraic numbers.
      %
\begin{lstlisting}[style=sageinput]
A = matrix(AA, 4,4, [3,134,-123,4, 2,1,34,4, 2,36,54,7, 0,0,3,1])
A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[   3  134 -123    4]
[   2    1   34    4]
[   2   36   54    7]
[   0    0    3    1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.characteristic_polynomial()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
x^4 - 59*x^3 - 990*x^2 + 18135*x - 14675
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.eigenvalues()
\end{lstlisting}
\par
The eigenvectors can be computed with this command, which again returns
        pairs: (eigenvalue, eigenvector).
      %
\begin{lstlisting}[style=sageinput]
A.eigenvectors_right()
\end{lstlisting}
\par

        Or you can ask for the \terminology{eigenspaces} which return pairs:
        (eigenvalue, subspace consisting of all eigenvectors which correspond).
      %
\begin{lstlisting}[style=sageinput]
A.eigenspaces_right()
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsubsection  One more example...}
\typeout{************************************************}
\subsubsection[One more example...]{One more example...}\label{subsubsection-50}
\begin{lstlisting}[style=sageinput]
B = matrix(AA, 2,2, [5,1,0,5])
B
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[5 1]
[0 5]
\end{lstlisting}
This matrix has only one eigenvalue, but it has algebraic multiplicity 2.%
\begin{lstlisting}[style=sageinput]
B.eigenvalues()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[5, 5]
\end{lstlisting}
\par
But 5 has \terminology{geometric multiplicity} only equal to one, because the corresponding
        eigenspace has dimension one.
      %
\begin{lstlisting}[style=sageinput]
B.eigenvectors_right()
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
B.eigenspaces_right()
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsection  Questions for Section 6.1}
\typeout{************************************************}
\subsection[Questions for Section 6.1]{Questions for Section 6.1}\label{subsection-129}
\begin{task}
\label{task-158}
Exercise 5 from section 6.1 of Strang.
      \end{task}
\begin{task}
\label{task-159}
Exercise 6 from section 6.1 of Strang.
      \end{task}
\begin{task}
\label{task-160}
Exercise 7 from section 6.1 of Strang.
      \end{task}
\begin{task}
\label{task-161}
Exercise 12 from section 6.1 of Strang.
      \end{task}
\begin{task}
\label{task-162}
Exercise 14 from section 6.1 of Strang.
      \end{task}
\begin{task}
\label{task-163}
Exercise 19 from section 6.1 of Strang.
      \end{task}
\typeout{************************************************}
\typeout{Section 6.2 Diagonalizing Matrices}
\typeout{************************************************}
\section[Diagonalizing Matrices]{Diagonalizing Matrices}\label{section-diagonalization}
\typeout{************************************************}
\typeout{Subsection  The assignment}
\typeout{************************************************}
\subsection[The assignment]{The assignment}\label{subsection-130}
\begin{itemize}[label=\textbullet]
\item{} Read section 6.2 of Strang (pages 298-307).\item{} Read the following.\item{} Prepare the items below for presentation.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Diagonalizing Matrices}
\typeout{************************************************}
\subsection[Diagonalizing Matrices]{Diagonalizing Matrices}\label{subsection-131}

      The big result here is this:
    %
\begin{theorem}\label{theorem-10}

        Let \(A\) be an \(n\times n\) square matrix. Then the following
        two conditions are equivalent:
        \begin{itemize}[label=\textbullet]
\item{}
            There is a basis \(\beta= \{ v_1, v_2, \ldots, v_n \}\) for
            \(\mathbb{R}^n\) consisting of eigenvectors for \(A\).
          \item{}
            It is possible to find an invertible matrix \(S\) so that
            \(A = S \Lambda S^{-1}\), where \(\Lambda\) is a diagonal
            matrix whose entries are the eigenvalues of \(A\).
          \end{itemize}
\end{theorem}
\par

      The connection between the two conditions is that the matrix \(S\) has
      as its columns the eigenvectors of \(A\). (In fact, that is really the
      heart of the proof of this theorem. The rest is just details.)
    %
\par

      If a matrix satisfies these two conditions, then we say it is
      \terminology{diagonalizable}. We should note right away that not all
      matrices are diagonalizable. We have already seen examples of matrices
      where the geometric multiplicity of an eigenvalue is less than the
      algebraic multiplicity, like
      \(A = \left( \begin{smallmatrix} 5 & 1 \\ 0 & 5 \end{smallmatrix}\right)\).
      In this case, it becomes impossible to find a basis consisting of eigenvectors.
    %
\par

      In a way, this allows us to see something interesting: maybe a matrix
      really wants to be a diagonal matrix, but we are looking at the
      transformation \(A\) using ``the wrong basis.'' By wrong, here I
      mean that the standard basis is not the most convenient one, and another
      one makes our lives easier.
    %
\typeout{************************************************}
\typeout{Subsection  Sage and Diagonalization}
\typeout{************************************************}
\subsection[Sage and Diagonalization]{Sage and Diagonalization}\label{subsection-132}

      Sage has built-in commands about diagonalization. We shall try a few out here.
      We need a matrix to play with, so we take this one:
    %
\begin{lstlisting}[style=sageinput]
A = matrix(AA, 3,3, [1,2,3,4,5,6,7,8,-1])
A.rank()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
3
\end{lstlisting}
\par

      We chose to define this matrix over AA because we need to find
      roots of polynomials when looking for eigenvalues. AA is the
      set of \terminology{algebraic numbers}, which just means the collection of
      all roots of polynomials with integer coefficients.
    %
\begin{lstlisting}[style=sageinput]
A.is_diagonalizable()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\par

      Sage has a command for finding the eigenvector decomposition
      \(A = S\Lambda S^{-1}\).
    %
\begin{lstlisting}[style=sageinput]
A.eigenmatrix_right()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
([ 11.816056999423874?                    0                    0]
[                   0 -0.3954315737468559?                    0]
[                   0                    0  -6.420625425677017?], [  1.000000000000000?   1.000000000000000?   1.000000000000000?]
[  2.369820536283515?  -0.866496699124881?   1.460961877127081?]
[  2.025471975618948? 0.11252060816763521?  -3.447516393310393?])
\end{lstlisting}
\par

      As you see, Sage returns a pair of matrices. One of them is diagonal, so that
      is probably \(\Lambda\). We'll use tuple unpacking to assign the matrices to
      sensible names.
    %
\begin{lstlisting}[style=sageinput]
Lambda, S = A.eigenmatrix_right()
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
Lambda
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 11.816056999423874?                    0                    0]
[                   0 -0.3954315737468559?                    0]
[                   0                    0  -6.420625425677017?]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
S
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[  1.000000000000000?   1.000000000000000?   1.000000000000000?]
[  2.369820536283515?  -0.866496699124881?   1.460961877127081?]
[  2.025471975618948? 0.11252060816763521?  -3.447516393310393?]
\end{lstlisting}
\par

      Note that \(S\) has the eigenvectors of \(A\) as its columns, and the
      corresponding eigenvalues are lined up as the diagonal entries of \(\Lambda\).
    %
\begin{lstlisting}[style=sageinput]
A.eigenvectors_right()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[(11.816056999423874?, [
(1.000000000000000?, 2.369820536283515?, 2.025471975618948?)
], 1), (-0.3954315737468559?, [
(1.000000000000000?, -0.866496699124881?, 0.11252060816763521?)
], 1), (-6.420625425677017?, [
(1.000000000000000?, 1.460961877127081?, -3.447516393310393?)
], 1)]
\end{lstlisting}
\par
Anyway, now we can check that everything lines up correctly:%
\begin{lstlisting}[style=sageinput]
S * Lambda * S.inverse()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[ 1.000000000000000?  2.000000000000000?  3.000000000000000?]
[ 4.000000000000000?  5.000000000000000?  6.000000000000000?]
[ 7.000000000000000?   8.00000000000000? -1.000000000000000?]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
S * Lambda * S.inverse() == A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
True
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsection  Questions for Section 6.2}
\typeout{************************************************}
\subsection[Questions for Section 6.2]{Questions for Section 6.2}\label{subsection-133}
\begin{task}
\label{task-164}

          Let \(e_1, e_2, e_3\) be the standard basis of \(\mathbb{R}^3\):
          \[
            e_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \quad
            e_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}, \quad
            e_3 = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}.
          \]
          Make an example of an invertible \(3 \times 3\) matrix \(S\).
          Write your matrix as a matrix of column vectors.
          \[
            S = \begin{pmatrix} | & | & | \\ v_1 & v_2 & v_3 \\
             | & | & | \end{pmatrix}
          \]
          How do you know that the set \(\{ Se_1, Se_2, Se_3 \}\) is a basis
          for \(\mathbb{R}^3\)?
        %
\par

          What is the connection between \(Se_1\), \(Se_2\), \(Se_3\),
          \(S^{-1}v_1\), \(S^{-1}v_2\), \(S^{-1}v_3\) and the original
          vectors \(e_1, e_2, e_3, v_1, v_2, v_3\)?
        %
\par

          Finally, how do we use this to understand the way that the decomposition
          \(A = S\Lambda S^{-1}\) works?
        %
\end{task}
\begin{task}
\label{task-165}

        Exercise 1 from section 6.2 of Strang.
      \end{task}
\begin{task}
\label{task-166}

        Exercise 2 from section 6.2 of Strang.
      \end{task}
\begin{task}
\label{task-167}

        Exercise 3 from section 6.2 of Strang.
      \end{task}
\begin{task}
\label{task-168}

        Exercise 13 from section 6.2 of Strang.
      \end{task}
\begin{task}
\label{task-169}

        Exercise 19 from section 6.2 of Strang.
      \end{task}
\typeout{************************************************}
\typeout{Section 6.3 The Spectral Theorem}
\typeout{************************************************}
\section[The Spectral Theorem]{The Spectral Theorem}\label{section-spectral-theorem}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-134}
\begin{itemize}[label=\textbullet]
\item{}Read chapter 6 section 4 of \emph{Strang}\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion: The Spectral Theorem for Symmetric Matrices}
\typeout{************************************************}
\subsection[Discussion: The Spectral Theorem for Symmetric Matrices]{Discussion: The Spectral Theorem for Symmetric Matrices}\label{subsection-135}

      We are now in a position to discuss a major result about the structure of
      symmetric (square) matrices: The Spectral Theorem.
    %
\begin{theorem}\label{theorem-11}

        Suppose that \(A\) is a symmetric \(n \times n\) matrix. Then
        there exists an orthonormal basis \(\{ q_1, q_2, \ldots , q_n \}\)
        of \(\mathbb{R}^n\) consisting of eigenvectors of \(A\).
        This means that we can factor \(A\) as a product
        \[
          A = Q \Lambda Q^{-1} = Q \Lambda Q^T,
        \]
        where \(Q\) is an orthogonal matrix having the vectors \(q_i\) as
        its columns, and \(\Lambda\) is a diagonal matrix with the eigenvalues
        of \(A\) as its entries.
      \end{theorem}
\par

      I think Strang's argument for the truth of this theorem is too terse, and hence
      confusing for a first time reader. The main points of the argument are these:
    %
\begin{enumerate}
\item{}
        If \(X\) is a symmetric matrix, then all of its eigenvalues are real
        numbers.
      \item{}
        If \(X\) is a symmetric matrix and \(v\) and \(w\) are eigenvectors
        of \(X\) which correspond to \emph{different} eigenvalues, then
        \(v\) and \(w\) are orthogonal vectors.
      \item{}
        If \(X\) is a symmetric matrix and \(\lambda\) is an eigenvalue of
        \(X\), then subspace of \(\lambda\)-eigenvectors for \(X\) has
        dimension equal to the multiplicity of \(\lambda\) as a root of
        the characteristic polynomial of \(X\). (This point is often stated by
        saying that the \terminology{geometric multiplicity} of \(\lambda\) is
        equal to the \terminology{algebraic multiplicity} of \(\lambda\).)
      \end{enumerate}
\par

      Let's clear up that bit about the different types of multiplicity. We can
      identify eigenvalues by finding them as roots of the characteristic polynomial
      \(p_A(t) = \mathrm{det}(A-t\cdot I)\) of \(A\). Of course, an particular
      root can be a root \emph{multiple times}. For example, \(5\) is a root of
      the polynomial \(t^2 - 5t + 25=0\) twice. So we say \(5\) has multiplicity
      two. In the context of eigenvalues, this multiplicity is called the
      \terminology{algebraic multiplicity} of an eigenvalue, since it comes out of
      the consideration of the algebra.
    %
\par

      Another way to count up the number of times a number counts as an eigenvalue
      is to use the number of eigenvectors corresponding to that number. But we
      only want to count up truly independent directions, so we should use the
      dimension of the subspace of eigenvectors. This is the \terminology{geometric
      multiplicity} of an eigenvalue. It is a fact that the geometric
      multiplicity is not greater than the algebraic multiplicity. But the two can
      be different. For example, consider this matrix:
      \[G = \begin{pmatrix} 5 & 1 \\ 0 & 5 \end{pmatrix}.\]
    %
\par

      Now, how does one understand the Spectral Theorem? It basically guarantees
      that we can always find (a) enough eigenvalues (as real numbers), and (b) for each eigenvalue,
      enough eigenvectors. The hardest parts of the proof come from part (b) where
      you have to produce enough eigenvectors. But in practice, if you have an
      example of a symmetric matrix, you can find the decomposition mentioned in
      the theorem pretty easily. First, find the eigenvalues. Then for each
      eigenvalue \(\lambda\), find an orthonormal basis for the
      \terminology{eigenspace}
      \[E_{\lambda} = \mathrm{null}(A-\lambda\cdot I).\]
      That second bit can be done in two steps, first find a basis for \(E_{\lambda}\)
      (special solutions!) and then apply the Gram-Schmidt algorithm to find an
      orthonormal basis for \(E_{\lambda}\). Collecting all of these bases
      together will make a basis for \(\mathbb{R}^n\).
    %
\typeout{************************************************}
\typeout{Subsection  Sage and the Spectral Theorem}
\typeout{************************************************}
\subsection[Sage and the Spectral Theorem]{Sage and the Spectral Theorem}\label{subsection-136}

      Sage does not have any built-in commands that deal with the spectral
      decomposition of a symmetric square matrix. But here are a few commands
      that you might find useful as you hack your solution together by hand:
    %
\par

      The first command you might find useful is \lstinline?.change_ring()?. This is
      helpful for those times when you define a matrix over some convenient ring
      like \lstinline?QQ?, but then want to work with eigevalues and eigenvectors and
      so need a bigger ring that you can take roots in. Using this command doesn't
      change the matrix, so much as tell Sage to think of it as having entries
      from a different set of numbers.
    %
\begin{lstlisting}[style=sageinput]
A = matrix(QQ, 2,2, [1,2,3,4])
A.change_ring(AA)
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1, 2]
[3, 4]
\end{lstlisting}
\par

      The command \lstinline?.jordan_form(transformation=True)? will return a pair
      consisting of
      a diagonal matrix with the eigenvalues as entries and an invertible matrix
      consisting of a basis of eigenvectors. These eigenvectors will NOT be an
      orthonormal basis. You will have to use Gram-Schmidt to fix this to a
      proper basis promised by the theorem.
    %
\par

      Note: the \terminology{Jordan Form} is a generalization of the diagonalization
      process that works for matrices which might not be symmetric. We'll use it
      here to short-cut some of the work.
    %
\par

      Let's do an example. First, we will make a symmetric matrix. Then we will
      find the eigenvalues and eigenvectors
    %
\begin{lstlisting}[style=sageinput]
A = matrix(QQ, 3,3, [1,2,4, 5,3,2, -1,3,3])
X = A.transpose()*A
Y = X.change_ring(AA); Y
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[27 14 11]
[14 22 23]
[11 23 29]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
D, S = Y.jordan_form(transformation=True)
D
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[1.606673922554331?|                 0|                 0]
[------------------+------------------+------------------]
[                 0|17.88104756294764?|                 0]
[------------------+------------------+------------------]
[                 0|                 0|58.51227851449803?]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
S
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[  1.000000000000000?   1.000000000000000?   1.000000000000000?]
[ -4.402902344870359? -0.2014369844407669?   1.214399978615443?]
[  3.295209704612669? -0.5726213322619663?   1.319152619443804?]
\end{lstlisting}
\par

      In this case, we have three different \(1\)-dimensional eigenspaces, so
      things are not too hard! If we apply Gram-Schmidt, we will just normalize
      those vectors.
    %
\begin{lstlisting}[style=sageinput]
Q, R = S.QR()
Q * D * Q.transpose()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[27.00000000000000? 14.00000000000000? 11.00000000000000?]
[14.00000000000000? 22.00000000000000? 23.00000000000000?]
[11.00000000000000? 23.00000000000000? 29.00000000000000?]
\end{lstlisting}
\par

      That is about as close as we can get to displaying the original \lstinline?X?.
    %
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-137}
\begin{task}
\label{task-170}

        From Strang section 6.4, do exercise 3.
      \end{task}
\begin{task}
\label{task-171}

        From Strang section 6.4, do exercise 4.
      \end{task}
\begin{task}
\label{task-172}

        From Strang section 6.4, do exercise 5.
      \end{task}
\begin{task}
\label{task-173}

        From Strang section 6.4, do exercise 6.
      \end{task}
\begin{task}
\label{task-174}

        From Strang section 6.4, do exercise 8.
      \end{task}
\begin{task}
\label{task-175}

        From Strang section 6.4, do exercise 11.
      \end{task}
\begin{task}
\label{task-176}

        From Strang section 6.4, do exercise 12.
      \end{task}
\begin{task}
\label{task-177}

        From Strang section 6.4, do exercise 24.
      \end{task}
\typeout{************************************************}
\typeout{Section 6.4 Going Further with Eigendata}
\typeout{************************************************}
\section[Going Further with Eigendata]{Going Further with Eigendata}\label{section-gf-eigendata}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-138}
\begin{itemize}[label=\textbullet]
\item{}
        Go back through the exercises in the first three sections of this chapter.
        Complete any items you
        did not complete the first time through. Prepare any that we have not
        discussed in class so that you will be ready to present them.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion}
\typeout{************************************************}
\subsection[Discussion]{Discussion}\label{subsection-139}
Now we take a short break to revisit and consolidate the learning you
      have done so far. Revisit the reading and the exercises you have done in
      the first three sections of Chapter Six. The important feature of this work should be
      learning to think about your own thinking. This sort of \terminology{meta-cognition}
      characterizes expert learners. Eventually, you want to be able to monitor
      your work at all times and recognize when you understand deeply and when
      you do not. This will allow you to self-correct.
    %
\par
To help you get started with meta-cognition, I listed learning goals in
      each section. To go further, you need to explicitly go through the process
      of reviewing what you can do and what you cannot. Here are some prompts to
      help you get started with this process.
      \begin{itemize}[label=\textbullet]
\item{}
          Review the learning goals from each section. Can you do the things
          described? Can you do them sometimes, or have you mastered them so you
          can do them consistently?
        \item{}
          Look through all of the tasks and go deeper into them. Can you
          connect each exercise to one of our pictures? Try to build a mental
          model of how the exercise and its solution work.
        \item{}
          If your first solution to an exercise involve a ``guess-and-check''
          approach, can you now complete the exercise in a \emph{purposeful}
          and systematic manner?
        \item{}
          Make a list of concepts or exercises that are not clear to you. Phrase
          each item in your list as a question, and make each question as
          specific as possible. Talk with fellow students or your
          instructor until you can answer your own questions.
        \end{itemize}

    %
\typeout{************************************************}
\typeout{Section 6.5 Singular Value Decomposition}
\typeout{************************************************}
\section[Singular Value Decomposition]{Singular Value Decomposition}\label{section-svd}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-140}
\begin{itemize}[label=\textbullet]
\item{}Read chapter 6 section 7 of \emph{Strang}\item{}Read the following and complete the exercises below.\end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion: The Singular Value Decomposition}
\typeout{************************************************}
\subsection[Discussion: The Singular Value Decomposition]{Discussion: The Singular Value Decomposition}\label{subsection-141}

      The \terminology{Singular Value Decomposition} is an adaptation of the ideas
      behind eigenvectors and eigenvalues for non-square matrices. If our matrix
      \(A\) is \(n \times m\), the idea is to
      choose
    %
\begin{itemize}[label=\textbullet]
\item{}An orthonormal basis \(\{ v_1, \ldots, v_n\}\) for \(\mathbb{R}^n\), \item{}and an orthonormal basis \(\{ u_1, \ldots, u_n \}\) for \(\mathbb{R}^m\)\end{itemize}
\par

      so that \[Av_i = \sigma_i u_i, \qquad \text{for $1 \leq i \leq r$}\]
      where \(r = rank(A)\). If we pile up all of those equations, we get a
      statement like this one:
    %
\[
      A \begin{pmatrix} | & | & \dots & | \\
                      v_1 & v_2 & \dots & v_r \\
                        | & | & \dots & |
                        \end{pmatrix} =
        \begin{pmatrix} \sigma_1 & 0 & \dots & 0 \\
                        0 & \sigma_2 & \dots & 0 \\
                        \vdots & \vdots & \ddots & \vdots  \\
                         0 & 0 & \dots & \sigma_r
        \end{pmatrix}
        \begin{pmatrix} | & | & \dots & | \\
                        u_1 & u_2 & \dots & u_r \\
                          | & | & \dots & |
                          \end{pmatrix}
    \]\par

      Eventually, this will lead us to a matrix decomposition of the form
    %
\[ A = U \Sigma V^T .\]\typeout{************************************************}
\typeout{Subsubsection  How to do it}
\typeout{************************************************}
\subsubsection[How to do it]{How to do it}\label{subsubsection-51}

        So, suppose that \(A\) is an \(m \times n\) matrix. The key fact we
        need is that \(A^TA\) is a symmetric \(n \times n\) matrix. Later,
        when discussing properties, it is important that
        \(AA^T\) is a symmetric \(m \times m\) matrix, and also that
        \(A(A^TA) = (AA^T)A\).
      %
\begin{enumerate}
\item{}
          
            Step One: Compute a spectral decomposition of \(A^TA\).
            Since \(A^TA\) is a square symmetric matrix, we can find an orthonormal
            basis \(v_1, v_2, \ldots, v_n\) for \(\mathbb{R}^n\) which consists
            of eigenvectors for \(A^TA\). If we bundle these together as the columns
            of a matrix \(V\), we can make
            the spectral decomposition
            \[
              A^TA = V D V^T,
            \]
            where \(D\) is a diagonal matrix having the eigenvalues
            \(\lambda_1, \lambda_2, \ldots, \lambda_n\) for entries.
            NOTE: We will organize things so that the eigenvalues get smaller as
            we go further in the list.
          %

          \par

            Now, some of the eigenvaules might be zero, but none will be negative.
            We will have exactly \(r = \mathrm{rank}(A)\) non-zero eigenvalues, and the
            other \(n-r\) will be equal to zero. (Those come from the directions
            in the null space!) Don't sweat it. Everything is going to be fine.
          %

        \item{}
          
            Step Two: For each \(0 \leq i \leq r\), set
            \[\sigma_i = \sqrt{\lambda_i} .\]
            These are the ``Singular Values'' named in the singular value decomposition.
          %

        \item{}
          
            Step Three: for each \(0 \leq i \leq r\), set
            \[
              u_i = \dfrac{1}{\sigma_i} Av_i.
            \]
            By the way things are set up, we are sure \(\sigma_i\) is not zero!
            The vectors we just made are a basis for the column space of \(A\).
          %

        \item{}
          
            Step Four: If \(m > r \), Choose an orthonormal basis \(u_{r+1}, \ldots, u_m\)
            for the null space of \(A^T\).
          %

        \item{}
          Step Five: Bundle together the \(v_i\)'s as columns of an
          \(n \times n\) matrix \(V\), and the \(u_i\)'s as the columns
          of an \(m \times m\) matrix \(U\). Both of these are orthogonal matrices.
          Then place the \(\sigma_i\)'s on the main diagonal of a rectangular
          \(m \times n\) matrix which is otherwise filled with zeros. Call that
          new matrix \(\Sigma\).
        \end{enumerate}
\par
Strang has a good run-down of the neat properties this set up has.%
\typeout{************************************************}
\typeout{Subsection  Sage and the SVD}
\typeout{************************************************}
\subsection[Sage and the SVD]{Sage and the SVD}\label{subsection-142}

      Sage has a built in command for the singular value decomposition of a matrix.
      If you have a matrix \lstinline?A?, the command is \lstinline?A.SVD()?. But there is a
      little trick to using it! At present, Sage only has this function implemented
      for matrices defined over rings of ``floating point numbers''. The best
      way around this is to either define your matrix with entries in the ring \lstinline?RDF?,
      or use the \lstinline?.change_ring(RDF)? method on you matrix before you use the SVD.
    %
\begin{lstlisting}[style=sageinput]
A = matrix(QQ, 2,2, [2,1,1,1])
A
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
[2, 1]
[1, 1]
\end{lstlisting}
\begin{lstlisting}[style=sageinput]
A.change_ring(RDF).SVD()
\end{lstlisting}
\begin{lstlisting}[style=sageoutput]
        (
        [-0.850650808352 -0.525731112119]  [2.61803398875           0.0]
        [-0.525731112119  0.850650808352], [          0.0 0.38196601125],

        [-0.850650808352 -0.525731112119]
        [-0.525731112119  0.850650808352]
        )
\end{lstlisting}
\typeout{************************************************}
\typeout{Subsection  Exercises}
\typeout{************************************************}
\subsection[Exercises]{Exercises}\label{subsection-143}
\begin{task}
\label{task-178}
Complete exercise 6.7.1 from \emph{Strang}.
      \end{task}
\begin{task}
\label{task-179}
Complete exercise 6.7.2 from \emph{Strang}.
      \end{task}
\begin{task}
\label{task-180}
Complete exercise 6.7.3 from \emph{Strang}.
      \end{task}
\begin{task}
\label{task-181}
Complete exercise 6.7.4 from \emph{Strang}.
      \end{task}
\begin{task}
\label{task-182}
Complete exercise 6.7.5 from \emph{Strang}.
      \end{task}
\begin{task}
\label{task-183}
Complete exercise 6.7.6 from \emph{Strang}.
      \end{task}
\begin{task}
\label{task-184}
Complete exercise 6.7.9 from \emph{Strang}.
      \end{task}
\begin{task}
\label{task-185}
Complete exercise 6.7.10 from \emph{Strang}.
      \end{task}
\begin{task}
\label{task-186}
Complete exercise 6.7.11 from \emph{Strang}.
      \end{task}
\begin{task}
\label{task-187}
Complete exercise 6.7.12 from \emph{Strang}.
      \end{task}
\typeout{************************************************}
\typeout{Section 6.6 Going Further with the SVD}
\typeout{************************************************}
\section[Going Further with the SVD]{Going Further with the SVD}\label{section-gf-svd}
\typeout{************************************************}
\typeout{Subsection  The Assignment}
\typeout{************************************************}
\subsection[The Assignment]{The Assignment}\label{subsection-144}
\begin{itemize}[label=\textbullet]
\item{}
        Go back through the exercises in the last section. Complete any items you
        did not complete the first time through. Prepare any that we have not
        discussed in class so that you will be ready to present them.
      \end{itemize}
\typeout{************************************************}
\typeout{Subsection  Discussion}
\typeout{************************************************}
\subsection[Discussion]{Discussion}\label{subsection-145}
Now we take a short break to revisit and consolidate the learning you
      have done so far. Revisit the reading and the exercises you have done in
      Chapter Six. The important feature of this work should be
      learning to think about your own thinking. This sort of \terminology{meta-cognition}
      characterizes expert learners. Eventually, you want to be able to monitor
      your work at all times and recognize when you understand deeply and when
      you do not. This will allow you to self-correct.
    %
\par
To help you get started with meta-cognition, I listed learning goals in
      each section. To go further, you need to explicitly go through the process
      of reviewing what you can do and what you cannot. Here are some prompts to
      help you get started with this process.
      \begin{itemize}[label=\textbullet]
\item{}
          Review the learning goals from each section. Can you do the things
          described? Can you do them sometimes, or have you mastered them so you
          can do them consistently?
        \item{}
          Look through all of the tasks and go deeper into them. Can you
          connect each exercise to one of our pictures? Try to build a mental
          model of how the exercise and its solution work.
        \item{}
          If your first solution to an exercise involve a ``guess-and-check''
          approach, can you now complete the exercise in a \emph{purposeful}
          and systematic manner?
        \item{}
          Make a list of concepts or exercises that are not clear to you. Phrase
          each item in your list as a question, and make each question as
          specific as possible. Talk with fellow students or your
          instructor until you can answer your own questions.
        \end{itemize}

    %
\end{document}
